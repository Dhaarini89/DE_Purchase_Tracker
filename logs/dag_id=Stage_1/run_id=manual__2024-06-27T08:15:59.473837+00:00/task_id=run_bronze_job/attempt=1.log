[2024-06-27T08:16:08.201+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T08:16:08.244+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T08:15:59.473837+00:00 [queued]>
[2024-06-27T08:16:08.256+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T08:15:59.473837+00:00 [queued]>
[2024-06-27T08:16:08.256+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-27T08:16:08.271+0000] {taskinstance.py:2330} INFO - Executing <Task(DockerOperator): run_bronze_job> on 2024-06-27 08:15:59.473837+00:00
[2024-06-27T08:16:08.280+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=1269) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-27T08:16:08.282+0000] {standard_task_runner.py:63} INFO - Started process 1270 to run task
[2024-06-27T08:16:08.280+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Stage_1', 'run_bronze_job', 'manual__2024-06-27T08:15:59.473837+00:00', '--job-id', '256', '--raw', '--subdir', 'DAGS_FOLDER/kafka_dag.py', '--cfg-path', '/tmp/tmpmydbg1iy']
[2024-06-27T08:16:08.282+0000] {standard_task_runner.py:91} INFO - Job 256: Subtask run_bronze_job
[2024-06-27T08:16:08.297+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-06-27T08:16:08.333+0000] {task_command.py:426} INFO - Running <TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T08:15:59.473837+00:00 [running]> on host 08bfa8b73cac
[2024-06-27T08:16:08.451+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Stage_1' AIRFLOW_CTX_TASK_ID='run_bronze_job' AIRFLOW_CTX_EXECUTION_DATE='2024-06-27T08:15:59.473837+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-27T08:15:59.473837+00:00'
[2024-06-27T08:16:08.452+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T08:16:08.487+0000] {docker.py:366} INFO - Starting docker container from image bitnami/spark:latest
[2024-06-27T08:16:08.491+0000] {docker.py:374} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-06-27T08:16:09.037+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:16:09.03 [0m[38;5;2mINFO [0m ==>
[2024-06-27T08:16:09.040+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:16:09.03 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-27T08:16:09.042+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:16:09.04 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-27T08:16:09.045+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:16:09.04 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-27T08:16:09.047+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:16:09.04 [0m[38;5;2mINFO [0m ==> Upgrade to Tanzu Application Catalog for production environments to access custom-configured and pre-packaged software components. Gain enhanced features, including Software Bill of Materials (SBOM), CVE scan result reports, and VEX documents. To learn more, visit [1mhttps://bitnami.com/enterprise[0m
[2024-06-27T08:16:09.049+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:16:09.04 [0m[38;5;2mINFO [0m ==>
[2024-06-27T08:16:09.059+0000] {docker.py:436} INFO - 
[2024-06-27T08:16:12.381+0000] {docker.py:436} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-27T08:16:12.509+0000] {docker.py:436} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2024-06-27T08:16:12.510+0000] {docker.py:436} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-27T08:16:12.535+0000] {docker.py:436} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
org.postgresql#postgresql added as a dependency
[2024-06-27T08:16:12.539+0000] {docker.py:436} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-618765d1-45f7-4e3d-a0ae-0bf84d0d6c96;1.0
	confs: [default]
[2024-06-27T08:16:14.631+0000] {docker.py:436} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T08:16:15.209+0000] {docker.py:436} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T08:16:15.337+0000] {docker.py:436} INFO - found org.apache.kafka#kafka-clients;2.8.1 in central
[2024-06-27T08:16:15.435+0000] {docker.py:436} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-27T08:16:15.494+0000] {docker.py:436} INFO - found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2024-06-27T08:16:15.711+0000] {docker.py:436} INFO - found org.slf4j#slf4j-api;1.7.32 in central
[2024-06-27T08:16:17.632+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2024-06-27T08:16:17.742+0000] {docker.py:436} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2024-06-27T08:16:17.877+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2024-06-27T08:16:18.215+0000] {docker.py:436} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-27T08:16:18.260+0000] {docker.py:436} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-27T08:16:18.613+0000] {docker.py:436} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-27T08:16:20.677+0000] {docker.py:436} INFO - found org.postgresql#postgresql;42.2.2 in central
[2024-06-27T08:16:20.697+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T08:16:20.807+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (118ms)
[2024-06-27T08:16:20.819+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar ...
[2024-06-27T08:16:20.950+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.2.2!postgresql.jar(bundle) (141ms)
[2024-06-27T08:16:20.960+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T08:16:20.981+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (30ms)
[2024-06-27T08:16:20.997+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...
[2024-06-27T08:16:21.526+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (544ms)
[2024-06-27T08:16:21.554+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-27T08:16:21.572+0000] {docker.py:436} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (41ms)
[2024-06-27T08:16:21.581+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-27T08:16:21.607+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (35ms)
[2024-06-27T08:16:21.616+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[2024-06-27T08:16:21.625+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (17ms)
[2024-06-27T08:16:21.639+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...
[2024-06-27T08:16:25.286+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (3659ms)
[2024-06-27T08:16:25.297+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-27T08:16:25.400+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (112ms)
[2024-06-27T08:16:25.414+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...
[2024-06-27T08:16:25.638+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (235ms)
[2024-06-27T08:16:25.649+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...
[2024-06-27T08:16:25.663+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (23ms)
[2024-06-27T08:16:25.673+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...
[2024-06-27T08:16:27.661+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (1997ms)
[2024-06-27T08:16:27.672+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-27T08:16:27.688+0000] {docker.py:436} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (27ms)
[2024-06-27T08:16:27.689+0000] {docker.py:436} INFO - :: resolution report :: resolve 8149ms :: artifacts dl 7001ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2024-06-27T08:16:27.690+0000] {docker.py:436} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
[2024-06-27T08:16:27.690+0000] {docker.py:436} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.2.2 from central in [default]
	org.slf4j#slf4j-api;1.7.32 from central in [default]
[2024-06-27T08:16:27.691+0000] {docker.py:436} INFO - org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
[2024-06-27T08:16:27.691+0000] {docker.py:436} INFO - ---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
[2024-06-27T08:16:27.692+0000] {docker.py:436} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2024-06-27T08:16:27.692+0000] {docker.py:436} INFO - |      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-27T08:16:27.699+0000] {docker.py:436} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-618765d1-45f7-4e3d-a0ae-0bf84d0d6c96
[2024-06-27T08:16:27.700+0000] {docker.py:436} INFO - confs: [default]
[2024-06-27T08:16:27.783+0000] {docker.py:436} INFO - 13 artifacts copied, 0 already retrieved (57403kB/84ms)
[2024-06-27T08:16:28.129+0000] {docker.py:436} INFO - 24/06/27 08:16:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-27T08:16:30.396+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SparkContext: Running Spark version 3.5.1
24/06/27 08:16:30 INFO SparkContext: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T08:16:30.398+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SparkContext: Java version 17.0.11
[2024-06-27T08:16:30.460+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO ResourceUtils: ==============================================================
[2024-06-27T08:16:30.462+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-27T08:16:30.464+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO ResourceUtils: ==============================================================
[2024-06-27T08:16:30.465+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-27T08:16:30.551+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-27T08:16:30.566+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO ResourceProfile: Limiting resource is cpu
[2024-06-27T08:16:30.568+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-27T08:16:30.663+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SecurityManager: Changing view acls to: spark
[2024-06-27T08:16:30.664+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SecurityManager: Changing modify acls to: spark
[2024-06-27T08:16:30.665+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SecurityManager: Changing view acls groups to:
[2024-06-27T08:16:30.667+0000] {docker.py:436} INFO - 24/06/27 08:16:30 INFO SecurityManager: Changing modify acls groups to: 
24/06/27 08:16:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-27T08:16:31.190+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO Utils: Successfully started service 'sparkDriver' on port 43439.
[2024-06-27T08:16:31.250+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkEnv: Registering MapOutputTracker
[2024-06-27T08:16:31.307+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-27T08:16:31.342+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-27T08:16:31.343+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-27T08:16:31.351+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-27T08:16:31.385+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5f17667-a334-4dcc-8f06-00ea4b68b5fb
[2024-06-27T08:16:31.404+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-27T08:16:31.435+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-27T08:16:31.745+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-27T08:16:31.891+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-27T08:16:31.997+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://localhost:43439/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:31.998+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at spark://localhost:43439/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476190379
[2024-06-27T08:16:31.999+0000] {docker.py:436} INFO - 24/06/27 08:16:31 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://localhost:43439/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.003+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://localhost:43439/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476190379
[2024-06-27T08:16:32.005+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:43439/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.008+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:43439/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476190379
24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://localhost:43439/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476190379
24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://localhost:43439/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476190379
24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:43439/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.017+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://localhost:43439/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476190379
[2024-06-27T08:16:32.019+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://localhost:43439/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476190379
[2024-06-27T08:16:32.020+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://localhost:43439/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:32.022+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:43439/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476190379
[2024-06-27T08:16:32.034+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.045+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:16:32.070+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476190379
24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T08:16:32.079+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.080+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:16:32.091+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476190379
[2024-06-27T08:16:32.094+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T08:16:32.120+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.120+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T08:16:32.142+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476190379
[2024-06-27T08:16:32.143+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T08:16:32.171+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.177+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T08:16:32.196+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:32.198+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T08:16:32.324+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.326+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T08:16:32.336+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476190379
[2024-06-27T08:16:32.337+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T08:16:32.361+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476190379
[2024-06-27T08:16:32.363+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T08:16:32.375+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:32.376+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T08:16:32.411+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476190379
24/06/27 08:16:32 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T08:16:32.618+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Starting executor ID driver on host localhost
[2024-06-27T08:16:32.629+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: OS info Linux, 6.5.0-41-generic, amd64
24/06/27 08:16:32 INFO Executor: Java version 17.0.11
[2024-06-27T08:16:32.653+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-27T08:16:32.655+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7263b31b for default.
[2024-06-27T08:16:32.706+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476190379
[2024-06-27T08:16:32.778+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T08:16:32.793+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:32.824+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T08:16:32.882+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476190379
[2024-06-27T08:16:32.887+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T08:16:32.902+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:32.978+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T08:16:32.984+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476190379
[2024-06-27T08:16:32.987+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T08:16:32.993+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:32.994+0000] {docker.py:436} INFO - 24/06/27 08:16:32 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:16:33.001+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476190379
[2024-06-27T08:16:33.005+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T08:16:33.015+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.016+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:16:33.026+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.027+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T08:16:33.037+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.039+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T08:16:33.045+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476190379
[2024-06-27T08:16:33.047+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T08:16:33.055+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.056+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T08:16:33.064+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476190379
[2024-06-27T08:16:33.072+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T08:16:33.085+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476190379
[2024-06-27T08:16:33.189+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:43439 after 69 ms (0 ms spent in bootstraps)
[2024-06-27T08:16:33.203+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp12664334911144423553.tmp
[2024-06-27T08:16:33.254+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp12664334911144423553.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T08:16:33.262+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.slf4j_slf4j-api-1.7.32.jar to class loader default
[2024-06-27T08:16:33.263+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.266+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp12391573507780348906.tmp
[2024-06-27T08:16:33.273+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp12391573507780348906.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:16:33.284+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T08:16:33.285+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476190379
24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp18335356089895387997.tmp
[2024-06-27T08:16:33.320+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp18335356089895387997.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T08:16:33.330+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader default
[2024-06-27T08:16:33.333+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.342+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp6833623578030183629.tmp
[2024-06-27T08:16:33.363+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp6833623578030183629.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T08:16:33.376+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-27T08:16:33.377+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.384+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp6674103709547800219.tmp
[2024-06-27T08:16:33.386+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp6674103709547800219.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T08:16:33.392+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2024-06-27T08:16:33.393+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.394+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp16591709093372227812.tmp
[2024-06-27T08:16:33.402+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp16591709093372227812.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:16:33.408+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T08:16:33.411+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476190379
[2024-06-27T08:16:33.413+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp13799817111133503190.tmp
[2024-06-27T08:16:33.417+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp13799817111133503190.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T08:16:33.425+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.spark-project.spark_unused-1.0.0.jar to class loader default
[2024-06-27T08:16:33.427+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:33.429+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp1845142084358075060.tmp
[2024-06-27T08:16:33.715+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp1845142084358075060.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T08:16:33.726+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader default
[2024-06-27T08:16:33.728+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476190379
[2024-06-27T08:16:33.728+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp262682339114465762.tmp
[2024-06-27T08:16:33.730+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp262682339114465762.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T08:16:33.740+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/commons-logging_commons-logging-1.1.3.jar to class loader default
[2024-06-27T08:16:33.741+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476190379
[2024-06-27T08:16:33.742+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp5315150775856852741.tmp
[2024-06-27T08:16:33.747+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp5315150775856852741.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T08:16:33.761+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-06-27T08:16:33.762+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476190379
24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp11596389200866625779.tmp
[2024-06-27T08:16:33.778+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp11596389200866625779.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T08:16:33.789+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.postgresql_postgresql-42.2.2.jar to class loader default
[2024-06-27T08:16:33.789+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476190379
[2024-06-27T08:16:33.790+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp12777372033547028248.tmp
[2024-06-27T08:16:33.853+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp12777372033547028248.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T08:16:33.872+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.kafka_kafka-clients-2.8.1.jar to class loader default
[2024-06-27T08:16:33.874+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Executor: Fetching spark://localhost:43439/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476190379
[2024-06-27T08:16:33.875+0000] {docker.py:436} INFO - 24/06/27 08:16:33 INFO Utils: Fetching spark://localhost:43439/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp14191187645391138378.tmp
[2024-06-27T08:16:34.268+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO Utils: /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/fetchFileTemp14191187645391138378.tmp has been previously copied to /tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T08:16:34.299+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO Executor: Adding file:/tmp/spark-9f48c5ca-d3d3-4b5d-97ee-be4e0b3efc18/userFiles-1d8b5324-4f7e-41ca-8d20-c96aed502801/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader default
[2024-06-27T08:16:34.326+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42373.
[2024-06-27T08:16:34.327+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO NettyBlockTransferService: Server created on localhost:42373
[2024-06-27T08:16:34.330+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-27T08:16:34.355+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 42373, None)
[2024-06-27T08:16:34.363+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO BlockManagerMasterEndpoint: Registering block manager localhost:42373 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 42373, None)
[2024-06-27T08:16:34.375+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 42373, None)
[2024-06-27T08:16:34.385+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 42373, None)
[2024-06-27T08:16:35.513+0000] {docker.py:436} INFO - 2024-06-27 08:16:35,512:create_spark_session:INFO:Spark session created successfully
[2024-06-27T08:16:35.558+0000] {docker.py:436} INFO - 24/06/27 08:16:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-27T08:16:35.569+0000] {docker.py:436} INFO - 24/06/27 08:16:35 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-27T08:16:39.045+0000] {docker.py:436} INFO - 2024-06-27 08:16:39,045:create_initial_dataframe:INFO:Initial dataframe created successfully:
[2024-06-27T08:16:39.579+0000] {docker.py:436} INFO - 24/06/27 08:16:39 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-27T08:16:39.652+0000] {docker.py:436} INFO - 24/06/27 08:16:39 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-27T08:16:39.715+0000] {docker.py:436} INFO - 24/06/27 08:16:39 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8 resolved to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8.
[2024-06-27T08:16:39.716+0000] {docker.py:436} INFO - 24/06/27 08:16:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-27T08:16:39.952+0000] {docker.py:436} INFO - 24/06/27 08:16:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/metadata using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/.metadata.7be2a3fa-9978-463a-ad1f-048a3c0f741a.tmp
[2024-06-27T08:16:40.198+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/.metadata.7be2a3fa-9978-463a-ad1f-048a3c0f741a.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/metadata
[2024-06-27T08:16:40.306+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO MicroBatchExecution: Starting [id = ff2e7b05-3eab-441e-93ac-d86a015b7fdc, runId = 19dcc9a1-8465-4b5c-a816-90d091d5a33c]. Use file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8 to store the query checkpoint.
[2024-06-27T08:16:40.329+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5440e5e1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4e3b2594]
[2024-06-27T08:16:40.391+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T08:16:40.401+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T08:16:40.402+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-27T08:16:40.407+0000] {docker.py:436} INFO - 24/06/27 08:16:40 INFO MicroBatchExecution: Stream started from {}
[2024-06-27T08:16:41.280+0000] {docker.py:436} INFO - 24/06/27 08:16:41 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-27T08:16:41.478+0000] {docker.py:436} INFO - 24/06/27 08:16:41 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
[2024-06-27T08:16:41.479+0000] {docker.py:436} INFO - 24/06/27 08:16:41 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/06/27 08:16:41 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/06/27 08:16:41 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/06/27 08:16:41 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
[2024-06-27T08:16:41.482+0000] {docker.py:436} INFO - 24/06/27 08:16:41 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:16:41.483+0000] {docker.py:436} INFO - 24/06/27 08:16:41 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 08:16:41 INFO AppInfoParser: Kafka startTimeMs: 1719476201478
[2024-06-27T08:16:42.059+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/sources/0/0 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/sources/0/.0.daeed706-0446-4e5b-955b-002a50beeb3f.tmp
[2024-06-27T08:16:42.091+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/sources/0/.0.daeed706-0446-4e5b-955b-002a50beeb3f.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/sources/0/0
[2024-06-27T08:16:42.092+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaMicroBatchStream: Initial offsets: {"store_source_data":{"0":8}}
[2024-06-27T08:16:42.117+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/0 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.0.6423e901-b5fc-4de5-a466-094618fa16cb.tmp
[2024-06-27T08:16:42.172+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.0.6423e901-b5fc-4de5-a466-094618fa16cb.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/0
[2024-06-27T08:16:42.173+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719476202104,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:16:42.602+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:42.693+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:42.757+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:42.759+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:42.818+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:42.821+0000] {docker.py:436} INFO - 24/06/27 08:16:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:43.201+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO CodeGenerator: Code generated in 213.879155 ms
[2024-06-27T08:16:43.322+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:16:43.342+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:16:43.368+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:16:43.368+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:16:43.369+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:16:43.371+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:16:43.378+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:16:43.511+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:16:43.546+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:16:43.549+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:16:43.554+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:16:43.577+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:16:43.578+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-27T08:16:43.679+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:16:43.695+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-06-27T08:16:43.891+0000] {docker.py:436} INFO - 24/06/27 08:16:43 INFO CodeGenerator: Code generated in 77.68023 ms
[2024-06-27T08:16:44.054+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO CodeGenerator: Code generated in 39.060326 ms
[2024-06-27T08:16:44.128+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO CodeGenerator: Code generated in 52.500953 ms
[2024-06-27T08:16:44.173+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:16:44.298+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:16:44.300+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 08:16:44 INFO AppInfoParser: Kafka startTimeMs: 1719476204298
[2024-06-27T08:16:44.303+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:16:44.321+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 8 for partition store_source_data-0
[2024-06-27T08:16:44.336+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:16:44.458+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:16:44.955+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:16:44.960+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 08:16:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:16:45.145+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:16:45.148+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2024-06-27T08:16:45.218+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 228829 bytes result sent to driver
[2024-06-27T08:16:45.287+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1645 ms on localhost (executor driver) (1/1)
[2024-06-27T08:16:45.341+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-27T08:16:45.361+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.963 s
[2024-06-27T08:16:45.387+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:16:45.391+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-06-27T08:16:45.412+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 2.070112 s
[2024-06-27T08:16:45.416+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:16:45.421+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:16:45.429+0000] {docker.py:436} INFO - Batch: 0
[2024-06-27T08:16:45.431+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:16:45.544+0000] {docker.py:436} INFO - 24/06/27 08:16:45 INFO CodeGenerator: Code generated in 12.632171 ms
[2024-06-27T08:16:46.831+0000] {docker.py:436} INFO - 24/06/27 08:16:46 INFO CodeGenerator: Code generated in 9.755438 ms
[2024-06-27T08:16:46.850+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
|{"item_purchased"...|
+--------------------+
only showing top 20 rows
[2024-06-27T08:16:46.850+0000] {docker.py:436} INFO - 24/06/27 08:16:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:16:46.871+0000] {docker.py:436} INFO - 24/06/27 08:16:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/0 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.0.f8fca2b8-40ad-455b-ab94-c8a5dccdcace.tmp
[2024-06-27T08:16:46.909+0000] {docker.py:436} INFO - 24/06/27 08:16:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.0.f8fca2b8-40ad-455b-ab94-c8a5dccdcace.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/0
[2024-06-27T08:16:46.969+0000] {docker.py:436} INFO - 24/06/27 08:16:46 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:16:40.376Z",
  "batchId" : 0,
  "numInputRows" : 44,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 6.735037501913363,
  "durationMs" : {
    "addBatch" : 4120,
    "commitOffsets" : 55,
    "getBatch" : 36,
    "latestOffset" : 1679,
    "queryPlanning" : 515,
    "triggerExecution" : 6528,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : null,
    "endOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "numInputRows" : 44,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 6.735037501913363,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 44
  }
}
[2024-06-27T08:16:54.164+0000] {docker.py:436} INFO - 24/06/27 08:16:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:16:56.954+0000] {docker.py:436} INFO - 24/06/27 08:16:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:06.963+0000] {docker.py:436} INFO - 24/06/27 08:17:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:16.970+0000] {docker.py:436} INFO - 24/06/27 08:17:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:22.964+0000] {docker.py:436} INFO - 24/06/27 08:17:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/1 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.1.606e3e63-271a-45a9-a829-39bd9960cf88.tmp
[2024-06-27T08:17:23.031+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.1.606e3e63-271a-45a9-a829-39bd9960cf88.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/1
[2024-06-27T08:17:23.032+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719476242949,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:17:23.134+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.142+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.225+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.226+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.251+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.255+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.329+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:17:23.331+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:17:23.336+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:17:23.337+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:17:23 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:17:23 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:17:23.337+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:17:23.342+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.344+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.346+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:17:23.347+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:17:23.349+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:17:23.350+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-27T08:17:23.352+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:17:23.354+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-27T08:17:23.387+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 52 for partition store_source_data-0
[2024-06-27T08:17:23.396+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:17:23.896+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:17:23.897+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:17:23.898+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=53, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:17:23.898+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:17:23.899+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2024-06-27T08:17:23.902+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 7164 bytes result sent to driver
[2024-06-27T08:17:23.904+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 553 ms on localhost (executor driver) (1/1)
[2024-06-27T08:17:23.905+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-27T08:17:23.907+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.567 s
[2024-06-27T08:17:23.907+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:17:23.908+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-06-27T08:17:23.908+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.575635 s
[2024-06-27T08:17:23.909+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:17:23.909+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:17:23.910+0000] {docker.py:436} INFO - Batch: 1
[2024-06-27T08:17:23.910+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:17:23.988+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:17:23.990+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:17:24.006+0000] {docker.py:436} INFO - 24/06/27 08:17:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/1 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.1.f1f7ce1c-dfad-466b-846c-cddeb58f432e.tmp
[2024-06-27T08:17:24.136+0000] {docker.py:436} INFO - 24/06/27 08:17:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.1.f1f7ce1c-dfad-466b-846c-cddeb58f432e.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/1
[2024-06-27T08:17:24.141+0000] {docker.py:436} INFO - 24/06/27 08:17:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:17:22.942Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8382229673093042,
  "durationMs" : {
    "addBatch" : 818,
    "commitOffsets" : 146,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 99,
    "triggerExecution" : 1193,
    "walCommit" : 112
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8382229673093042,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:17:34.150+0000] {docker.py:436} INFO - 24/06/27 08:17:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:44.154+0000] {docker.py:436} INFO - 24/06/27 08:17:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:44.853+0000] {docker.py:436} INFO - 24/06/27 08:17:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:17:54.160+0000] {docker.py:436} INFO - 24/06/27 08:17:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:04.165+0000] {docker.py:436} INFO - 24/06/27 08:18:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:14.169+0000] {docker.py:436} INFO - 24/06/27 08:18:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:22.001+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/2 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.2.ba005507-2c9c-41f3-ac37-ee521e8aebd1.tmp
[2024-06-27T08:18:22.090+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.2.ba005507-2c9c-41f3-ac37-ee521e8aebd1.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/2
[2024-06-27T08:18:22.095+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719476301985,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:18:22.185+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.202+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.237+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.239+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.266+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.269+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.289+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:18:22.290+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:18:22.292+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:18:22.292+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:18:22.293+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:18:22.293+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Missing parents: List()
24/06/27 08:18:22 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:18:22.299+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:18:22.308+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:18:22.311+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:18:22.312+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:18:22.313+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:18:22.316+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-06-27T08:18:22.318+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:18:22.319+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-06-27T08:18:22.339+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 53 for partition store_source_data-0
[2024-06-27T08:18:22.344+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:18:22.845+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:18:22.858+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=54, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:18:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:18:22 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2024-06-27T08:18:22.861+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 7164 bytes result sent to driver
[2024-06-27T08:18:22.903+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 584 ms on localhost (executor driver) (1/1)
[2024-06-27T08:18:22.904+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.608 s
[2024-06-27T08:18:22.906+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:18:22.918+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-27T08:18:22.920+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-06-27T08:18:22.924+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.631001 s
[2024-06-27T08:18:22.925+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:18:22.927+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 2
-------------------------------------------
[2024-06-27T08:18:23.042+0000] {docker.py:436} INFO - 24/06/27 08:18:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:18:23.043+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:18:23.066+0000] {docker.py:436} INFO - 24/06/27 08:18:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/2 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.2.420b0c9a-802e-4a3b-84cb-d1d74b93693d.tmp
[2024-06-27T08:18:23.143+0000] {docker.py:436} INFO - 24/06/27 08:18:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.2.420b0c9a-802e-4a3b-84cb-d1d74b93693d.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/2
[2024-06-27T08:18:23.147+0000] {docker.py:436} INFO - 24/06/27 08:18:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:18:21.979Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8583690987124464,
  "durationMs" : {
    "addBatch" : 826,
    "commitOffsets" : 107,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 112,
    "triggerExecution" : 1165,
    "walCommit" : 109
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8583690987124464,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:18:33.148+0000] {docker.py:436} INFO - 24/06/27 08:18:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:43.147+0000] {docker.py:436} INFO - 24/06/27 08:18:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:53.161+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:03.167+0000] {docker.py:436} INFO - 24/06/27 08:19:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:11.071+0000] {docker.py:436} INFO - 24/06/27 08:19:11 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:19:13.175+0000] {docker.py:436} INFO - 24/06/27 08:19:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:23.182+0000] {docker.py:436} INFO - 24/06/27 08:19:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:33.189+0000] {docker.py:436} INFO - 24/06/27 08:19:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:43.189+0000] {docker.py:436} INFO - 24/06/27 08:19:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:53.195+0000] {docker.py:436} INFO - 24/06/27 08:19:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:58.013+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/3 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.3.73573c62-c908-4e26-a619-a360e72fad52.tmp
[2024-06-27T08:19:58.178+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.3.73573c62-c908-4e26-a619-a360e72fad52.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/3
[2024-06-27T08:19:58.181+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719476397978,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:19:58.285+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.333+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.434+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.445+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.501+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.524+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.627+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:19:58.631+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:19:58.637+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:19:58.638+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:19:58.639+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:19:58.649+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:19:58.662+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:19:58.683+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.708+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.710+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:19:58.714+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:19:58.727+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:19:58.732+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-06-27T08:19:58.744+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:19:58.751+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2024-06-27T08:19:58.928+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 54 for partition store_source_data-0
[2024-06-27T08:19:58.941+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:19:59.435+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:19:59.436+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=55, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:19:59.437+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:19:59.437+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2024-06-27T08:19:59.442+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 7164 bytes result sent to driver
[2024-06-27T08:19:59.447+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 703 ms on localhost (executor driver) (1/1)
24/06/27 08:19:59 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-06-27T08:19:59.449+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 0.770 s
[2024-06-27T08:19:59.451+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:19:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/27 08:19:59 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 0.820450 s
24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:19:59.452+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 3
-------------------------------------------
[2024-06-27T08:19:59.499+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:19:59.500+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:19:59.537+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/3 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.3.526db61a-7a62-4319-b84d-b550f978d19d.tmp
[2024-06-27T08:19:59.637+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.3.526db61a-7a62-4319-b84d-b550f978d19d.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/3
[2024-06-27T08:19:59.639+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:19:57.976Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.6020469596628537,
  "durationMs" : {
    "addBatch" : 1153,
    "commitOffsets" : 141,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 163,
    "triggerExecution" : 1661,
    "walCommit" : 200
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.6020469596628537,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:20:09.647+0000] {docker.py:436} INFO - 24/06/27 08:20:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:19.653+0000] {docker.py:436} INFO - 24/06/27 08:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:29.672+0000] {docker.py:436} INFO - 24/06/27 08:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:39.676+0000] {docker.py:436} INFO - 24/06/27 08:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:49.689+0000] {docker.py:436} INFO - 24/06/27 08:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:51.227+0000] {docker.py:436} INFO - 24/06/27 08:20:51 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:20:59.690+0000] {docker.py:436} INFO - 24/06/27 08:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:09.691+0000] {docker.py:436} INFO - 24/06/27 08:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:19.697+0000] {docker.py:436} INFO - 24/06/27 08:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:29.708+0000] {docker.py:436} INFO - 24/06/27 08:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:39.718+0000] {docker.py:436} INFO - 24/06/27 08:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:49.722+0000] {docker.py:436} INFO - 24/06/27 08:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:59.722+0000] {docker.py:436} INFO - 24/06/27 08:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:09.722+0000] {docker.py:436} INFO - 24/06/27 08:22:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:19.725+0000] {docker.py:436} INFO - 24/06/27 08:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:29.732+0000] {docker.py:436} INFO - 24/06/27 08:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:39.735+0000] {docker.py:436} INFO - 24/06/27 08:22:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:49.736+0000] {docker.py:436} INFO - 24/06/27 08:22:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:59.745+0000] {docker.py:436} INFO - 24/06/27 08:22:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:09.755+0000] {docker.py:436} INFO - 24/06/27 08:23:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:19.764+0000] {docker.py:436} INFO - 24/06/27 08:23:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:29.770+0000] {docker.py:436} INFO - 24/06/27 08:23:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:39.780+0000] {docker.py:436} INFO - 24/06/27 08:23:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:49.790+0000] {docker.py:436} INFO - 24/06/27 08:23:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:53.027+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/4 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.4.5e7ac327-63a9-463f-8321-28bd44665fc5.tmp
[2024-06-27T08:23:53.242+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.4.5e7ac327-63a9-463f-8321-28bd44665fc5.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/4
24/06/27 08:23:53 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719476632975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:23:53.297+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.365+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.367+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.445+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.448+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.566+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:23:53.568+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:23:53.568+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:23:53.569+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:23:53 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:23:53 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:23:53.570+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:23:53.575+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.586+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.586+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:23:53.590+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:23:53.596+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:23:53.597+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-06-27T08:23:53.604+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:23:53.609+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-06-27T08:23:53.666+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 55 for partition store_source_data-0
[2024-06-27T08:23:53.670+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:23:54.172+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:23:54.175+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=56, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:23:54.177+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:23:54.178+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2024-06-27T08:23:54.211+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 7164 bytes result sent to driver
[2024-06-27T08:23:54.215+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 610 ms on localhost (executor driver) (1/1)
[2024-06-27T08:23:54.219+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.647 s
[2024-06-27T08:23:54.220+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:23:54.227+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/27 08:23:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/27 08:23:54 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.659744 s
[2024-06-27T08:23:54.228+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:23:54.230+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.235+0000] {docker.py:436} INFO - Batch: 4
[2024-06-27T08:23:54.235+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.320+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:23:54.321+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:23:54.348+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/4 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.4.0aa7ba36-c51b-4993-a643-7ae3d108f691.tmp
[2024-06-27T08:23:54.424+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.4.0aa7ba36-c51b-4993-a643-7ae3d108f691.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/4
[2024-06-27T08:23:54.425+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:23:52.973Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.6925207756232687,
  "durationMs" : {
    "addBatch" : 1019,
    "commitOffsets" : 96,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 66,
    "triggerExecution" : 1444,
    "walCommit" : 260
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.6925207756232687,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:24:02.470+0000] {docker.py:436} INFO - 24/06/27 08:24:02 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:24:04.424+0000] {docker.py:436} INFO - 24/06/27 08:24:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:14.425+0000] {docker.py:436} INFO - 24/06/27 08:24:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:24.429+0000] {docker.py:436} INFO - 24/06/27 08:24:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:34.432+0000] {docker.py:436} INFO - 24/06/27 08:24:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:44.445+0000] {docker.py:436} INFO - 24/06/27 08:24:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:54.445+0000] {docker.py:436} INFO - 24/06/27 08:24:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:04.455+0000] {docker.py:436} INFO - 24/06/27 08:25:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:14.460+0000] {docker.py:436} INFO - 24/06/27 08:25:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:24.468+0000] {docker.py:436} INFO - 24/06/27 08:25:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:34.470+0000] {docker.py:436} INFO - 24/06/27 08:25:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:44.475+0000] {docker.py:436} INFO - 24/06/27 08:25:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:54.476+0000] {docker.py:436} INFO - 24/06/27 08:25:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:04.486+0000] {docker.py:436} INFO - 24/06/27 08:26:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:14.492+0000] {docker.py:436} INFO - 24/06/27 08:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:24.505+0000] {docker.py:436} INFO - 24/06/27 08:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:34.511+0000] {docker.py:436} INFO - 24/06/27 08:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:44.517+0000] {docker.py:436} INFO - 24/06/27 08:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:54.522+0000] {docker.py:436} INFO - 24/06/27 08:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:04.522+0000] {docker.py:436} INFO - 24/06/27 08:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:14.530+0000] {docker.py:436} INFO - 24/06/27 08:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:24.532+0000] {docker.py:436} INFO - 24/06/27 08:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:34.533+0000] {docker.py:436} INFO - 24/06/27 08:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:44.538+0000] {docker.py:436} INFO - 24/06/27 08:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:54.549+0000] {docker.py:436} INFO - 24/06/27 08:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:04.550+0000] {docker.py:436} INFO - 24/06/27 08:28:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:14.550+0000] {docker.py:436} INFO - 24/06/27 08:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:24.553+0000] {docker.py:436} INFO - 24/06/27 08:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:34.556+0000] {docker.py:436} INFO - 24/06/27 08:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:44.559+0000] {docker.py:436} INFO - 24/06/27 08:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:54.567+0000] {docker.py:436} INFO - 24/06/27 08:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:04.572+0000] {docker.py:436} INFO - 24/06/27 08:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:14.578+0000] {docker.py:436} INFO - 24/06/27 08:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:24.583+0000] {docker.py:436} INFO - 24/06/27 08:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:34.592+0000] {docker.py:436} INFO - 24/06/27 08:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:43.962+0000] {docker.py:436} INFO - 24/06/27 08:29:43 INFO Metrics: Metrics scheduler closed
[2024-06-27T08:29:43.963+0000] {docker.py:436} INFO - 24/06/27 08:29:43 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:29:43.963+0000] {docker.py:436} INFO - 24/06/27 08:29:43 INFO Metrics: Metrics reporters closed
[2024-06-27T08:29:43.968+0000] {docker.py:436} INFO - 24/06/27 08:29:43 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-1 unregistered
[2024-06-27T08:29:44.600+0000] {docker.py:436} INFO - 24/06/27 08:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:54.608+0000] {docker.py:436} INFO - 24/06/27 08:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:04.608+0000] {docker.py:436} INFO - 24/06/27 08:30:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:14.619+0000] {docker.py:436} INFO - 24/06/27 08:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:23.667+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/5 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.5.f3ba8db5-a114-4b43-a4f2-0bc1a04acb38.tmp
[2024-06-27T08:30:23.841+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.5.f3ba8db5-a114-4b43-a4f2-0bc1a04acb38.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/5
[2024-06-27T08:30:23.842+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1719477023652,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:30:24.034+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.094+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.205+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.221+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.288+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.347+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.468+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:30:24.481+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:30:24.489+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:30:24.491+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:30:24.504+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:30:24.505+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:30:24.505+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:30:24.525+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:30:24.566+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:30:24.567+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:30:24.593+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:30:24.650+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:30:24 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-06-27T08:30:24.665+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:30:24.667+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-06-27T08:30:24.900+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:30:25.083+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:30:25.110+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:30:25.111+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO AppInfoParser: Kafka startTimeMs: 1719477025082
[2024-06-27T08:30:25.116+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:30:25.121+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 56 for partition store_source_data-0
[2024-06-27T08:30:25.255+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:30:25.347+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:30:25.847+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:30:25.848+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:30:25.849+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=57, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:30:25.852+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:30:25.853+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2024-06-27T08:30:25.875+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 7164 bytes result sent to driver
[2024-06-27T08:30:25.878+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1227 ms on localhost (executor driver) (1/1)
24/06/27 08:30:25 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-06-27T08:30:25.885+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 1.377 s
24/06/27 08:30:25 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:30:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/27 08:30:25 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 1.404438 s
24/06/27 08:30:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:30:25.886+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 5
-------------------------------------------
[2024-06-27T08:30:26.030+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:30:26.036+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:30:26.163+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/5 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.5.076485c0-c03b-4b02-9631-beade7661747.tmp
[2024-06-27T08:30:26.376+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.5.076485c0-c03b-4b02-9631-beade7661747.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/5
[2024-06-27T08:30:26.378+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:30:23.651Z",
  "batchId" : 5,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.36710719530102787,
  "durationMs" : {
    "addBatch" : 1927,
    "commitOffsets" : 326,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 278,
    "triggerExecution" : 2724,
    "walCommit" : 191
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.36710719530102787,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:30:36.387+0000] {docker.py:436} INFO - 24/06/27 08:30:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:46.390+0000] {docker.py:436} INFO - 24/06/27 08:30:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:56.395+0000] {docker.py:436} INFO - 24/06/27 08:30:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:06.405+0000] {docker.py:436} INFO - 24/06/27 08:31:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:13.934+0000] {docker.py:436} INFO - 24/06/27 08:31:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:31:16.408+0000] {docker.py:436} INFO - 24/06/27 08:31:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:23.811+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/6 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.6.a35268e0-d331-4dc3-b564-856981917d75.tmp
[2024-06-27T08:31:23.988+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.6.a35268e0-d331-4dc3-b564-856981917d75.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/6
[2024-06-27T08:31:23.994+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1719477083792,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:31:24.053+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.089+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.225+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.237+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.311+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.380+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.467+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:31:24.469+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:31:24.471+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:31:24.471+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:31:24.473+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:31:24.474+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:31:24.483+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:31:24.488+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.520+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.521+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:31:24.521+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:31:24.531+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:31:24.531+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-06-27T08:31:24.532+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:31:24.532+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2024-06-27T08:31:24.631+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 57 for partition store_source_data-0
[2024-06-27T08:31:24.659+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:31:25.173+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:31:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:31:25.174+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=58, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:31:25.177+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:31:25.178+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2024-06-27T08:31:25.209+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 7164 bytes result sent to driver
[2024-06-27T08:31:25.214+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 684 ms on localhost (executor driver) (1/1)
24/06/27 08:31:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-06-27T08:31:25.224+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.736 s
[2024-06-27T08:31:25.226+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:31:25.228+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-06-27T08:31:25.233+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.763397 s
[2024-06-27T08:31:25.233+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:31:25.234+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:31:25.234+0000] {docker.py:436} INFO - Batch: 6
[2024-06-27T08:31:25.235+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:31:25.292+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:31:25.292+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:31:25.318+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/6 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.6.64ca0a46-c2da-48a4-bc29-842b8046ee8b.tmp
[2024-06-27T08:31:25.374+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.6.64ca0a46-c2da-48a4-bc29-842b8046ee8b.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/6
[2024-06-27T08:31:25.376+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:31:23.789Z",
  "batchId" : 6,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6309148264984227,
  "durationMs" : {
    "addBatch" : 1153,
    "commitOffsets" : 80,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 145,
    "triggerExecution" : 1585,
    "walCommit" : 202
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6309148264984227,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:31:35.390+0000] {docker.py:436} INFO - 24/06/27 08:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:45.390+0000] {docker.py:436} INFO - 24/06/27 08:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:55.391+0000] {docker.py:436} INFO - 24/06/27 08:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:05.398+0000] {docker.py:436} INFO - 24/06/27 08:32:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:13.484+0000] {docker.py:436} INFO - 24/06/27 08:32:13 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:32:15.407+0000] {docker.py:436} INFO - 24/06/27 08:32:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:25.416+0000] {docker.py:436} INFO - 24/06/27 08:32:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:35.420+0000] {docker.py:436} INFO - 24/06/27 08:32:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:45.421+0000] {docker.py:436} INFO - 24/06/27 08:32:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:55.431+0000] {docker.py:436} INFO - 24/06/27 08:32:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:05.435+0000] {docker.py:436} INFO - 24/06/27 08:33:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:15.442+0000] {docker.py:436} INFO - 24/06/27 08:33:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:25.448+0000] {docker.py:436} INFO - 24/06/27 08:33:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:35.455+0000] {docker.py:436} INFO - 24/06/27 08:33:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:45.456+0000] {docker.py:436} INFO - 24/06/27 08:33:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:55.464+0000] {docker.py:436} INFO - 24/06/27 08:33:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:05.466+0000] {docker.py:436} INFO - 24/06/27 08:34:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:15.467+0000] {docker.py:436} INFO - 24/06/27 08:34:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:25.472+0000] {docker.py:436} INFO - 24/06/27 08:34:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:35.480+0000] {docker.py:436} INFO - 24/06/27 08:34:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:45.486+0000] {docker.py:436} INFO - 24/06/27 08:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:55.489+0000] {docker.py:436} INFO - 24/06/27 08:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:05.492+0000] {docker.py:436} INFO - 24/06/27 08:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:15.501+0000] {docker.py:436} INFO - 24/06/27 08:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:25.503+0000] {docker.py:436} INFO - 24/06/27 08:35:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:35.509+0000] {docker.py:436} INFO - 24/06/27 08:35:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:45.509+0000] {docker.py:436} INFO - 24/06/27 08:35:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:55.513+0000] {docker.py:436} INFO - 24/06/27 08:35:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:05.518+0000] {docker.py:436} INFO - 24/06/27 08:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:15.521+0000] {docker.py:436} INFO - 24/06/27 08:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:25.531+0000] {docker.py:436} INFO - 24/06/27 08:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:35.536+0000] {docker.py:436} INFO - 24/06/27 08:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:43.970+0000] {docker.py:436} INFO - 24/06/27 08:36:43 INFO Metrics: Metrics scheduler closed
24/06/27 08:36:43 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:36:43.973+0000] {docker.py:436} INFO - 24/06/27 08:36:43 INFO Metrics: Metrics reporters closed
[2024-06-27T08:36:43.973+0000] {docker.py:436} INFO - 24/06/27 08:36:43 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-2 unregistered
[2024-06-27T08:36:45.544+0000] {docker.py:436} INFO - 24/06/27 08:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:50.198+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/7 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.7.eb3cc6a4-ea90-4ca2-bfb1-c49e65bc3d20.tmp
[2024-06-27T08:36:50.347+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.7.eb3cc6a4-ea90-4ca2-bfb1-c49e65bc3d20.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/7
24/06/27 08:36:50 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1719477410160,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:36:50.467+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.498+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.524+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.537+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.628+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.644+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.716+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:36:50.718+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:36:50.727+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:36:50.727+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:36:50.728+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:36:50.728+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:36:50.731+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:36:50.741+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.773+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.797+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:36:50.797+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:36:50.798+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:36:50.800+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-06-27T08:36:50.811+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:36:50.819+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-06-27T08:36:50.872+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:36:50.905+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:36:50.930+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:36:50.967+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO AppInfoParser: Kafka startTimeMs: 1719477410904
[2024-06-27T08:36:50.981+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:36:50.997+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 58 for partition store_source_data-0
[2024-06-27T08:36:51.111+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:36:51.153+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:36:51.639+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:36:51.644+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=59, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:36:51.656+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:36:51.665+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2024-06-27T08:36:51.669+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 7164 bytes result sent to driver
[2024-06-27T08:36:51.683+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 882 ms on localhost (executor driver) (1/1)
[2024-06-27T08:36:51.690+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.951 s
[2024-06-27T08:36:51.690+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:36:51.693+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-06-27T08:36:51.699+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/06/27 08:36:51 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.976717 s
24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:36:51.699+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 7
-------------------------------------------
[2024-06-27T08:36:51.840+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:36:51.843+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:36:51.863+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/7 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.7.1e813462-7fe9-4eb0-a8d8-d33a9d4303a2.tmp
[2024-06-27T08:36:51.996+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.7.1e813462-7fe9-4eb0-a8d8-d33a9d4303a2.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/7
[2024-06-27T08:36:51.999+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:36:50.143Z",
  "batchId" : 7,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.5393743257820928,
  "durationMs" : {
    "addBatch" : 1339,
    "commitOffsets" : 156,
    "getBatch" : 0,
    "latestOffset" : 17,
    "queryPlanning" : 156,
    "triggerExecution" : 1854,
    "walCommit" : 185
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.5393743257820928,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:37:02.005+0000] {docker.py:436} INFO - 24/06/27 08:37:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:12.010+0000] {docker.py:436} INFO - 24/06/27 08:37:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:22.015+0000] {docker.py:436} INFO - 24/06/27 08:37:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:32.024+0000] {docker.py:436} INFO - 24/06/27 08:37:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:36.913+0000] {docker.py:436} INFO - 24/06/27 08:37:36 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:37:42.024+0000] {docker.py:436} INFO - 24/06/27 08:37:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:52.027+0000] {docker.py:436} INFO - 24/06/27 08:37:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:02.030+0000] {docker.py:436} INFO - 24/06/27 08:38:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:12.041+0000] {docker.py:436} INFO - 24/06/27 08:38:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:22.052+0000] {docker.py:436} INFO - 24/06/27 08:38:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:32.053+0000] {docker.py:436} INFO - 24/06/27 08:38:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:42.058+0000] {docker.py:436} INFO - 24/06/27 08:38:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:52.062+0000] {docker.py:436} INFO - 24/06/27 08:38:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:02.070+0000] {docker.py:436} INFO - 24/06/27 08:39:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:12.076+0000] {docker.py:436} INFO - 24/06/27 08:39:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:22.081+0000] {docker.py:436} INFO - 24/06/27 08:39:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:32.086+0000] {docker.py:436} INFO - 24/06/27 08:39:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:42.091+0000] {docker.py:436} INFO - 24/06/27 08:39:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:52.092+0000] {docker.py:436} INFO - 24/06/27 08:39:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:55.687+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/8 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.8.3428c9bb-6513-481c-b507-610565c01cdf.tmp
[2024-06-27T08:39:56.068+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.8.3428c9bb-6513-481c-b507-610565c01cdf.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/8
24/06/27 08:39:56 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1719477595662,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:39:56.127+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.128+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.156+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.159+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.172+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.176+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.241+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:39:56.242+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:39:56.242+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:39:56.243+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:39:56.244+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:39:56 INFO DAGScheduler: Missing parents: List()
24/06/27 08:39:56 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:39:56.255+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:39:56.256+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:39:56.257+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:39:56.263+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:39:56.264+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:39:56 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-06-27T08:39:56.266+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:39:56.273+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-06-27T08:39:56.334+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 59 for partition store_source_data-0
[2024-06-27T08:39:56.335+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:39:56.844+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:39:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:39:56 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2024-06-27T08:39:56.860+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 7164 bytes result sent to driver
[2024-06-27T08:39:56.861+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 593 ms on localhost (executor driver) (1/1)
[2024-06-27T08:39:56.872+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-06-27T08:39:56.895+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.641 s
24/06/27 08:39:56 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:39:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-06-27T08:39:56.896+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.657605 s
[2024-06-27T08:39:56.898+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:39:56.912+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 8
-------------------------------------------
[2024-06-27T08:39:57.130+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:39:57.132+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:39:57.239+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/8 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.8.d2268a63-e6b9-4be8-881e-4a236318d68f.tmp
[2024-06-27T08:39:57.413+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.8.d2268a63-e6b9-4be8-881e-4a236318d68f.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/8
24/06/27 08:39:57 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:39:55.644Z",
  "batchId" : 8,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.5672149744753262,
  "durationMs" : {
    "addBatch" : 1001,
    "commitOffsets" : 266,
    "getBatch" : 0,
    "latestOffset" : 18,
    "queryPlanning" : 75,
    "triggerExecution" : 1763,
    "walCommit" : 395
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.5672149744753262,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:40:07.421+0000] {docker.py:436} INFO - 24/06/27 08:40:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:17.425+0000] {docker.py:436} INFO - 24/06/27 08:40:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:27.441+0000] {docker.py:436} INFO - 24/06/27 08:40:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:37.444+0000] {docker.py:436} INFO - 24/06/27 08:40:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:45.004+0000] {docker.py:436} INFO - 24/06/27 08:40:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:40:47.453+0000] {docker.py:436} INFO - 24/06/27 08:40:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:57.454+0000] {docker.py:436} INFO - 24/06/27 08:40:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:07.461+0000] {docker.py:436} INFO - 24/06/27 08:41:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:17.462+0000] {docker.py:436} INFO - 24/06/27 08:41:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:27.467+0000] {docker.py:436} INFO - 24/06/27 08:41:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:37.469+0000] {docker.py:436} INFO - 24/06/27 08:41:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:47.470+0000] {docker.py:436} INFO - 24/06/27 08:41:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:57.478+0000] {docker.py:436} INFO - 24/06/27 08:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:07.486+0000] {docker.py:436} INFO - 24/06/27 08:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:17.498+0000] {docker.py:436} INFO - 24/06/27 08:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:19.516+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/9 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.9.b24b122e-d466-403d-bf1e-944c3f2bf382.tmp
[2024-06-27T08:42:19.747+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.9.b24b122e-d466-403d-bf1e-944c3f2bf382.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/9
[2024-06-27T08:42:19.748+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1719477739493,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:19.828+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.835+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.857+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.857+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.914+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.926+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.976+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:19.981+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:19.985+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:42:19.985+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:42:19.986+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:19.987+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:19.988+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:19.994+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:42:20.040+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:42:20.047+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:42:20.057+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
24/06/27 08:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:42:20 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/06/27 08:42:20 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:42:20.066+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2024-06-27T08:42:20.111+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 60 for partition store_source_data-0
[2024-06-27T08:42:20.127+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:20.641+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:20.641+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=61, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:20 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
24/06/27 08:42:20 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 7164 bytes result sent to driver
[2024-06-27T08:42:20.649+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 594 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:20.654+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.663 s
[2024-06-27T08:42:20.661+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:20.661+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-06-27T08:42:20.668+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-06-27T08:42:20.668+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.688780 s
[2024-06-27T08:42:20.698+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:20.703+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 9
[2024-06-27T08:42:20.704+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:20.795+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:42:20.796+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:20.821+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/9 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.9.2e80148d-06d0-4fee-9679-1b80087d3875.tmp
[2024-06-27T08:42:20.885+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.9.2e80148d-06d0-4fee-9679-1b80087d3875.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/9
[2024-06-27T08:42:20.888+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:19.490Z",
  "batchId" : 9,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7168458781362007,
  "durationMs" : {
    "addBatch" : 964,
    "commitOffsets" : 91,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 83,
    "triggerExecution" : 1395,
    "walCommit" : 254
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7168458781362007,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:30.719+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/10 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.10.0f226f32-61a2-43e4-a1cf-555d3c9c291a.tmp
[2024-06-27T08:42:30.862+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.10.0f226f32-61a2-43e4-a1cf-555d3c9c291a.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/10
24/06/27 08:42:30 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1719477750688,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:30.939+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.944+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.973+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.981+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.059+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.078+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.175+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:31.181+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:31.186+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:42:31.192+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:42:31.193+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:31.194+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:31.197+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:31.203+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.250+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.291+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:42:31.292+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:31.300+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:42:31.301+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-06-27T08:42:31.321+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes) 
24/06/27 08:42:31 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2024-06-27T08:42:31.426+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 61 for partition store_source_data-0
[2024-06-27T08:42:31.446+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:31.950+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:31.950+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=62, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:42:31.951+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:42:31.951+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2024-06-27T08:42:31.997+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 7164 bytes result sent to driver
[2024-06-27T08:42:31.998+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 702 ms on localhost (executor driver) (1/1)
24/06/27 08:42:31 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-06-27T08:42:32.004+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.800 s
[2024-06-27T08:42:32.005+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:32.005+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-06-27T08:42:32.006+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.822260 s
[2024-06-27T08:42:32.008+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:32.008+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:32.011+0000] {docker.py:436} INFO - Batch: 10
[2024-06-27T08:42:32.011+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:32.072+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:32.073+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:42:32.128+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/10 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.10.39a63939-6307-4e77-bcba-38ed2d4e2b74.tmp
[2024-06-27T08:42:32.236+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.10.39a63939-6307-4e77-bcba-38ed2d4e2b74.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/10
24/06/27 08:42:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:30.678Z",
  "batchId" : 10,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 62.5,
  "processedRowsPerSecond" : 0.6426735218508998,
  "durationMs" : {
    "addBatch" : 1114,
    "commitOffsets" : 169,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 98,
    "triggerExecution" : 1556,
    "walCommit" : 165
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 62.5,
    "processedRowsPerSecond" : 0.6426735218508998,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:42.238+0000] {docker.py:436} INFO - 24/06/27 08:42:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:52.240+0000] {docker.py:436} INFO - 24/06/27 08:42:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:02.262+0000] {docker.py:436} INFO - 24/06/27 08:43:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:12.260+0000] {docker.py:436} INFO - 24/06/27 08:43:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:15.241+0000] {docker.py:436} INFO - 24/06/27 08:43:15 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:43:15.283+0000] {docker.py:436} INFO - 24/06/27 08:43:15 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:43:22.263+0000] {docker.py:436} INFO - 24/06/27 08:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:32.272+0000] {docker.py:436} INFO - 24/06/27 08:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:42.276+0000] {docker.py:436} INFO - 24/06/27 08:43:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:52.284+0000] {docker.py:436} INFO - 24/06/27 08:43:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:02.288+0000] {docker.py:436} INFO - 24/06/27 08:44:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:12.298+0000] {docker.py:436} INFO - 24/06/27 08:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:22.302+0000] {docker.py:436} INFO - 24/06/27 08:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:32.304+0000] {docker.py:436} INFO - 24/06/27 08:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:42.310+0000] {docker.py:436} INFO - 24/06/27 08:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:52.317+0000] {docker.py:436} INFO - 24/06/27 08:44:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:02.317+0000] {docker.py:436} INFO - 24/06/27 08:45:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:12.316+0000] {docker.py:436} INFO - 24/06/27 08:45:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:22.324+0000] {docker.py:436} INFO - 24/06/27 08:45:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:32.327+0000] {docker.py:436} INFO - 24/06/27 08:45:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:42.341+0000] {docker.py:436} INFO - 24/06/27 08:45:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:48.187+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/11 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.11.6cf4cc53-b7bd-4af4-9935-a4f35acce034.tmp
[2024-06-27T08:45:48.372+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.11.6cf4cc53-b7bd-4af4-9935-a4f35acce034.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/11
[2024-06-27T08:45:48.372+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1719477948147,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:45:48.509+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.510+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.584+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.597+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.701+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.721+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.941+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:45:48.985+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:45:48 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:45:48 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:45:48 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:45:48 INFO DAGScheduler: Missing parents: List()
24/06/27 08:45:48 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:45:49.007+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:45:49.039+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:45:49.095+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
24/06/27 08:45:49 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:45:49.095+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:45:49.118+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-06-27T08:45:49.120+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:45:49.120+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2024-06-27T08:45:49.180+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 62 for partition store_source_data-0
[2024-06-27T08:45:49.219+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:45:49.726+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:45:49.729+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=63, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:45:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:45:49 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2024-06-27T08:45:49.730+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 7164 bytes result sent to driver
[2024-06-27T08:45:49.732+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 633 ms on localhost (executor driver) (1/1)
[2024-06-27T08:45:49.734+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-06-27T08:45:49.736+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.776 s
[2024-06-27T08:45:49.745+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:45:49.750+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-06-27T08:45:49.761+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.799331 s
24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:45:49.762+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 11
-------------------------------------------
[2024-06-27T08:45:49.816+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:45:49.817+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:45:49.871+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/11 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.11.9b6861ff-c14a-4920-bc31-04c35776a4df.tmp
[2024-06-27T08:45:50.058+0000] {docker.py:436} INFO - 24/06/27 08:45:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.11.9b6861ff-c14a-4920-bc31-04c35776a4df.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/11
24/06/27 08:45:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:45:48.146Z",
  "batchId" : 11,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.5243838489774515,
  "durationMs" : {
    "addBatch" : 1289,
    "commitOffsets" : 237,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 158,
    "triggerExecution" : 1907,
    "walCommit" : 221
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.5243838489774515,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:46:00.053+0000] {docker.py:436} INFO - 24/06/27 08:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:10.056+0000] {docker.py:436} INFO - 24/06/27 08:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:12.588+0000] {docker.py:436} INFO - 24/06/27 08:46:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:46:20.070+0000] {docker.py:436} INFO - 24/06/27 08:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:30.074+0000] {docker.py:436} INFO - 24/06/27 08:46:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:40.081+0000] {docker.py:436} INFO - 24/06/27 08:46:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:50.091+0000] {docker.py:436} INFO - 24/06/27 08:46:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:00.101+0000] {docker.py:436} INFO - 24/06/27 08:47:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:10.106+0000] {docker.py:436} INFO - 24/06/27 08:47:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:20.111+0000] {docker.py:436} INFO - 24/06/27 08:47:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:30.112+0000] {docker.py:436} INFO - 24/06/27 08:47:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:40.117+0000] {docker.py:436} INFO - 24/06/27 08:47:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:50.126+0000] {docker.py:436} INFO - 24/06/27 08:47:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:00.128+0000] {docker.py:436} INFO - 24/06/27 08:48:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:10.133+0000] {docker.py:436} INFO - 24/06/27 08:48:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:20.144+0000] {docker.py:436} INFO - 24/06/27 08:48:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:30.149+0000] {docker.py:436} INFO - 24/06/27 08:48:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:40.150+0000] {docker.py:436} INFO - 24/06/27 08:48:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:50.157+0000] {docker.py:436} INFO - 24/06/27 08:48:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:00.161+0000] {docker.py:436} INFO - 24/06/27 08:49:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:10.167+0000] {docker.py:436} INFO - 24/06/27 08:49:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:20.169+0000] {docker.py:436} INFO - 24/06/27 08:49:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:30.177+0000] {docker.py:436} INFO - 24/06/27 08:49:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:40.182+0000] {docker.py:436} INFO - 24/06/27 08:49:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:50.191+0000] {docker.py:436} INFO - 24/06/27 08:49:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:00.195+0000] {docker.py:436} INFO - 24/06/27 08:50:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:10.204+0000] {docker.py:436} INFO - 24/06/27 08:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:20.208+0000] {docker.py:436} INFO - 24/06/27 08:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:30.217+0000] {docker.py:436} INFO - 24/06/27 08:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:40.220+0000] {docker.py:436} INFO - 24/06/27 08:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:50.227+0000] {docker.py:436} INFO - 24/06/27 08:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:00.231+0000] {docker.py:436} INFO - 24/06/27 08:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:08.368+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/12 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.12.172b934b-03d2-4611-bd18-d2da509f400d.tmp
[2024-06-27T08:51:08.682+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/.12.172b934b-03d2-4611-bd18-d2da509f400d.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/offsets/12
24/06/27 08:51:08 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1719478268293,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:51:08.748+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.766+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.829+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.847+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.968+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:09.015+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:09.081+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:51:09.089+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:51:09.091+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:51:09.091+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:51:09.091+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:51:09.092+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:51:09.093+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:51:09.105+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 10.6 KiB, free 434.4 MiB)
[2024-06-27T08:51:09.141+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.4 MiB)
[2024-06-27T08:51:09.141+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:42373 (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:51:09.156+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:51:09.207+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:51:09 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-06-27T08:51:09.224+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11070 bytes)
[2024-06-27T08:51:09.225+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2024-06-27T08:51:09.377+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to offset 63 for partition store_source_data-0
[2024-06-27T08:51:09.434+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:51:09.931+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:51:09.931+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:51:09.932+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor-3, groupId=spark-kafka-source-983dc0b7-3f53-4066-a347-d8d8d7427db1-295679457-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=64, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:51:09.933+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:51:09.934+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2024-06-27T08:51:09.941+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 7164 bytes result sent to driver
[2024-06-27T08:51:09.942+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 730 ms on localhost (executor driver) (1/1)
[2024-06-27T08:51:09.943+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-06-27T08:51:09.972+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.865 s
[2024-06-27T08:51:09.973+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:51:09.974+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-06-27T08:51:09.976+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.887232 s
[2024-06-27T08:51:09.977+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:51:09.978+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:51:09.979+0000] {docker.py:436} INFO - Batch: 12
[2024-06-27T08:51:09.979+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:51:10.182+0000] {docker.py:436} INFO - +--------------------+
|   extracted_content|
+--------------------+
|{"item_purchased"...|
+--------------------+
[2024-06-27T08:51:10.183+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:51:10.271+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/12 using temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.12.b5918141-95d1-423c-81b2-729e06e81fb7.tmp
[2024-06-27T08:51:10.600+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/.12.b5918141-95d1-423c-81b2-729e06e81fb7.tmp to file:/tmp/temporary-2271a3d9-c0e4-4724-a29d-5fca3366caa8/commits/12
[2024-06-27T08:51:10.602+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ff2e7b05-3eab-441e-93ac-d86a015b7fdc",
  "runId" : "19dcc9a1-8465-4b5c-a816-90d091d5a33c",
  "name" : null,
  "timestamp" : "2024-06-27T08:51:08.287Z",
  "batchId" : 12,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 62.5,
  "processedRowsPerSecond" : 0.43308791684711995,
  "durationMs" : {
    "addBatch" : 1415,
    "commitOffsets" : 414,
    "getBatch" : 1,
    "latestOffset" : 6,
    "queryPlanning" : 96,
    "triggerExecution" : 2309,
    "walCommit" : 376
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 62.5,
    "processedRowsPerSecond" : 0.43308791684711995,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@18ab567",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:51:20.610+0000] {docker.py:436} INFO - 24/06/27 08:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:30.610+0000] {docker.py:436} INFO - 24/06/27 08:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:40.617+0000] {docker.py:436} INFO - 24/06/27 08:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:50.617+0000] {docker.py:436} INFO - 24/06/27 08:51:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:55.162+0000] {docker.py:436} INFO - 24/06/27 08:51:55 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:42373 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2024-06-27T08:52:00.631+0000] {docker.py:436} INFO - 24/06/27 08:52:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:10.638+0000] {docker.py:436} INFO - 24/06/27 08:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:20.640+0000] {docker.py:436} INFO - 24/06/27 08:52:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:30.641+0000] {docker.py:436} INFO - 24/06/27 08:52:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:40.652+0000] {docker.py:436} INFO - 24/06/27 08:52:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:50.659+0000] {docker.py:436} INFO - 24/06/27 08:52:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:00.660+0000] {docker.py:436} INFO - 24/06/27 08:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:10.669+0000] {docker.py:436} INFO - 24/06/27 08:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:20.669+0000] {docker.py:436} INFO - 24/06/27 08:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:30.677+0000] {docker.py:436} INFO - 24/06/27 08:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:36.457+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:36.596+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T08:53:36.615+0000] {process_utils.py:132} INFO - Sending 15 to group 1270. PIDs of all processes in the group: [1270]
[2024-06-27T08:53:36.643+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 1270
[2024-06-27T08:53:36.658+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-06-27T08:53:36.659+0000] {docker.py:528} INFO - Stopping docker container
[2024-06-27T08:53:36.687+0000] {logging_mixin.py:188} WARNING - --- Logging error ---
[2024-06-27T08:53:36.913+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.914+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/client.py", line 265, in _raise_for_status
    response.raise_for_status()
[2024-06-27T08:53:36.915+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
[2024-06-27T08:53:36.924+0000] {logging_mixin.py:188} WARNING - requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.45/containers/create
[2024-06-27T08:53:36.925+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:36.925+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.926+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 371, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.927+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 398, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.927+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/container.py", line 439, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.928+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/container.py", line 456, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.928+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/client.py", line 271, in _result
    self._raise_for_status(response)
[2024-06-27T08:53:36.928+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/client.py", line 267, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.930+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
[2024-06-27T08:53:36.932+0000] {logging_mixin.py:188} WARNING - docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.45/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/***tmp1m4nrpz0")
[2024-06-27T08:53:36.933+0000] {logging_mixin.py:188} WARNING - 
During handling of the above exception, another exception occurred:
[2024-06-27T08:53:36.933+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.933+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.934+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
[2024-06-27T08:53:36.934+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
[2024-06-27T08:53:36.935+0000] {logging_mixin.py:188} WARNING - ConnectionRefusedError: [Errno 111] Connection refused
[2024-06-27T08:53:36.935+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:36.936+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.937+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.937+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
[2024-06-27T08:53:36.938+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
[2024-06-27T08:53:36.938+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
[2024-06-27T08:53:36.938+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
[2024-06-27T08:53:36.939+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
[2024-06-27T08:53:36.939+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.939+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
[2024-06-27T08:53:36.940+0000] {logging_mixin.py:188} WARNING - urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26dfe600>: Failed to establish a new connection: [Errno 111] Connection refused
[2024-06-27T08:53:36.940+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:36.941+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.941+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
[2024-06-27T08:53:36.942+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.942+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.943+0000] {logging_mixin.py:188} WARNING - urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26dfe600>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:36.943+0000] {logging_mixin.py:188} WARNING - 
During handling of the above exception, another exception occurred:
[2024-06-27T08:53:36.944+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.945+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 438, in _run_image_with_mounts
    result = self.cli.wait(self.container["Id"])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.945+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.946+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/container.py", line 1346, in wait
    res = self._post(url, timeout=timeout, params=params)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.946+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.946+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.947+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.947+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.948+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.948+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
[2024-06-27T08:53:36.948+0000] {logging_mixin.py:188} WARNING - requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26dfe600>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:36.949+0000] {logging_mixin.py:188} WARNING - 
During handling of the above exception, another exception occurred:
[2024-06-27T08:53:36.949+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.950+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.950+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
[2024-06-27T08:53:36.956+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
[2024-06-27T08:53:36.956+0000] {logging_mixin.py:188} WARNING - ConnectionRefusedError: [Errno 111] Connection refused
[2024-06-27T08:53:36.956+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:36.957+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.958+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.958+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
[2024-06-27T08:53:36.965+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
[2024-06-27T08:53:36.966+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
[2024-06-27T08:53:36.966+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
[2024-06-27T08:53:36.967+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
[2024-06-27T08:53:36.969+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.970+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
[2024-06-27T08:53:36.971+0000] {logging_mixin.py:188} WARNING - urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c68680>: Failed to establish a new connection: [Errno 111] Connection refused
[2024-06-27T08:53:36.973+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:36.973+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.973+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
[2024-06-27T08:53:36.974+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.974+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.975+0000] {logging_mixin.py:188} WARNING - urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c68680>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:36.975+0000] {logging_mixin.py:188} WARNING - 
During handling of the above exception, another exception occurred:
[2024-06-27T08:53:36.975+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.977+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
[2024-06-27T08:53:36.978+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.978+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.979+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.979+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.980+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.980+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 509, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.981+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 380, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.982+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 462, in _run_image_with_mounts
    self.cli.remove_container(self.container["Id"])
[2024-06-27T08:53:36.985+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.985+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/container.py", line 1033, in remove_container
    res = self._delete(
          ^^^^^^^^^^^^^
[2024-06-27T08:53:36.986+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.986+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/client.py", line 244, in _delete
    return self.delete(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.987+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 671, in delete
    return self.request("DELETE", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.987+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.987+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.988+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
[2024-06-27T08:53:36.988+0000] {logging_mixin.py:188} WARNING - requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c68680>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:36.990+0000] {logging_mixin.py:188} WARNING - 
During handling of the above exception, another exception occurred:
[2024-06-27T08:53:36.990+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.990+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.991+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
[2024-06-27T08:53:36.991+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
[2024-06-27T08:53:36.992+0000] {logging_mixin.py:188} WARNING - ConnectionRefusedError: [Errno 111] Connection refused
[2024-06-27T08:53:36.992+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:36.993+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:36.994+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:36.995+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
[2024-06-27T08:53:36.998+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
[2024-06-27T08:53:37.000+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
[2024-06-27T08:53:37.001+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
[2024-06-27T08:53:37.002+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
[2024-06-27T08:53:37.002+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.003+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
[2024-06-27T08:53:37.003+0000] {logging_mixin.py:188} WARNING - urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c69160>: Failed to establish a new connection: [Errno 111] Connection refused
[2024-06-27T08:53:37.004+0000] {logging_mixin.py:188} WARNING - 
The above exception was the direct cause of the following exception:
[2024-06-27T08:53:37.004+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:37.008+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
[2024-06-27T08:53:37.009+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.009+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.010+0000] {logging_mixin.py:188} WARNING - urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c69160>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:37.010+0000] {logging_mixin.py:188} WARNING - 
During handling of the above exception, another exception occurred:
[2024-06-27T08:53:37.013+0000] {logging_mixin.py:188} WARNING - Traceback (most recent call last):
[2024-06-27T08:53:37.014+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 1160, in emit
    msg = self.format(record)
          ^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.015+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 999, in format
    return fmt.format(record)
           ^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.016+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 711, in format
    record.exc_text = self.formatException(record.exc_info)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.017+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 661, in formatException
    traceback.print_exception(ei[0], ei[1], tb, None, sio)
[2024-06-27T08:53:37.017+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/traceback.py", line 124, in print_exception
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.017+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/traceback.py", line 733, in __init__
    self.stack = StackSummary._extract_from_extended_frame_gen(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.018+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/traceback.py", line 430, in _extract_from_extended_frame_gen
    result.append(FrameSummary(
                  ^^^^^^^^^^^^^
[2024-06-27T08:53:37.018+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/traceback.py", line 265, in __init__
    def __init__(self, filename, lineno, name, *, lookup_line=True,
[2024-06-27T08:53:37.018+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 2612, in signal_handler
    self.task.on_kill()
[2024-06-27T08:53:37.019+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/providers/docker/operators/docker.py", line 532, in on_kill
    self.cli.stop(self.container["Id"])
[2024-06-27T08:53:37.019+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.020+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/container.py", line 1210, in stop
    res = self._post(url, params=params, timeout=conn_timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.020+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.058+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.059+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.061+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.061+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-06-27T08:53:37.062+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
[2024-06-27T08:53:37.068+0000] {logging_mixin.py:188} WARNING - requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c69160>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:37.070+0000] {logging_mixin.py:188} WARNING - Call stack:
[2024-06-27T08:53:37.175+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/bin/***", line 8, in <module>
    sys.exit(main())
[2024-06-27T08:53:37.175+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/__main__.py", line 58, in main
    args.func(args)
[2024-06-27T08:53:37.176+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
[2024-06-27T08:53:37.176+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
[2024-06-27T08:53:37.177+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
[2024-06-27T08:53:37.177+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/scheduler_command.py", line 58, in scheduler
    run_command_with_daemon_option(
[2024-06-27T08:53:37.177+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/daemon_utils.py", line 85, in run_command_with_daemon_option
    callback()
[2024-06-27T08:53:37.178+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/scheduler_command.py", line 61, in <lambda>
    callback=lambda: _run_scheduler_job(args),
[2024-06-27T08:53:37.178+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/scheduler_command.py", line 49, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
[2024-06-27T08:53:37.179+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
[2024-06-27T08:53:37.179+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
[2024-06-27T08:53:37.179+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
[2024-06-27T08:53:37.180+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/jobs/scheduler_job_runner.py", line 834, in _execute
    self.job.executor.start()
[2024-06-27T08:53:37.180+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/executors/local_executor.py", line 373, in start
    self.impl.start()
[2024-06-27T08:53:37.180+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/executors/local_executor.py", line 312, in start
    worker.start()
[2024-06-27T08:53:37.181+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
[2024-06-27T08:53:37.181+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
[2024-06-27T08:53:37.182+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
[2024-06-27T08:53:37.182+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
[2024-06-27T08:53:37.182+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
[2024-06-27T08:53:37.183+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
[2024-06-27T08:53:37.183+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/executors/local_executor.py", line 78, in run
    return super().run()
[2024-06-27T08:53:37.183+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
[2024-06-27T08:53:37.184+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/executors/local_executor.py", line 202, in do_work
    self.execute_work(key=key, command=command)
[2024-06-27T08:53:37.184+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/executors/local_executor.py", line 95, in execute_work
    state = self._execute_work_in_fork(command)
[2024-06-27T08:53:37.194+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/executors/local_executor.py", line 135, in _execute_work_in_fork
    args.func(args)
[2024-06-27T08:53:37.194+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
[2024-06-27T08:53:37.195+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
[2024-06-27T08:53:37.195+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/task_command.py", line 441, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
[2024-06-27T08:53:37.195+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/task_command.py", line 219, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
[2024-06-27T08:53:37.196+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/task_command.py", line 281, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
[2024-06-27T08:53:37.196+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
[2024-06-27T08:53:37.197+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/jobs/job.py", line 402, in run_job
    return execute_job(job, execute_callable=execute_callable)
[2024-06-27T08:53:37.197+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/jobs/job.py", line 431, in execute_job
    ret = execute_callable()
[2024-06-27T08:53:37.197+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/jobs/local_task_job_runner.py", line 168, in _execute
    self.task_runner.start()
[2024-06-27T08:53:37.198+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py", line 51, in start
    self.process = self._start_by_fork()
[2024-06-27T08:53:37.198+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py", line 103, in _start_by_fork
    ret = args.func(args, dag=self.dag)
[2024-06-27T08:53:37.198+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
[2024-06-27T08:53:37.199+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
[2024-06-27T08:53:37.199+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/task_command.py", line 441, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
[2024-06-27T08:53:37.200+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/task_command.py", line 221, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
[2024-06-27T08:53:37.200+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/cli/commands/task_command.py", line 300, in _run_raw_task
    return ti._run_raw_task(
[2024-06-27T08:53:37.201+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
[2024-06-27T08:53:37.202+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 2545, in _run_raw_task
    self.handle_failure(e, test_mode, context, session=session)
[2024-06-27T08:53:37.202+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
[2024-06-27T08:53:37.202+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 3003, in handle_failure
    _handle_failure(
[2024-06-27T08:53:37.203+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 861, in _handle_failure
    failure_context = TaskInstance.fetch_handle_failure_context(
[2024-06-27T08:53:37.203+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
[2024-06-27T08:53:37.204+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
[2024-06-27T08:53:37.205+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/models/taskinstance.py", line 2905, in fetch_handle_failure_context
    cls.logger().error("Task failed with exception", exc_info=(type(error), error, tb))
[2024-06-27T08:53:37.206+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 1568, in error
    self._log(ERROR, msg, args, **kwargs)
[2024-06-27T08:53:37.207+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 1684, in _log
    self.handle(record)
[2024-06-27T08:53:37.207+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 1700, in handle
    self.callHandlers(record)
[2024-06-27T08:53:37.207+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 1762, in callHandlers
    hdlr.handle(record)
[2024-06-27T08:53:37.208+0000] {logging_mixin.py:188} WARNING -   File "/usr/local/lib/python3.12/logging/__init__.py", line 1028, in handle
    self.emit(record)
[2024-06-27T08:53:37.208+0000] {logging_mixin.py:188} WARNING -   File "/home/***/.local/lib/python3.12/site-packages/***/utils/log/file_task_handler.py", line 248, in emit
    self.handler.emit(record)
[2024-06-27T08:53:37.209+0000] {logging_mixin.py:188} WARNING - Message: 'Task failed with exception'
Arguments: ()
[2024-06-27T08:53:37.324+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=Stage_1, task_id=run_bronze_job, run_id=manual__2024-06-27T08:15:59.473837+00:00, execution_date=20240627T081559, start_date=20240627T081608, end_date=20240627T085337
[2024-06-27T08:53:37.420+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 256 for task run_bronze_job (HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/486385f17fedbe438435cbc502af007ee2178a181f6ca85e24c18dc6ec40e50c?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c68680>: Failed to establish a new connection: [Errno 111] Connection refused')); 1270)
[2024-06-27T08:53:37.484+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=1270, status='terminated', exitcode=1, started='08:16:07') (1270) terminated with exit code 1
[2024-06-27T08:53:37.487+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 143
[2024-06-27T08:53:37.539+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T08:53:37.696+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
