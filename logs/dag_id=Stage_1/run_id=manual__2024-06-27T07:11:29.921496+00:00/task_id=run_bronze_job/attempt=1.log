[2024-06-27T07:11:35.384+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T07:11:35.423+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:11:29.921496+00:00 [queued]>
[2024-06-27T07:11:35.436+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:11:29.921496+00:00 [queued]>
[2024-06-27T07:11:35.436+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-27T07:11:35.451+0000] {taskinstance.py:2330} INFO - Executing <Task(DockerOperator): run_bronze_job> on 2024-06-27 07:11:29.921496+00:00
[2024-06-27T07:11:35.458+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=343) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-27T07:11:35.459+0000] {standard_task_runner.py:63} INFO - Started process 344 to run task
[2024-06-27T07:11:35.458+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Stage_1', 'run_bronze_job', 'manual__2024-06-27T07:11:29.921496+00:00', '--job-id', '126', '--raw', '--subdir', 'DAGS_FOLDER/kafka_dag.py', '--cfg-path', '/tmp/tmpj4a2snzo']
[2024-06-27T07:11:35.460+0000] {standard_task_runner.py:91} INFO - Job 126: Subtask run_bronze_job
[2024-06-27T07:11:35.473+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-06-27T07:11:35.505+0000] {task_command.py:426} INFO - Running <TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:11:29.921496+00:00 [running]> on host 08bfa8b73cac
[2024-06-27T07:11:35.591+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Stage_1' AIRFLOW_CTX_TASK_ID='run_bronze_job' AIRFLOW_CTX_EXECUTION_DATE='2024-06-27T07:11:29.921496+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-27T07:11:29.921496+00:00'
[2024-06-27T07:11:35.593+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T07:11:35.624+0000] {docker.py:366} INFO - Starting docker container from image bitnami/spark:latest
[2024-06-27T07:11:35.627+0000] {docker.py:374} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-06-27T07:11:36.031+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:11:36.03 [0m[38;5;2mINFO [0m ==>
[2024-06-27T07:11:36.032+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:11:36.03 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-27T07:11:36.034+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:11:36.03 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-27T07:11:36.035+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:11:36.03 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-27T07:11:36.037+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:11:36.03 [0m[38;5;2mINFO [0m ==> Upgrade to Tanzu Application Catalog for production environments to access custom-configured and pre-packaged software components. Gain enhanced features, including Software Bill of Materials (SBOM), CVE scan result reports, and VEX documents. To learn more, visit [1mhttps://bitnami.com/enterprise[0m
[2024-06-27T07:11:36.038+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:11:36.03 [0m[38;5;2mINFO [0m ==>
[2024-06-27T07:11:36.046+0000] {docker.py:436} INFO - 
[2024-06-27T07:11:38.211+0000] {docker.py:436} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-27T07:11:38.298+0000] {docker.py:436} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-27T07:11:38.305+0000] {docker.py:436} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-06-27T07:11:38.306+0000] {docker.py:436} INFO - org.postgresql#postgresql added as a dependency
[2024-06-27T07:11:38.307+0000] {docker.py:436} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-8ec1bad1-bbea-454b-bfb5-514dbb353916;1.0
	confs: [default]
[2024-06-27T07:11:39.367+0000] {docker.py:436} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T07:11:39.798+0000] {docker.py:436} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T07:11:39.856+0000] {docker.py:436} INFO - found org.apache.kafka#kafka-clients;2.8.1 in central
[2024-06-27T07:11:39.924+0000] {docker.py:436} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-27T07:11:39.980+0000] {docker.py:436} INFO - found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2024-06-27T07:11:40.211+0000] {docker.py:436} INFO - found org.slf4j#slf4j-api;1.7.32 in central
[2024-06-27T07:11:40.719+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2024-06-27T07:11:40.833+0000] {docker.py:436} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2024-06-27T07:11:40.988+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2024-06-27T07:11:41.180+0000] {docker.py:436} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-27T07:11:41.216+0000] {docker.py:436} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-27T07:11:41.620+0000] {docker.py:436} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-27T07:11:43.120+0000] {docker.py:436} INFO - found org.postgresql#postgresql;42.2.2 in central
[2024-06-27T07:11:43.140+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T07:11:43.196+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (64ms)
[2024-06-27T07:11:43.207+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar ...
[2024-06-27T07:11:43.312+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.2.2!postgresql.jar(bundle) (116ms)
[2024-06-27T07:11:43.324+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T07:11:43.353+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (39ms)
[2024-06-27T07:11:43.385+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...
[2024-06-27T07:11:43.903+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (548ms)
[2024-06-27T07:11:43.921+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-27T07:11:43.933+0000] {docker.py:436} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (29ms)
[2024-06-27T07:11:43.949+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-27T07:11:43.976+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (39ms)
[2024-06-27T07:11:43.985+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[2024-06-27T07:11:43.995+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (17ms)
[2024-06-27T07:11:44.008+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...
[2024-06-27T07:11:47.367+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (3371ms)
[2024-06-27T07:11:47.379+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-27T07:11:47.473+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (104ms)
[2024-06-27T07:11:47.495+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...
[2024-06-27T07:11:47.706+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (229ms)
[2024-06-27T07:11:47.723+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...
[2024-06-27T07:11:47.743+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (31ms)
[2024-06-27T07:11:47.769+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...
[2024-06-27T07:11:49.965+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (2206ms)
[2024-06-27T07:11:49.982+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-27T07:11:50.044+0000] {docker.py:436} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (77ms)
[2024-06-27T07:11:50.045+0000] {docker.py:436} INFO - :: resolution report :: resolve 4824ms :: artifacts dl 6913ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.2.2 from central in [default]
[2024-06-27T07:11:50.046+0000] {docker.py:436} INFO - org.slf4j#slf4j-api;1.7.32 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2024-06-27T07:11:50.046+0000] {docker.py:436} INFO - |      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-27T07:11:50.055+0000] {docker.py:436} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-8ec1bad1-bbea-454b-bfb5-514dbb353916
[2024-06-27T07:11:50.056+0000] {docker.py:436} INFO - confs: [default]
[2024-06-27T07:11:50.190+0000] {docker.py:436} INFO - 13 artifacts copied, 0 already retrieved (57403kB/135ms)
[2024-06-27T07:11:50.663+0000] {docker.py:436} INFO - 24/06/27 07:11:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-27T07:11:52.379+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SparkContext: Running Spark version 3.5.1
[2024-06-27T07:11:52.380+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SparkContext: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T07:11:52.381+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SparkContext: Java version 17.0.11
[2024-06-27T07:11:52.410+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO ResourceUtils: ==============================================================
[2024-06-27T07:11:52.411+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-27T07:11:52.412+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO ResourceUtils: ==============================================================
[2024-06-27T07:11:52.413+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-27T07:11:52.443+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-27T07:11:52.452+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO ResourceProfile: Limiting resource is cpu
[2024-06-27T07:11:52.453+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-27T07:11:52.521+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SecurityManager: Changing view acls to: spark
[2024-06-27T07:11:52.522+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SecurityManager: Changing modify acls to: spark
[2024-06-27T07:11:52.523+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SecurityManager: Changing view acls groups to:
[2024-06-27T07:11:52.524+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SecurityManager: Changing modify acls groups to:
[2024-06-27T07:11:52.527+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-27T07:11:52.887+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO Utils: Successfully started service 'sparkDriver' on port 37837.
[2024-06-27T07:11:52.937+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SparkEnv: Registering MapOutputTracker
[2024-06-27T07:11:52.999+0000] {docker.py:436} INFO - 24/06/27 07:11:52 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-27T07:11:53.032+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-27T07:11:53.033+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-27T07:11:53.041+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-27T07:11:53.067+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6ef1016c-cc8e-4d4b-a271-8bbfd82056ee
[2024-06-27T07:11:53.086+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-27T07:11:53.111+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-27T07:11:53.316+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-27T07:11:53.384+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-27T07:11:53.438+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://localhost:37837/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.439+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at spark://localhost:37837/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://localhost:37837/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://localhost:37837/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472312370
[2024-06-27T07:11:53.440+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:37837/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:37837/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472312370
[2024-06-27T07:11:53.440+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://localhost:37837/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.441+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://localhost:37837/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:37837/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://localhost:37837/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://localhost:37837/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472312370
[2024-06-27T07:11:53.443+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://localhost:37837/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:37837/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472312370
[2024-06-27T07:11:53.447+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.449+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:11:53.459+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472312370
[2024-06-27T07:11:53.460+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:11:53.465+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.466+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:11:53.471+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472312370
[2024-06-27T07:11:53.473+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:11:53.481+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.482+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:11:53.486+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472312370
[2024-06-27T07:11:53.487+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:11:53.494+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.494+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:11:53.499+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472312370
[2024-06-27T07:11:53.500+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:11:53.549+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.550+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:11:53.559+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472312370
[2024-06-27T07:11:53.559+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:11:53.567+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472312370
[2024-06-27T07:11:53.568+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:11:53.573+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472312370
[2024-06-27T07:11:53.573+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:11:53.594+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472312370
[2024-06-27T07:11:53.595+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:11:53.697+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Starting executor ID driver on host localhost
[2024-06-27T07:11:53.698+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T07:11:53.699+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Java version 17.0.11
[2024-06-27T07:11:53.709+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-27T07:11:53.710+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1ed70cc7 for default.
[2024-06-27T07:11:53.733+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472312370
[2024-06-27T07:11:53.759+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:11:53.763+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472312370
[2024-06-27T07:11:53.784+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:11:53.788+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472312370
[2024-06-27T07:11:53.791+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:11:53.795+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472312370
[2024-06-27T07:11:53.823+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:11:53.827+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472312370
[2024-06-27T07:11:53.828+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:11:53.831+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.832+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:11:53.835+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472312370
[2024-06-27T07:11:53.836+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:11:53.839+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.840+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:11:53.843+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.844+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:11:53.847+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.848+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:11:53.852+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472312370
[2024-06-27T07:11:53.852+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:11:53.855+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472312370
[2024-06-27T07:11:53.856+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:11:53.859+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472312370
[2024-06-27T07:11:53.865+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:11:53.872+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching spark://localhost:37837/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472312370
[2024-06-27T07:11:53.918+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:37837 after 34 ms (0 ms spent in bootstraps)
[2024-06-27T07:11:53.926+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Fetching spark://localhost:37837/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp672800490119150588.tmp
[2024-06-27T07:11:53.969+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp672800490119150588.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:11:53.984+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader default
[2024-06-27T07:11:53.985+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Fetching spark://localhost:37837/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472312370
[2024-06-27T07:11:53.986+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: Fetching spark://localhost:37837/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp1531725155465821578.tmp
[2024-06-27T07:11:53.991+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp1531725155465821578.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:11:54.000+0000] {docker.py:436} INFO - 24/06/27 07:11:53 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/27 07:11:53 INFO Executor: Fetching spark://localhost:37837/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472312370
24/06/27 07:11:53 INFO Utils: Fetching spark://localhost:37837/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp231761271351543363.tmp
[2024-06-27T07:11:54.004+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp231761271351543363.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:11:54.010+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.spark-project.spark_unused-1.0.0.jar to class loader default
[2024-06-27T07:11:54.011+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472312370
[2024-06-27T07:11:54.014+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp8932978792253799591.tmp
[2024-06-27T07:11:54.017+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp8932978792253799591.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:11:54.021+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.slf4j_slf4j-api-1.7.32.jar to class loader default
[2024-06-27T07:11:54.022+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:54.023+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp4834936355928197762.tmp
[2024-06-27T07:11:54.026+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp4834936355928197762.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:11:54.030+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T07:11:54.031+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472312370
[2024-06-27T07:11:54.031+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp758575888022636739.tmp
[2024-06-27T07:11:54.036+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp758575888022636739.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:11:54.040+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.postgresql_postgresql-42.2.2.jar to class loader default
[2024-06-27T07:11:54.041+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472312370
[2024-06-27T07:11:54.041+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp12748436828875437692.tmp
[2024-06-27T07:11:54.048+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp12748436828875437692.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:11:54.053+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-27T07:11:54.054+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472312370
[2024-06-27T07:11:54.056+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp16387998279431195128.tmp
[2024-06-27T07:11:54.058+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp16387998279431195128.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:11:54.064+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2024-06-27T07:11:54.065+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472312370
[2024-06-27T07:11:54.065+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp8001952856016731477.tmp
[2024-06-27T07:11:54.072+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp8001952856016731477.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:11:54.078+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T07:11:54.078+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472312370
[2024-06-27T07:11:54.079+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp2816208095317706351.tmp
[2024-06-27T07:11:54.291+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp2816208095317706351.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:11:54.299+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader default
[2024-06-27T07:11:54.300+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472312370
[2024-06-27T07:11:54.300+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp7550571500419390294.tmp
[2024-06-27T07:11:54.406+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp7550571500419390294.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:11:54.412+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader default
[2024-06-27T07:11:54.413+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472312370
[2024-06-27T07:11:54.414+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp10614950222265009681.tmp
[2024-06-27T07:11:54.431+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp10614950222265009681.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:11:54.437+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.kafka_kafka-clients-2.8.1.jar to class loader default
[2024-06-27T07:11:54.438+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Fetching spark://localhost:37837/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472312370
[2024-06-27T07:11:54.438+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Fetching spark://localhost:37837/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp6284650270214672088.tmp
[2024-06-27T07:11:54.440+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/fetchFileTemp6284650270214672088.tmp has been previously copied to /tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:11:54.445+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Executor: Adding file:/tmp/spark-b0bea354-4c60-4436-b295-358472aa1ef9/userFiles-5638ae7e-6b28-4c2f-ab2d-c20c049b21e9/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-06-27T07:11:54.455+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35905.
[2024-06-27T07:11:54.455+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO NettyBlockTransferService: Server created on localhost:35905
[2024-06-27T07:11:54.457+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-27T07:11:54.464+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 35905, None)
[2024-06-27T07:11:54.469+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO BlockManagerMasterEndpoint: Registering block manager localhost:35905 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 35905, None)
[2024-06-27T07:11:54.475+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 35905, None)
[2024-06-27T07:11:54.476+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 35905, None)
[2024-06-27T07:11:54.910+0000] {docker.py:436} INFO - 2024-06-27 07:11:54,910:create_spark_session:INFO:Spark session created successfully
[2024-06-27T07:11:54.919+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-27T07:11:54.923+0000] {docker.py:436} INFO - 24/06/27 07:11:54 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-27T07:11:56.596+0000] {docker.py:436} INFO - 2024-06-27 07:11:56,595:create_initial_dataframe:INFO:Initial dataframe created successfully:
[2024-06-27T07:11:58.256+0000] {docker.py:436} INFO - 24/06/27 07:11:58 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-27T07:11:58.644+0000] {docker.py:436} INFO - 24/06/27 07:11:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-27T07:11:58.674+0000] {docker.py:436} INFO - 24/06/27 07:11:58 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521 resolved to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521.
[2024-06-27T07:11:58.674+0000] {docker.py:436} INFO - 24/06/27 07:11:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-27T07:11:58.773+0000] {docker.py:436} INFO - 24/06/27 07:11:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/metadata using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/.metadata.b8fcae02-3cb1-40f3-8348-6584d2ae1675.tmp
[2024-06-27T07:11:58.881+0000] {docker.py:436} INFO - 24/06/27 07:11:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/.metadata.b8fcae02-3cb1-40f3-8348-6584d2ae1675.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/metadata
[2024-06-27T07:11:58.935+0000] {docker.py:436} INFO - 24/06/27 07:11:58 INFO MicroBatchExecution: Starting [id = fa3aaf6a-54af-4cdb-832e-202d5e6b7860, runId = 48e2cb0b-ede9-41ce-ab8b-ffefd71989ef]. Use file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521 to store the query checkpoint.
[2024-06-27T07:11:58.947+0000] {docker.py:436} INFO - 24/06/27 07:11:58 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@133968bc] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1fc9f80f]
[2024-06-27T07:11:59.021+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T07:11:59.026+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T07:11:59.027+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-27T07:11:59.033+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO MicroBatchExecution: Stream started from {}
[2024-06-27T07:11:59.542+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-27T07:11:59.682+0000] {docker.py:436} INFO - 24/06/27 07:11:59 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/06/27 07:11:59 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
[2024-06-27T07:11:59.683+0000] {docker.py:436} INFO - 24/06/27 07:11:59 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
[2024-06-27T07:11:59.684+0000] {docker.py:436} INFO - 24/06/27 07:11:59 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
[2024-06-27T07:11:59.685+0000] {docker.py:436} INFO - 24/06/27 07:11:59 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
[2024-06-27T07:11:59.687+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:11:59.688+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T07:11:59.689+0000] {docker.py:436} INFO - 24/06/27 07:11:59 INFO AppInfoParser: Kafka startTimeMs: 1719472319682
[2024-06-27T07:12:00.077+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/sources/0/0 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/sources/0/.0.e822c104-8c77-447f-9579-66e594623844.tmp
[2024-06-27T07:12:00.109+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/sources/0/.0.e822c104-8c77-447f-9579-66e594623844.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/sources/0/0
24/06/27 07:12:00 INFO KafkaMicroBatchStream: Initial offsets: {"store_source_data":{"0":8}}
[2024-06-27T07:12:00.135+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/0 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.0.8785c879-abfa-4f26-a949-b47157f3a606.tmp
[2024-06-27T07:12:00.188+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.0.8785c879-abfa-4f26-a949-b47157f3a606.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/0
24/06/27 07:12:00 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719472320124,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:12:00.609+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:12:00.665+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:12:00.735+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:12:00.741+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:12:00.786+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:12:00.789+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:12:00.975+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:12:00.985+0000] {docker.py:436} INFO - 24/06/27 07:12:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:12:01.004+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:12:01.005+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:12:01.006+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:12:01.008+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:12:01.019+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:12:01.219+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:12:01.251+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:12:01.254+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:12:01.260+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:12:01.280+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:12:01.281+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-27T07:12:01.362+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:12:01.377+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-06-27T07:12:01.784+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO CodeGenerator: Code generated in 244.838516 ms
[2024-06-27T07:12:01.914+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO CodeGenerator: Code generated in 43.009917 ms
[2024-06-27T07:12:01.968+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO CodeGenerator: Code generated in 39.43269 ms
[2024-06-27T07:12:02.003+0000] {docker.py:436} INFO - 24/06/27 07:12:01 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T07:12:02.055+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO AppInfoParser: Kafka version: 2.8.1
24/06/27 07:12:02 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T07:12:02.056+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO AppInfoParser: Kafka startTimeMs: 1719472322054
[2024-06-27T07:12:02.057+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T07:12:02.065+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 8 for partition store_source_data-0
[2024-06-27T07:12:02.076+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T07:12:02.185+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:12:02.708+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:12:02.708+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:12:02.710+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=26, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:12:02.917+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:12:02.920+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2024-06-27T07:12:02.952+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3634 bytes result sent to driver
[2024-06-27T07:12:02.965+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1630 ms on localhost (executor driver) (1/1)
[2024-06-27T07:12:02.969+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-27T07:12:02.974+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.934 s
[2024-06-27T07:12:02.980+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:12:02.981+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-06-27T07:12:02.983+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 1.998077 s
[2024-06-27T07:12:02.986+0000] {docker.py:436} INFO - 24/06/27 07:12:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:12:02.989+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 0
-------------------------------------------
[2024-06-27T07:12:03.117+0000] {docker.py:436} INFO - 24/06/27 07:12:03 INFO CodeGenerator: Code generated in 13.435777 ms
[2024-06-27T07:12:04.368+0000] {docker.py:436} INFO - 24/06/27 07:12:04 INFO CodeGenerator: Code generated in 14.185828 ms
[2024-06-27T07:12:04.380+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:12:04.381+0000] {docker.py:436} INFO - 24/06/27 07:12:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:12:04.392+0000] {docker.py:436} INFO - 24/06/27 07:12:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/0 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.0.5f9fa234-921c-4878-b169-e7f26da2ae84.tmp
[2024-06-27T07:12:04.421+0000] {docker.py:436} INFO - 24/06/27 07:12:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.0.5f9fa234-921c-4878-b169-e7f26da2ae84.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/0
[2024-06-27T07:12:04.458+0000] {docker.py:436} INFO - 24/06/27 07:12:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:11:59.010Z",
  "batchId" : 0,
  "numInputRows" : 18,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 3.326557013491037,
  "durationMs" : {
    "addBatch" : 3694,
    "commitOffsets" : 36,
    "getBatch" : 21,
    "latestOffset" : 1086,
    "queryPlanning" : 472,
    "triggerExecution" : 5410,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : null,
    "endOffset" : {
      "store_source_data" : {
        "0" : 26
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 26
      }
    },
    "numInputRows" : 18,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 3.326557013491037,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 18
  }
}
[2024-06-27T07:12:08.781+0000] {docker.py:436} INFO - 24/06/27 07:12:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:12:14.450+0000] {docker.py:436} INFO - 24/06/27 07:12:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:12:24.456+0000] {docker.py:436} INFO - 24/06/27 07:12:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:12:34.465+0000] {docker.py:436} INFO - 24/06/27 07:12:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:12:44.468+0000] {docker.py:436} INFO - 24/06/27 07:12:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:12:54.482+0000] {docker.py:436} INFO - 24/06/27 07:12:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:13:04.428+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/1 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.1.2a426cba-6b13-45c7-914c-51c0f0a4b820.tmp
[2024-06-27T07:13:04.461+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.1.2a426cba-6b13-45c7-914c-51c0f0a4b820.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/1
[2024-06-27T07:13:04.462+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719472384413,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:13:04.487+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:04.500+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:04.561+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:04.567+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:04.588+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:04.591+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:04.607+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:13:04.609+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:13:04.610+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:13:04 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:13:04.611+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:13:04.612+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:13:04.614+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:13:04.621+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:13:04.633+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:13:04.634+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:13:04.635+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:13:04.637+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:13:04.638+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-27T07:13:04.649+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes) 
24/06/27 07:13:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-27T07:13:04.688+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 26 for partition store_source_data-0
[2024-06-27T07:13:04.694+0000] {docker.py:436} INFO - 24/06/27 07:13:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:13:05.196+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:13:05.197+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:13:05.197+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=27, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:13:05.200+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:13:05.201+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2024-06-27T07:13:05.204+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2028 bytes result sent to driver
[2024-06-27T07:13:05.207+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 563 ms on localhost (executor driver) (1/1)
[2024-06-27T07:13:05.208+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-27T07:13:05.209+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.593 s
[2024-06-27T07:13:05.210+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:13:05.211+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-06-27T07:13:05.212+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.601936 s
[2024-06-27T07:13:05.212+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:13:05.213+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 1
-------------------------------------------
[2024-06-27T07:13:05.268+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:13:05.269+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:13:05.280+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/1 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.1.e4f26abf-a741-441a-8dfc-d1f28de2fb58.tmp
[2024-06-27T07:13:05.402+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.1.e4f26abf-a741-441a-8dfc-d1f28de2fb58.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/1
[2024-06-27T07:13:05.408+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:13:04.409Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 1.0070493454179255,
  "durationMs" : {
    "addBatch" : 764,
    "commitOffsets" : 134,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 39,
    "triggerExecution" : 993,
    "walCommit" : 50
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 26
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 27
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 27
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 1.0070493454179255,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:13:05.608+0000] {docker.py:436} INFO - 24/06/27 07:13:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:13:15.410+0000] {docker.py:436} INFO - 24/06/27 07:13:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:13:25.417+0000] {docker.py:436} INFO - 24/06/27 07:13:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:13:35.433+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:13:45.438+0000] {docker.py:436} INFO - 24/06/27 07:13:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:13:55.442+0000] {docker.py:436} INFO - 24/06/27 07:13:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:05.444+0000] {docker.py:436} INFO - 24/06/27 07:14:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:15.453+0000] {docker.py:436} INFO - 24/06/27 07:14:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:22.617+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/2 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.2.781ab751-9cbd-4576-83e1-c63d492fe160.tmp
[2024-06-27T07:14:22.657+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.2.781ab751-9cbd-4576-83e1-c63d492fe160.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/2
[2024-06-27T07:14:22.658+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719472462604,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:14:22.682+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.695+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.761+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.768+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.827+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.828+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.850+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:14:22.852+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:14:22.854+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:14:22.856+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:14:22.862+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:14:22.864+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:14:22.866+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:14:22.884+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:14:22.924+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:14:22.931+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:14:22.936+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:14:22.939+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:14:22.941+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-06-27T07:14:22.946+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:14:22.954+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-06-27T07:14:23.005+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 27 for partition store_source_data-0
[2024-06-27T07:14:23.016+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:14:23.545+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:14:23.570+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:14:23 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2024-06-27T07:14:23.597+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2114 bytes result sent to driver
[2024-06-27T07:14:23.602+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 655 ms on localhost (executor driver) (1/1)
[2024-06-27T07:14:23.603+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-27T07:14:23.603+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.734 s
[2024-06-27T07:14:23.604+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:14:23.607+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 2
-------------------------------------------
[2024-06-27T07:14:23.607+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-06-27T07:14:23.608+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.753294 s
24/06/27 07:14:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:14:23.787+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:14:23.793+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:14:23.937+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/2 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.2.ebf98f46-c171-439a-bcc5-201144022a1a.tmp
[2024-06-27T07:14:23.991+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.2.ebf98f46-c171-439a-bcc5-201144022a1a.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/2
[2024-06-27T07:14:23.991+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:14:22.603Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7246376811594204,
  "durationMs" : {
    "addBatch" : 1103,
    "commitOffsets" : 179,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 42,
    "triggerExecution" : 1380,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 27
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7246376811594204,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:14:33.993+0000] {docker.py:436} INFO - 24/06/27 07:14:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:36.606+0000] {docker.py:436} INFO - 24/06/27 07:14:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:14:44.001+0000] {docker.py:436} INFO - 24/06/27 07:14:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:54.009+0000] {docker.py:436} INFO - 24/06/27 07:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:04.013+0000] {docker.py:436} INFO - 24/06/27 07:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:14.018+0000] {docker.py:436} INFO - 24/06/27 07:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:24.027+0000] {docker.py:436} INFO - 24/06/27 07:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:34.032+0000] {docker.py:436} INFO - 24/06/27 07:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:44.033+0000] {docker.py:436} INFO - 24/06/27 07:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:54.034+0000] {docker.py:436} INFO - 24/06/27 07:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:04.042+0000] {docker.py:436} INFO - 24/06/27 07:16:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:14.050+0000] {docker.py:436} INFO - 24/06/27 07:16:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:24.060+0000] {docker.py:436} INFO - 24/06/27 07:16:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:34.069+0000] {docker.py:436} INFO - 24/06/27 07:16:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:44.075+0000] {docker.py:436} INFO - 24/06/27 07:16:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:54.085+0000] {docker.py:436} INFO - 24/06/27 07:16:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:04.091+0000] {docker.py:436} INFO - 24/06/27 07:17:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:14.098+0000] {docker.py:436} INFO - 24/06/27 07:17:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:24.104+0000] {docker.py:436} INFO - 24/06/27 07:17:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:34.110+0000] {docker.py:436} INFO - 24/06/27 07:17:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:44.110+0000] {docker.py:436} INFO - 24/06/27 07:17:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:54.120+0000] {docker.py:436} INFO - 24/06/27 07:17:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:04.121+0000] {docker.py:436} INFO - 24/06/27 07:18:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:14.131+0000] {docker.py:436} INFO - 24/06/27 07:18:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:19.978+0000] {docker.py:436} INFO - 24/06/27 07:18:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/3 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.3.f19f5997-db01-4f3e-a9c0-53766bcf11ac.tmp
[2024-06-27T07:18:20.031+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.3.f19f5997-db01-4f3e-a9c0-53766bcf11ac.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/3
24/06/27 07:18:20 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719472699957,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:18:20.069+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.081+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.110+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.124+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.184+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.188+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.200+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:18:20.203+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:18:20.216+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:18:20.217+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:18:20.218+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:18:20.218+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:18:20.231+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:18:20.244+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:18:20.260+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:18:20.266+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:18:20.278+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/27 07:18:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:18:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-06-27T07:18:20.284+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:18:20.293+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2024-06-27T07:18:20.350+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 28 for partition store_source_data-0
[2024-06-27T07:18:20.354+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:18:20.855+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:18:20.860+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:18:20.861+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:18:20.861+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:18:20.862+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2024-06-27T07:18:20.868+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2114 bytes result sent to driver
[2024-06-27T07:18:20.872+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 588 ms on localhost (executor driver) (1/1)
[2024-06-27T07:18:20.873+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-06-27T07:18:20.875+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 0.642 s
[2024-06-27T07:18:20.876+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:18:20.877+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-06-27T07:18:20.879+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 0.676370 s
[2024-06-27T07:18:20.880+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:18:20.882+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:18:20.883+0000] {docker.py:436} INFO - Batch: 3
[2024-06-27T07:18:20.884+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:18:20.965+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:18:20.965+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:18:21.034+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/3 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.3.f3e161a2-dd82-439a-8881-47be018f7c31.tmp
[2024-06-27T07:18:21.165+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.3.f3e161a2-dd82-439a-8881-47be018f7c31.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/3
24/06/27 07:18:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:18:19.955Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.828500414250207,
  "durationMs" : {
    "addBatch" : 882,
    "commitOffsets" : 202,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 1207,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.828500414250207,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:18:31.170+0000] {docker.py:436} INFO - 24/06/27 07:18:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:33.009+0000] {docker.py:436} INFO - 24/06/27 07:18:33 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:18:41.168+0000] {docker.py:436} INFO - 24/06/27 07:18:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:51.177+0000] {docker.py:436} INFO - 24/06/27 07:18:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:01.187+0000] {docker.py:436} INFO - 24/06/27 07:19:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:11.192+0000] {docker.py:436} INFO - 24/06/27 07:19:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:21.137+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/4 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.4.b38038dd-c5fc-4e3f-b794-311a84f9b2ec.tmp
[2024-06-27T07:19:21.275+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.4.b38038dd-c5fc-4e3f-b794-311a84f9b2ec.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/4
[2024-06-27T07:19:21.277+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719472761116,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:19:21.305+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.310+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.331+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.336+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.359+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.363+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.407+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:19:21.417+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:19:21.419+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:19:21.420+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:19:21.421+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:19:21.422+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Missing parents: List()
24/06/27 07:19:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:19:21.428+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:19:21.441+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:19:21.443+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:19:21.447+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:19:21.449+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:19:21.449+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-06-27T07:19:21.451+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:19:21.461+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-06-27T07:19:21.558+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 29 for partition store_source_data-0
[2024-06-27T07:19:21.603+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:19:22.106+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:19:22.109+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:19:22.111+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2024-06-27T07:19:22.142+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2114 bytes result sent to driver
[2024-06-27T07:19:22.145+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 695 ms on localhost (executor driver) (1/1)
[2024-06-27T07:19:22.147+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-06-27T07:19:22.150+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.724 s
[2024-06-27T07:19:22.151+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:19:22.153+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-06-27T07:19:22.154+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.736375 s
[2024-06-27T07:19:22.156+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:19:22.157+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:19:22.158+0000] {docker.py:436} INFO - Batch: 4
[2024-06-27T07:19:22.159+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:19:22.231+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:19:22.232+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:19:22.258+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/4 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.4.0ed42497-e0b4-4053-a7ea-78915c384590.tmp
[2024-06-27T07:19:22.418+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.4.0ed42497-e0b4-4053-a7ea-78915c384590.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/4
[2024-06-27T07:19:22.430+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:19:21.112Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.7662835249042146,
  "durationMs" : {
    "addBatch" : 922,
    "commitOffsets" : 182,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 34,
    "triggerExecution" : 1305,
    "walCommit" : 162
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.7662835249042146,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:19:32.432+0000] {docker.py:436} INFO - 24/06/27 07:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:34.256+0000] {docker.py:436} INFO - 24/06/27 07:19:34 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:19:42.440+0000] {docker.py:436} INFO - 24/06/27 07:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:52.446+0000] {docker.py:436} INFO - 24/06/27 07:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:02.456+0000] {docker.py:436} INFO - 24/06/27 07:20:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:12.464+0000] {docker.py:436} INFO - 24/06/27 07:20:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:22.464+0000] {docker.py:436} INFO - 24/06/27 07:20:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:32.471+0000] {docker.py:436} INFO - 24/06/27 07:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:42.481+0000] {docker.py:436} INFO - 24/06/27 07:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:52.489+0000] {docker.py:436} INFO - 24/06/27 07:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:02.500+0000] {docker.py:436} INFO - 24/06/27 07:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:08.891+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/5 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.5.811f87eb-f35f-42e2-8408-96ca959a17b1.tmp
[2024-06-27T07:21:08.960+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.5.811f87eb-f35f-42e2-8408-96ca959a17b1.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/5
24/06/27 07:21:08 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1719472868875,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:21:09.050+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.081+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.150+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.155+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.241+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.266+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.352+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:21:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:21:09 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:21:09.353+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:21:09.353+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:21:09.354+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:21:09.354+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:21:09.394+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:21:09.479+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:21:09.483+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:21:09.484+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:21:09.501+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:21:09.508+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-06-27T07:21:09.513+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:21:09.575+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-06-27T07:21:09.792+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 30 for partition store_source_data-0
[2024-06-27T07:21:09.801+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:21:10.303+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:21:10.303+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=31, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:21:10.306+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:21:10 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2024-06-27T07:21:10.311+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2114 bytes result sent to driver
[2024-06-27T07:21:10.315+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 807 ms on localhost (executor driver) (1/1)
[2024-06-27T07:21:10.315+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-06-27T07:21:10.316+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.956 s
[2024-06-27T07:21:10.317+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:21:10.318+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-06-27T07:21:10.319+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.970380 s
[2024-06-27T07:21:10.320+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:21:10.320+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:21:10.321+0000] {docker.py:436} INFO - Batch: 5
[2024-06-27T07:21:10.321+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:21:10.372+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:21:10.372+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:21:10.390+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/5 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.5.c0577f6e-e413-4d88-b054-84e0b9699a66.tmp
[2024-06-27T07:21:10.445+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.5.c0577f6e-e413-4d88-b054-84e0b9699a66.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/5
[2024-06-27T07:21:10.447+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:21:08.874Z",
  "batchId" : 5,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6369426751592356,
  "durationMs" : {
    "addBatch" : 1285,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 130,
    "triggerExecution" : 1570,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6369426751592356,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:21:20.449+0000] {docker.py:436} INFO - 24/06/27 07:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:21.935+0000] {docker.py:436} INFO - 24/06/27 07:21:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:21:30.459+0000] {docker.py:436} INFO - 24/06/27 07:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:40.466+0000] {docker.py:436} INFO - 24/06/27 07:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:50.469+0000] {docker.py:436} INFO - 24/06/27 07:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:00.474+0000] {docker.py:436} INFO - 24/06/27 07:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:09.724+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/6 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.6.c927b00a-53b0-49f9-a88f-0ab6f533d60a.tmp
[2024-06-27T07:22:09.948+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.6.c927b00a-53b0-49f9-a88f-0ab6f533d60a.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/6
24/06/27 07:22:09 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1719472929646,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:22:09.971+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:09.985+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.057+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.069+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.101+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.112+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.144+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:22:10.148+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:22:10.150+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:22:10.151+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:22:10.152+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:22:10.154+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:22:10.160+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:22:10.180+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:22:10.201+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:22:10.202+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:22:10.202+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:22:10.221+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:22:10.221+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-06-27T07:22:10.224+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:22:10.232+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2024-06-27T07:22:10.371+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 31 for partition store_source_data-0
[2024-06-27T07:22:10.371+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:22:10.860+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:22:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:22:10.865+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=32, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:22:10 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:22:10 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2024-06-27T07:22:10.923+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2114 bytes result sent to driver
24/06/27 07:22:10 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 699 ms on localhost (executor driver) (1/1)
24/06/27 07:22:10 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-06-27T07:22:10.936+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.772 s
24/06/27 07:22:10 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:22:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-06-27T07:22:10.937+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.786216 s
24/06/27 07:22:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:22:10.937+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 6
-------------------------------------------
[2024-06-27T07:22:11.056+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:22:11.057+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:22:11.090+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/6 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.6.c83d4740-3895-42fa-87b6-bb43a766e430.tmp
[2024-06-27T07:22:11.238+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.6.c83d4740-3895-42fa-87b6-bb43a766e430.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/6
[2024-06-27T07:22:11.243+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:22:09.643Z",
  "batchId" : 6,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.6261740763932373,
  "durationMs" : {
    "addBatch" : 1036,
    "commitOffsets" : 184,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 77,
    "triggerExecution" : 1597,
    "walCommit" : 295
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.6261740763932373,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:22:21.242+0000] {docker.py:436} INFO - 24/06/27 07:22:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:22.965+0000] {docker.py:436} INFO - 24/06/27 07:22:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:22:31.243+0000] {docker.py:436} INFO - 24/06/27 07:22:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:41.246+0000] {docker.py:436} INFO - 24/06/27 07:22:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:51.253+0000] {docker.py:436} INFO - 24/06/27 07:22:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:01.275+0000] {docker.py:436} INFO - 24/06/27 07:23:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:06.317+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/7 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.7.c02cd407-0d06-4910-a58d-8711102f4199.tmp
[2024-06-27T07:23:06.434+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.7.c02cd407-0d06-4910-a58d-8711102f4199.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/7
[2024-06-27T07:23:06.435+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1719472986278,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:23:06.498+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.506+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.578+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.587+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.658+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.660+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.723+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:23:06.725+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:23:06.737+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:23:06.742+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:23:06 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:23:06 INFO DAGScheduler: Missing parents: List()
24/06/27 07:23:06 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:23:06.744+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:23:06.760+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:23:06.764+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:23:06.775+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:23:06.776+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:23:06.777+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-06-27T07:23:06.783+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:23:06.787+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-06-27T07:23:06.969+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 32 for partition store_source_data-0
[2024-06-27T07:23:07.008+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:23:07.517+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:23:07.519+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=33, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:23:07.521+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:23:07.522+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2024-06-27T07:23:07.550+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2114 bytes result sent to driver
[2024-06-27T07:23:07.553+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 772 ms on localhost (executor driver) (1/1)
[2024-06-27T07:23:07.554+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-06-27T07:23:07.555+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.814 s
[2024-06-27T07:23:07.555+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:23:07.556+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-06-27T07:23:07.560+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.830318 s
[2024-06-27T07:23:07.561+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:23:07.566+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:23:07.566+0000] {docker.py:436} INFO - Batch: 7
[2024-06-27T07:23:07.567+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:23:07.656+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:23:07.657+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:23:07.690+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/7 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.7.e1719ec2-1ed2-4827-8acf-dd36c79d9202.tmp
[2024-06-27T07:23:07.794+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.7.e1719ec2-1ed2-4827-8acf-dd36c79d9202.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/7
[2024-06-27T07:23:07.796+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:23:06.267Z",
  "batchId" : 7,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 40.0,
  "processedRowsPerSecond" : 0.655307994757536,
  "durationMs" : {
    "addBatch" : 1139,
    "commitOffsets" : 133,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 82,
    "triggerExecution" : 1526,
    "walCommit" : 158
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 40.0,
    "processedRowsPerSecond" : 0.655307994757536,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:23:17.802+0000] {docker.py:436} INFO - 24/06/27 07:23:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:19.354+0000] {docker.py:436} INFO - 24/06/27 07:23:19 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:23:27.802+0000] {docker.py:436} INFO - 24/06/27 07:23:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:37.805+0000] {docker.py:436} INFO - 24/06/27 07:23:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:47.811+0000] {docker.py:436} INFO - 24/06/27 07:23:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:57.852+0000] {docker.py:436} INFO - 24/06/27 07:23:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:59.652+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/8 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.8.34e536ab-c4dd-44bc-b7ea-8442a7a877c7.tmp
[2024-06-27T07:23:59.754+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.8.34e536ab-c4dd-44bc-b7ea-8442a7a877c7.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/8
[2024-06-27T07:23:59.758+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1719473039560,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:23:59.801+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.809+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.870+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.893+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.939+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.941+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.966+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:23:59.970+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:23:59.977+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:23:59 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:23:59.989+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:23:59.989+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:23:59.999+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:23:59.999+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:24:00.023+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:24:00.023+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:24:00.023+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:24:00.032+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:24:00.033+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-06-27T07:24:00.037+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:24:00.045+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-06-27T07:24:00.163+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 33 for partition store_source_data-0
[2024-06-27T07:24:00.184+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:24:00.683+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:24:00.709+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:24:00.721+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=34, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:24:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:24:00 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2024-06-27T07:24:00.748+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2028 bytes result sent to driver
[2024-06-27T07:24:00.751+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 716 ms on localhost (executor driver) (1/1)
[2024-06-27T07:24:00.752+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-06-27T07:24:00.759+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.784 s
[2024-06-27T07:24:00.760+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:24:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
24/06/27 07:24:00 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.792061 s
[2024-06-27T07:24:00.761+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:24:00.762+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 8
-------------------------------------------
[2024-06-27T07:24:01.016+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:24:01.019+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:24:01.056+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/8 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.8.9f46f9c0-e3b2-4484-9a17-7b98ab266f6d.tmp
[2024-06-27T07:24:01.204+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.8.9f46f9c0-e3b2-4484-9a17-7b98ab266f6d.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/8
[2024-06-27T07:24:01.209+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:23:59.556Z",
  "batchId" : 8,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 41.666666666666664,
  "processedRowsPerSecond" : 0.6067961165048544,
  "durationMs" : {
    "addBatch" : 1204,
    "commitOffsets" : 174,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 68,
    "triggerExecution" : 1648,
    "walCommit" : 197
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 41.666666666666664,
    "processedRowsPerSecond" : 0.6067961165048544,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:24:11.210+0000] {docker.py:436} INFO - 24/06/27 07:24:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:13.148+0000] {docker.py:436} INFO - 24/06/27 07:24:13 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:24:21.223+0000] {docker.py:436} INFO - 24/06/27 07:24:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:31.229+0000] {docker.py:436} INFO - 24/06/27 07:24:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:41.233+0000] {docker.py:436} INFO - 24/06/27 07:24:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:51.243+0000] {docker.py:436} INFO - 24/06/27 07:24:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:01.244+0000] {docker.py:436} INFO - 24/06/27 07:25:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:11.247+0000] {docker.py:436} INFO - 24/06/27 07:25:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:21.249+0000] {docker.py:436} INFO - 24/06/27 07:25:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:31.252+0000] {docker.py:436} INFO - 24/06/27 07:25:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:32.086+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/9 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.9.7ff3db11-ac61-402e-94db-d0c3a3e860a2.tmp
[2024-06-27T07:25:32.142+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.9.7ff3db11-ac61-402e-94db-d0c3a3e860a2.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/9
[2024-06-27T07:25:32.143+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1719473132069,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:25:32.161+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.163+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.179+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.188+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.220+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.226+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.244+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:25:32.245+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:25:32.246+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:25:32 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:25:32 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:25:32 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:25:32.247+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:25:32.250+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:25:32.254+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:25:32.261+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:25:32.262+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:25:32.263+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:25:32 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-06-27T07:25:32.265+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:25:32.273+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2024-06-27T07:25:32.331+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 34 for partition store_source_data-0
[2024-06-27T07:25:32.341+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:25:32.843+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:25:32.843+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:25:32.844+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=35, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:25:32.844+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:25:32 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2024-06-27T07:25:32.846+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2028 bytes result sent to driver
[2024-06-27T07:25:32.848+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 583 ms on localhost (executor driver) (1/1)
[2024-06-27T07:25:32.848+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-06-27T07:25:32.850+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.602 s
[2024-06-27T07:25:32.850+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:25:32.851+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-06-27T07:25:32.852+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.607433 s
[2024-06-27T07:25:32.852+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:25:32.853+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 9
-------------------------------------------
[2024-06-27T07:25:32.880+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:25:32.881+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:25:32.898+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/9 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.9.f9246bad-f698-450c-b8a6-bb53f6394053.tmp
[2024-06-27T07:25:32.995+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.9.f9246bad-f698-450c-b8a6-bb53f6394053.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/9
[2024-06-27T07:25:32.996+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:25:32.065Z",
  "batchId" : 9,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.075268817204301,
  "durationMs" : {
    "addBatch" : 712,
    "commitOffsets" : 114,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 25,
    "triggerExecution" : 930,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.075268817204301,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:25:42.997+0000] {docker.py:436} INFO - 24/06/27 07:25:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:44.325+0000] {docker.py:436} INFO - 24/06/27 07:25:44 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:25:53.006+0000] {docker.py:436} INFO - 24/06/27 07:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:03.011+0000] {docker.py:436} INFO - 24/06/27 07:26:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:13.011+0000] {docker.py:436} INFO - 24/06/27 07:26:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:23.014+0000] {docker.py:436} INFO - 24/06/27 07:26:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:33.015+0000] {docker.py:436} INFO - 24/06/27 07:26:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:43.016+0000] {docker.py:436} INFO - 24/06/27 07:26:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:53.020+0000] {docker.py:436} INFO - 24/06/27 07:26:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:03.029+0000] {docker.py:436} INFO - 24/06/27 07:27:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:06.375+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/10 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.10.6598eafe-865c-4ed2-827a-1ba717a59545.tmp
[2024-06-27T07:27:06.426+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.10.6598eafe-865c-4ed2-827a-1ba717a59545.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/10
[2024-06-27T07:27:06.427+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1719473226359,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:27:06.471+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.475+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.509+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.516+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.545+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.555+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.573+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:27:06.577+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:27:06.597+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:27:06 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:27:06 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:27:06 INFO DAGScheduler: Missing parents: List()
24/06/27 07:27:06 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:27:06.600+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:27:06.613+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:27:06.617+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:27:06.652+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:27:06.652+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:27:06 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-06-27T07:27:06.654+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:27:06.661+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2024-06-27T07:27:06.782+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 35 for partition store_source_data-0
[2024-06-27T07:27:06.786+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:27:07.312+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:27:07.312+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:27:07.313+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:27:07.344+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:27:07.345+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2024-06-27T07:27:07.346+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2028 bytes result sent to driver
[2024-06-27T07:27:07.361+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 708 ms on localhost (executor driver) (1/1)
[2024-06-27T07:27:07.362+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-06-27T07:27:07.372+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.772 s
[2024-06-27T07:27:07.374+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:27:07.374+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-06-27T07:27:07.397+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.819456 s
[2024-06-27T07:27:07.399+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:27:07.399+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:27:07.400+0000] {docker.py:436} INFO - Batch: 10
[2024-06-27T07:27:07.400+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:27:07.595+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:27:07.599+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:27:07.636+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/10 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.10.1d1996e3-869c-4e35-bae1-70d7cded8f4e.tmp
[2024-06-27T07:27:07.730+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.10.1d1996e3-869c-4e35-bae1-70d7cded8f4e.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/10
[2024-06-27T07:27:07.733+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:27:06.358Z",
  "batchId" : 10,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.7283321194464676,
  "durationMs" : {
    "addBatch" : 1126,
    "commitOffsets" : 128,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 48,
    "triggerExecution" : 1373,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.7283321194464676,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:27:17.746+0000] {docker.py:436} INFO - 24/06/27 07:27:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:20.365+0000] {docker.py:436} INFO - 24/06/27 07:27:20 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:27:27.754+0000] {docker.py:436} INFO - 24/06/27 07:27:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:37.755+0000] {docker.py:436} INFO - 24/06/27 07:27:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:47.755+0000] {docker.py:436} INFO - 24/06/27 07:27:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:57.757+0000] {docker.py:436} INFO - 24/06/27 07:27:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:07.759+0000] {docker.py:436} INFO - 24/06/27 07:28:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:17.768+0000] {docker.py:436} INFO - 24/06/27 07:28:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:27.776+0000] {docker.py:436} INFO - 24/06/27 07:28:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:37.784+0000] {docker.py:436} INFO - 24/06/27 07:28:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:47.793+0000] {docker.py:436} INFO - 24/06/27 07:28:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:57.804+0000] {docker.py:436} INFO - 24/06/27 07:28:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:07.804+0000] {docker.py:436} INFO - 24/06/27 07:29:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:17.807+0000] {docker.py:436} INFO - 24/06/27 07:29:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:27.817+0000] {docker.py:436} INFO - 24/06/27 07:29:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:37.822+0000] {docker.py:436} INFO - 24/06/27 07:29:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:47.827+0000] {docker.py:436} INFO - 24/06/27 07:29:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:57.830+0000] {docker.py:436} INFO - 24/06/27 07:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:07.832+0000] {docker.py:436} INFO - 24/06/27 07:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:17.833+0000] {docker.py:436} INFO - 24/06/27 07:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:27.839+0000] {docker.py:436} INFO - 24/06/27 07:30:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:37.844+0000] {docker.py:436} INFO - 24/06/27 07:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:47.846+0000] {docker.py:436} INFO - 24/06/27 07:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:57.848+0000] {docker.py:436} INFO - 24/06/27 07:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:07.854+0000] {docker.py:436} INFO - 24/06/27 07:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:17.862+0000] {docker.py:436} INFO - 24/06/27 07:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:27.872+0000] {docker.py:436} INFO - 24/06/27 07:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:37.878+0000] {docker.py:436} INFO - 24/06/27 07:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:47.882+0000] {docker.py:436} INFO - 24/06/27 07:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:57.882+0000] {docker.py:436} INFO - 24/06/27 07:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:07.891+0000] {docker.py:436} INFO - 24/06/27 07:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:17.894+0000] {docker.py:436} INFO - 24/06/27 07:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:27.905+0000] {docker.py:436} INFO - 24/06/27 07:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:35.493+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/11 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.11.a34e55dd-9d0d-4f4b-b555-763a6f420433.tmp
[2024-06-27T07:32:35.603+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.11.a34e55dd-9d0d-4f4b-b555-763a6f420433.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/11
[2024-06-27T07:32:35.607+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1719473555471,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:32:35.635+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.639+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.665+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.667+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.725+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.729+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.739+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:32:35.740+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:32:35.743+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:32:35.745+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:32:35.746+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:32:35.748+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:32:35.749+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:32:35.762+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:32:35.772+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:32:35.773+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:32:35.774+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:32:35.775+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:32:35 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-06-27T07:32:35.778+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:32:35.790+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2024-06-27T07:32:35.844+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 36 for partition store_source_data-0
[2024-06-27T07:32:35.849+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:32:36.352+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:32:36.353+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:32:36.353+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=37, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:32:36.354+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:32:36 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2024-06-27T07:32:36.357+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2028 bytes result sent to driver
[2024-06-27T07:32:36.362+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 581 ms on localhost (executor driver) (1/1)
24/06/27 07:32:36 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-06-27T07:32:36.363+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 11
-------------------------------------------
[2024-06-27T07:32:36.364+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.606 s
24/06/27 07:32:36 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:32:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
24/06/27 07:32:36 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.620215 s
24/06/27 07:32:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:32:36.401+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:32:36.401+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:32:36.423+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/11 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.11.ac53b9cf-5a86-4637-b21a-dddc17903af8.tmp
[2024-06-27T07:32:36.482+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.11.ac53b9cf-5a86-4637-b21a-dddc17903af8.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/11
[2024-06-27T07:32:36.483+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:32:35.463Z",
  "batchId" : 11,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 62.5,
  "processedRowsPerSecond" : 0.9823182711198428,
  "durationMs" : {
    "addBatch" : 753,
    "commitOffsets" : 83,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 37,
    "triggerExecution" : 1018,
    "walCommit" : 136
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 62.5,
    "processedRowsPerSecond" : 0.9823182711198428,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:32:46.484+0000] {docker.py:436} INFO - 24/06/27 07:32:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:49.682+0000] {docker.py:436} INFO - 24/06/27 07:32:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:32:56.492+0000] {docker.py:436} INFO - 24/06/27 07:32:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:06.498+0000] {docker.py:436} INFO - 24/06/27 07:33:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:16.505+0000] {docker.py:436} INFO - 24/06/27 07:33:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:26.516+0000] {docker.py:436} INFO - 24/06/27 07:33:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:36.525+0000] {docker.py:436} INFO - 24/06/27 07:33:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:46.526+0000] {docker.py:436} INFO - 24/06/27 07:33:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:56.528+0000] {docker.py:436} INFO - 24/06/27 07:33:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:06.538+0000] {docker.py:436} INFO - 24/06/27 07:34:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:16.546+0000] {docker.py:436} INFO - 24/06/27 07:34:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:26.552+0000] {docker.py:436} INFO - 24/06/27 07:34:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:36.563+0000] {docker.py:436} INFO - 24/06/27 07:34:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:46.574+0000] {docker.py:436} INFO - 24/06/27 07:34:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:56.582+0000] {docker.py:436} INFO - 24/06/27 07:34:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:06.583+0000] {docker.py:436} INFO - 24/06/27 07:35:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:16.589+0000] {docker.py:436} INFO - 24/06/27 07:35:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:26.599+0000] {docker.py:436} INFO - 24/06/27 07:35:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:36.609+0000] {docker.py:436} INFO - 24/06/27 07:35:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:46.612+0000] {docker.py:436} INFO - 24/06/27 07:35:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:56.616+0000] {docker.py:436} INFO - 24/06/27 07:35:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:06.620+0000] {docker.py:436} INFO - 24/06/27 07:36:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:16.624+0000] {docker.py:436} INFO - 24/06/27 07:36:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:26.625+0000] {docker.py:436} INFO - 24/06/27 07:36:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:36.627+0000] {docker.py:436} INFO - 24/06/27 07:36:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:46.634+0000] {docker.py:436} INFO - 24/06/27 07:36:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:56.640+0000] {docker.py:436} INFO - 24/06/27 07:36:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:06.641+0000] {docker.py:436} INFO - 24/06/27 07:37:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:16.650+0000] {docker.py:436} INFO - 24/06/27 07:37:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:26.661+0000] {docker.py:436} INFO - 24/06/27 07:37:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:36.669+0000] {docker.py:436} INFO - 24/06/27 07:37:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:46.668+0000] {docker.py:436} INFO - 24/06/27 07:37:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:56.670+0000] {docker.py:436} INFO - 24/06/27 07:37:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:01.859+0000] {docker.py:436} INFO - 24/06/27 07:38:01 INFO Metrics: Metrics scheduler closed
[2024-06-27T07:38:01.861+0000] {docker.py:436} INFO - 24/06/27 07:38:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T07:38:01.862+0000] {docker.py:436} INFO - 24/06/27 07:38:01 INFO Metrics: Metrics reporters closed
[2024-06-27T07:38:01.868+0000] {docker.py:436} INFO - 24/06/27 07:38:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-1 unregistered
[2024-06-27T07:38:06.677+0000] {docker.py:436} INFO - 24/06/27 07:38:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:16.679+0000] {docker.py:436} INFO - 24/06/27 07:38:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:26.686+0000] {docker.py:436} INFO - 24/06/27 07:38:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:36.696+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:46.699+0000] {docker.py:436} INFO - 24/06/27 07:38:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:56.700+0000] {docker.py:436} INFO - 24/06/27 07:38:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:06.703+0000] {docker.py:436} INFO - 24/06/27 07:39:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:16.709+0000] {docker.py:436} INFO - 24/06/27 07:39:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:26.716+0000] {docker.py:436} INFO - 24/06/27 07:39:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:36.725+0000] {docker.py:436} INFO - 24/06/27 07:39:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:46.724+0000] {docker.py:436} INFO - 24/06/27 07:39:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:56.729+0000] {docker.py:436} INFO - 24/06/27 07:39:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:06.734+0000] {docker.py:436} INFO - 24/06/27 07:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:16.741+0000] {docker.py:436} INFO - 24/06/27 07:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:26.750+0000] {docker.py:436} INFO - 24/06/27 07:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:36.755+0000] {docker.py:436} INFO - 24/06/27 07:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:46.764+0000] {docker.py:436} INFO - 24/06/27 07:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:56.770+0000] {docker.py:436} INFO - 24/06/27 07:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:06.776+0000] {docker.py:436} INFO - 24/06/27 07:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:16.779+0000] {docker.py:436} INFO - 24/06/27 07:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:26.787+0000] {docker.py:436} INFO - 24/06/27 07:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:36.786+0000] {docker.py:436} INFO - 24/06/27 07:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:46.787+0000] {docker.py:436} INFO - 24/06/27 07:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:56.791+0000] {docker.py:436} INFO - 24/06/27 07:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:06.798+0000] {docker.py:436} INFO - 24/06/27 07:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:16.803+0000] {docker.py:436} INFO - 24/06/27 07:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:26.809+0000] {docker.py:436} INFO - 24/06/27 07:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:36.810+0000] {docker.py:436} INFO - 24/06/27 07:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:46.820+0000] {docker.py:436} INFO - 24/06/27 07:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:56.831+0000] {docker.py:436} INFO - 24/06/27 07:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:06.836+0000] {docker.py:436} INFO - 24/06/27 07:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:16.839+0000] {docker.py:436} INFO - 24/06/27 07:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:26.845+0000] {docker.py:436} INFO - 24/06/27 07:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:36.872+0000] {docker.py:436} INFO - 24/06/27 07:43:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:46.856+0000] {docker.py:436} INFO - 24/06/27 07:43:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:56.862+0000] {docker.py:436} INFO - 24/06/27 07:43:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:06.871+0000] {docker.py:436} INFO - 24/06/27 07:44:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:16.878+0000] {docker.py:436} INFO - 24/06/27 07:44:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:26.886+0000] {docker.py:436} INFO - 24/06/27 07:44:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:36.891+0000] {docker.py:436} INFO - 24/06/27 07:44:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:46.900+0000] {docker.py:436} INFO - 24/06/27 07:44:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:56.904+0000] {docker.py:436} INFO - 24/06/27 07:44:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:06.904+0000] {docker.py:436} INFO - 24/06/27 07:45:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:16.911+0000] {docker.py:436} INFO - 24/06/27 07:45:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:26.916+0000] {docker.py:436} INFO - 24/06/27 07:45:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:36.923+0000] {docker.py:436} INFO - 24/06/27 07:45:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:46.923+0000] {docker.py:436} INFO - 24/06/27 07:45:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:56.926+0000] {docker.py:436} INFO - 24/06/27 07:45:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:06.931+0000] {docker.py:436} INFO - 24/06/27 07:46:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:16.937+0000] {docker.py:436} INFO - 24/06/27 07:46:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:26.946+0000] {docker.py:436} INFO - 24/06/27 07:46:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:36.946+0000] {docker.py:436} INFO - 24/06/27 07:46:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:46.947+0000] {docker.py:436} INFO - 24/06/27 07:46:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:56.947+0000] {docker.py:436} INFO - 24/06/27 07:46:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:06.952+0000] {docker.py:436} INFO - 24/06/27 07:47:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:16.958+0000] {docker.py:436} INFO - 24/06/27 07:47:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:26.958+0000] {docker.py:436} INFO - 24/06/27 07:47:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:36.967+0000] {docker.py:436} INFO - 24/06/27 07:47:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:46.974+0000] {docker.py:436} INFO - 24/06/27 07:47:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:56.986+0000] {docker.py:436} INFO - 24/06/27 07:47:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:59.899+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/12 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.12.d1f02b7f-b508-428e-a227-a0866a333edc.tmp
[2024-06-27T07:47:59.942+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.12.d1f02b7f-b508-428e-a227-a0866a333edc.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/12
[2024-06-27T07:47:59.942+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1719474479885,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:47:59.958+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.960+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.979+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.981+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.009+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.012+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.057+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:48:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:48:00.064+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:48:00.069+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:48:00.069+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:48:00.070+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:48:00.070+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:48:00.075+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:48:00.088+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:48:00.099+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
24/06/27 07:48:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
24/06/27 07:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:48:00 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-06-27T07:48:00.099+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:48:00.109+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2024-06-27T07:48:00.165+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T07:48:00.200+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka version: 2.8.1
24/06/27 07:48:00 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 07:48:00 INFO AppInfoParser: Kafka startTimeMs: 1719474480199
[2024-06-27T07:48:00.201+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T07:48:00.202+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 37 for partition store_source_data-0
[2024-06-27T07:48:00.211+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T07:48:00.220+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:48:00.739+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:48:00.740+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:48:00.741+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:48:00.782+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2024-06-27T07:48:00.797+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2028 bytes result sent to driver
[2024-06-27T07:48:00.811+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 708 ms on localhost (executor driver) (1/1)
[2024-06-27T07:48:00.812+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-06-27T07:48:00.813+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.740 s
24/06/27 07:48:00 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:48:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-06-27T07:48:00.814+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.753215 s
[2024-06-27T07:48:00.816+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:48:00.818+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:48:00.819+0000] {docker.py:436} INFO - Batch: 12
[2024-06-27T07:48:00.822+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:48:00.915+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:48:00.915+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:48:01.060+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/12 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.12.d38f94b6-b9b4-44a0-87db-13f75f2ce672.tmp
[2024-06-27T07:48:01.319+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.12.d38f94b6-b9b4-44a0-87db-13f75f2ce672.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/12
[2024-06-27T07:48:01.327+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:47:59.884Z",
  "batchId" : 12,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6963788300835655,
  "durationMs" : {
    "addBatch" : 942,
    "commitOffsets" : 410,
    "getBatch" : 1,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 1436,
    "walCommit" : 55
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6963788300835655,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:48:10.625+0000] {docker.py:436} INFO - 24/06/27 07:48:10 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:48:11.327+0000] {docker.py:436} INFO - 24/06/27 07:48:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:21.342+0000] {docker.py:436} INFO - 24/06/27 07:48:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:31.342+0000] {docker.py:436} INFO - 24/06/27 07:48:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:41.346+0000] {docker.py:436} INFO - 24/06/27 07:48:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:51.348+0000] {docker.py:436} INFO - 24/06/27 07:48:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:01.349+0000] {docker.py:436} INFO - 24/06/27 07:49:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:11.354+0000] {docker.py:436} INFO - 24/06/27 07:49:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:21.356+0000] {docker.py:436} INFO - 24/06/27 07:49:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:31.366+0000] {docker.py:436} INFO - 24/06/27 07:49:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:41.372+0000] {docker.py:436} INFO - 24/06/27 07:49:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:51.382+0000] {docker.py:436} INFO - 24/06/27 07:49:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:59.768+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/13 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.13.dbf8f134-82fd-4996-8ae1-97fdc577bceb.tmp
[2024-06-27T07:49:59.847+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.13.dbf8f134-82fd-4996-8ae1-97fdc577bceb.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/13
24/06/27 07:49:59 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1719474599740,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:49:59.894+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.899+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.939+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.965+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.970+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.989+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:49:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:49:59 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:49:59 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:49:59 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:49:59 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:49:59.992+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:49:59.996+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:50:00.000+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:50:00.001+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:50:00.001+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:50:00.003+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:50:00.003+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-06-27T07:50:00.005+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:50:00.007+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2024-06-27T07:50:00.033+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 38 for partition store_source_data-0
[2024-06-27T07:50:00.036+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:50:00.544+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:50:00.545+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:50:00.548+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:50:00.551+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
24/06/27 07:50:00 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2028 bytes result sent to driver
[2024-06-27T07:50:00.554+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 550 ms on localhost (executor driver) (1/1)
[2024-06-27T07:50:00.555+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-06-27T07:50:00.559+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 0.568 s
[2024-06-27T07:50:00.559+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:50:00.560+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-06-27T07:50:00.561+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.574338 s
[2024-06-27T07:50:00.565+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:50:00.566+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:50:00.566+0000] {docker.py:436} INFO - Batch: 13
[2024-06-27T07:50:00.567+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:50:00.640+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:50:00.645+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:50:00.660+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/13 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.13.63d1edaf-b5cc-42a5-b0e3-0ee5dc01edc0.tmp
[2024-06-27T07:50:00.711+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.13.63d1edaf-b5cc-42a5-b0e3-0ee5dc01edc0.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/13
[2024-06-27T07:50:00.713+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:49:59.737Z",
  "batchId" : 13,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.026694045174538,
  "durationMs" : {
    "addBatch" : 744,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 54,
    "triggerExecution" : 974,
    "walCommit" : 105
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.026694045174538,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:50:02.512+0000] {docker.py:436} INFO - 24/06/27 07:50:02 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:50:10.733+0000] {docker.py:436} INFO - 24/06/27 07:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:20.738+0000] {docker.py:436} INFO - 24/06/27 07:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:30.746+0000] {docker.py:436} INFO - 24/06/27 07:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:40.746+0000] {docker.py:436} INFO - 24/06/27 07:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:50.752+0000] {docker.py:436} INFO - 24/06/27 07:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:00.754+0000] {docker.py:436} INFO - 24/06/27 07:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:10.762+0000] {docker.py:436} INFO - 24/06/27 07:51:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:20.763+0000] {docker.py:436} INFO - 24/06/27 07:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:30.768+0000] {docker.py:436} INFO - 24/06/27 07:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:40.771+0000] {docker.py:436} INFO - 24/06/27 07:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:42.112+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/14 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.14.97acc286-24f3-4a03-a617-ba72c4ab04c7.tmp
[2024-06-27T07:51:42.256+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.14.97acc286-24f3-4a03-a617-ba72c4ab04c7.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/14
24/06/27 07:51:42 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1719474702093,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:51:42.304+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.329+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.381+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.444+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.451+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.474+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:51:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:51:42.474+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:51:42.475+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:51:42.475+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:51:42.476+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:51:42.476+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:51:42.485+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:51:42.486+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:51:42.494+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:51:42.495+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:51:42.496+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:51:42.497+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-06-27T07:51:42.497+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:51:42.503+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2024-06-27T07:51:42.679+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 39 for partition store_source_data-0
[2024-06-27T07:51:42.721+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:51:43.230+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:51:43.233+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:51:43 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:51:43.245+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
24/06/27 07:51:43 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2028 bytes result sent to driver
[2024-06-27T07:51:43.266+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 14
-------------------------------------------
[2024-06-27T07:51:43.266+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 758 ms on localhost (executor driver) (1/1)
24/06/27 07:51:43 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 0.793 s
24/06/27 07:51:43 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:51:43 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/06/27 07:51:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/06/27 07:51:43 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.796363 s
24/06/27 07:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:51:43.390+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:51:43.391+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:51:43.425+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/14 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.14.d3462333-1015-4d00-8766-52b2a69af902.tmp
[2024-06-27T07:51:43.554+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.14.d3462333-1015-4d00-8766-52b2a69af902.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/14
[2024-06-27T07:51:43.560+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:51:42.091Z",
  "batchId" : 14,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.683526999316473,
  "durationMs" : {
    "addBatch" : 1072,
    "commitOffsets" : 171,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 55,
    "triggerExecution" : 1463,
    "walCommit" : 162
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.683526999316473,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:51:53.557+0000] {docker.py:436} INFO - 24/06/27 07:51:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:55.020+0000] {docker.py:436} INFO - 24/06/27 07:51:55 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:52:03.567+0000] {docker.py:436} INFO - 24/06/27 07:52:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:13.577+0000] {docker.py:436} INFO - 24/06/27 07:52:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:23.581+0000] {docker.py:436} INFO - 24/06/27 07:52:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:33.592+0000] {docker.py:436} INFO - 24/06/27 07:52:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:43.598+0000] {docker.py:436} INFO - 24/06/27 07:52:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:53.601+0000] {docker.py:436} INFO - 24/06/27 07:52:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:59.319+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/15 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.15.9a5b30b6-1b68-486e-901e-21b748afc26f.tmp
[2024-06-27T07:52:59.370+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.15.9a5b30b6-1b68-486e-901e-21b748afc26f.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/15
[2024-06-27T07:52:59.371+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1719474779306,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:52:59.422+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.436+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.447+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.452+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.472+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.506+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.551+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:52:59.552+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:52:59.553+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:52:59.554+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:52:59.555+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:52:59.556+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:52:59.557+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[47] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:52:59.585+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:52:59.595+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:52:59.617+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:52:59.617+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:52:59.622+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[47] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:52:59.624+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-06-27T07:52:59.628+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:52:59.639+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2024-06-27T07:52:59.782+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 40 for partition store_source_data-0
[2024-06-27T07:52:59.794+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:53:00.295+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:53:00.295+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:53:00.297+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=41, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:53:00.298+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:53:00.299+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2024-06-27T07:53:00.300+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2028 bytes result sent to driver
[2024-06-27T07:53:00.302+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 675 ms on localhost (executor driver) (1/1)
[2024-06-27T07:53:00.304+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-06-27T07:53:00.304+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 0.742 s
[2024-06-27T07:53:00.305+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:53:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-06-27T07:53:00.305+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.751707 s
[2024-06-27T07:53:00.307+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:53:00.308+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 15
[2024-06-27T07:53:00.308+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:53:00.349+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:53:00.350+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:53:00.369+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/15 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.15.feda0535-eb98-43d5-a858-d7290f3ef250.tmp
[2024-06-27T07:53:00.505+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.15.feda0535-eb98-43d5-a858-d7290f3ef250.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/15
[2024-06-27T07:53:00.509+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:52:59.305Z",
  "batchId" : 15,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8340283569641367,
  "durationMs" : {
    "addBatch" : 912,
    "commitOffsets" : 155,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 66,
    "triggerExecution" : 1199,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8340283569641367,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:53:04.477+0000] {docker.py:436} INFO - 24/06/27 07:53:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:53:10.513+0000] {docker.py:436} INFO - 24/06/27 07:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:20.518+0000] {docker.py:436} INFO - 24/06/27 07:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:30.524+0000] {docker.py:436} INFO - 24/06/27 07:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:40.524+0000] {docker.py:436} INFO - 24/06/27 07:53:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:50.525+0000] {docker.py:436} INFO - 24/06/27 07:53:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:00.526+0000] {docker.py:436} INFO - 24/06/27 07:54:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:10.528+0000] {docker.py:436} INFO - 24/06/27 07:54:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:20.538+0000] {docker.py:436} INFO - 24/06/27 07:54:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:30.549+0000] {docker.py:436} INFO - 24/06/27 07:54:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:40.551+0000] {docker.py:436} INFO - 24/06/27 07:54:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:47.760+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/16 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.16.1f64c0fc-b404-4fd8-ae61-93a2e4da9033.tmp
[2024-06-27T07:54:47.824+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.16.1f64c0fc-b404-4fd8-ae61-93a2e4da9033.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/16
[2024-06-27T07:54:47.825+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1719474887744,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:54:47.851+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.865+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.877+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.896+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.919+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.928+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.944+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:54:47.945+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:54:47.947+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:54:47.951+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:54:47.952+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:54:47.960+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Missing parents: List()
24/06/27 07:54:47 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[50] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:54:47.962+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:54:47.966+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:54:47.967+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:54:47.968+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:54:47.970+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[50] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:54:47.971+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-06-27T07:54:47.974+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:54:47.977+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2024-06-27T07:54:47.991+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 41 for partition store_source_data-0
[2024-06-27T07:54:47.998+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:54:48.505+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:54:48.506+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:54:48.514+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:54:48.522+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:54:48 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2024-06-27T07:54:48.523+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2028 bytes result sent to driver
[2024-06-27T07:54:48.534+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 16
-------------------------------------------
[2024-06-27T07:54:48.535+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 552 ms on localhost (executor driver) (1/1)
24/06/27 07:54:48 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
24/06/27 07:54:48 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.566 s
24/06/27 07:54:48 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:54:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
24/06/27 07:54:48 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.581490 s
24/06/27 07:54:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:54:48.602+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:54:48.603+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:54:48.621+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/16 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.16.2c9ae198-6fc4-4beb-9ba0-fbbb576f5358.tmp
[2024-06-27T07:54:48.678+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.16.2c9ae198-6fc4-4beb-9ba0-fbbb576f5358.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/16
[2024-06-27T07:54:48.681+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:54:47.743Z",
  "batchId" : 16,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.070663811563169,
  "durationMs" : {
    "addBatch" : 735,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 48,
    "triggerExecution" : 934,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.070663811563169,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:54:55.159+0000] {docker.py:436} INFO - 24/06/27 07:54:55 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:54:58.682+0000] {docker.py:436} INFO - 24/06/27 07:54:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:08.684+0000] {docker.py:436} INFO - 24/06/27 07:55:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:18.685+0000] {docker.py:436} INFO - 24/06/27 07:55:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:28.688+0000] {docker.py:436} INFO - 24/06/27 07:55:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:38.688+0000] {docker.py:436} INFO - 24/06/27 07:55:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:48.697+0000] {docker.py:436} INFO - 24/06/27 07:55:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:58.702+0000] {docker.py:436} INFO - 24/06/27 07:55:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:08.711+0000] {docker.py:436} INFO - 24/06/27 07:56:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:18.714+0000] {docker.py:436} INFO - 24/06/27 07:56:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:28.722+0000] {docker.py:436} INFO - 24/06/27 07:56:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:38.726+0000] {docker.py:436} INFO - 24/06/27 07:56:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:48.737+0000] {docker.py:436} INFO - 24/06/27 07:56:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:58.739+0000] {docker.py:436} INFO - 24/06/27 07:56:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:08.744+0000] {docker.py:436} INFO - 24/06/27 07:57:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:18.752+0000] {docker.py:436} INFO - 24/06/27 07:57:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:28.760+0000] {docker.py:436} INFO - 24/06/27 07:57:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:38.770+0000] {docker.py:436} INFO - 24/06/27 07:57:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:48.777+0000] {docker.py:436} INFO - 24/06/27 07:57:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:58.782+0000] {docker.py:436} INFO - 24/06/27 07:57:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:08.783+0000] {docker.py:436} INFO - 24/06/27 07:58:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:18.784+0000] {docker.py:436} INFO - 24/06/27 07:58:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:28.789+0000] {docker.py:436} INFO - 24/06/27 07:58:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:38.790+0000] {docker.py:436} INFO - 24/06/27 07:58:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:47.976+0000] {docker.py:436} INFO - 24/06/27 07:58:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/17 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.17.3eb85b80-4a1a-43fc-8a1c-51d49320a667.tmp
[2024-06-27T07:58:48.019+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.17.3eb85b80-4a1a-43fc-8a1c-51d49320a667.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/17
[2024-06-27T07:58:48.019+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1719475127966,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:58:48.039+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.044+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.047+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.059+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.076+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:58:48.077+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:58:48.078+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:58:48.079+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:58:48.079+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:58:48.080+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:58:48.080+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[53] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:58:48.083+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:58:48.085+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:58:48.086+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:58:48.090+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:58:48.091+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[53] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:58:48.091+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-06-27T07:58:48.092+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:58:48.101+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2024-06-27T07:58:48.122+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 42 for partition store_source_data-0
[2024-06-27T07:58:48.128+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:58:48.631+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:58:48.632+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:58:48.633+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=43, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:58:48.633+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:58:48.634+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2024-06-27T07:58:48.635+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2028 bytes result sent to driver
[2024-06-27T07:58:48.636+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 544 ms on localhost (executor driver) (1/1)
[2024-06-27T07:58:48.636+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-06-27T07:58:48.637+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.556 s
[2024-06-27T07:58:48.637+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:58:48.638+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-06-27T07:58:48.638+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.560727 s
[2024-06-27T07:58:48.639+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:58:48.639+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 17
[2024-06-27T07:58:48.639+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:58:48.665+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:58:48.665+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:58:48.675+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/17 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.17.6a550e68-fbdf-40d1-8d6e-e92495375df0.tmp
[2024-06-27T07:58:48.705+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.17.6a550e68-fbdf-40d1-8d6e-e92495375df0.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/17
[2024-06-27T07:58:48.707+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:58:47.965Z",
  "batchId" : 17,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.3531799729364005,
  "durationMs" : {
    "addBatch" : 632,
    "commitOffsets" : 39,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 14,
    "triggerExecution" : 739,
    "walCommit" : 53
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.3531799729364005,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:58:58.711+0000] {docker.py:436} INFO - 24/06/27 07:58:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:00.775+0000] {docker.py:436} INFO - 24/06/27 07:59:00 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:59:08.721+0000] {docker.py:436} INFO - 24/06/27 07:59:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:18.721+0000] {docker.py:436} INFO - 24/06/27 07:59:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:28.728+0000] {docker.py:436} INFO - 24/06/27 07:59:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:34.855+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/18 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.18.3fefb70a-068a-411b-8dc4-6b9f5ade75f6.tmp
[2024-06-27T07:59:34.900+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.18.3fefb70a-068a-411b-8dc4-6b9f5ade75f6.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/18
24/06/27 07:59:34 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1719475174843,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:59:34.923+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.926+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.938+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.940+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.975+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.983+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:35.032+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:59:35.046+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:59:35 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:59:35 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:59:35 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:59:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:59:35.051+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 07:59:35 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T07:59:35.056+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T07:59:35.061+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:59:35.065+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:59:35.067+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:59:35.068+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-06-27T07:59:35.071+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:59:35.077+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2024-06-27T07:59:35.094+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 43 for partition store_source_data-0
[2024-06-27T07:59:35.098+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:59:35.618+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:59:35.631+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:59:35.632+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:59:35.635+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:59:35.640+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2024-06-27T07:59:35.650+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2028 bytes result sent to driver
[2024-06-27T07:59:35.658+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 588 ms on localhost (executor driver) (1/1)
[2024-06-27T07:59:35.659+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-06-27T07:59:35.663+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
[2024-06-27T07:59:35.665+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:59:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-06-27T07:59:35.667+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.621846 s
[2024-06-27T07:59:35.668+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 18
-------------------------------------------
[2024-06-27T07:59:35.668+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:59:35.773+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T07:59:35.781+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:59:35.833+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/18 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.18.042b1af2-dd9c-4bcc-9a2e-60a93048c838.tmp
[2024-06-27T07:59:35.949+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.18.042b1af2-dd9c-4bcc-9a2e-60a93048c838.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/18
[2024-06-27T07:59:35.949+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T07:59:34.842Z",
  "batchId" : 18,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9049773755656109,
  "durationMs" : {
    "addBatch" : 852,
    "commitOffsets" : 164,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 34,
    "triggerExecution" : 1105,
    "walCommit" : 54
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9049773755656109,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:59:41.530+0000] {docker.py:436} INFO - 24/06/27 07:59:41 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T07:59:45.949+0000] {docker.py:436} INFO - 24/06/27 07:59:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:55.957+0000] {docker.py:436} INFO - 24/06/27 07:59:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:05.960+0000] {docker.py:436} INFO - 24/06/27 08:00:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:15.961+0000] {docker.py:436} INFO - 24/06/27 08:00:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:25.803+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/19 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.19.2ae3353c-5020-439f-80d7-8f49745c5ce0.tmp
[2024-06-27T08:00:25.850+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.19.2ae3353c-5020-439f-80d7-8f49745c5ce0.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/19
[2024-06-27T08:00:25.851+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1719475225788,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:00:25.863+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.864+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.876+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.878+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.926+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.930+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.963+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:00:25.964+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:00:25.966+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:00:25.967+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:00:25.967+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:00:25.972+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO DAGScheduler: Missing parents: List()
24/06/27 08:00:25 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:00:25.975+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:00:25.981+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:00:25.982+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:00:25.983+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:00:25.983+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:00:25.984+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-06-27T08:00:25.986+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:00:25.986+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2024-06-27T08:00:26.019+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 44 for partition store_source_data-0
[2024-06-27T08:00:26.025+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:00:26.537+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:00:26.541+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:00:26.574+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:00:26 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2024-06-27T08:00:26.575+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2028 bytes result sent to driver
[2024-06-27T08:00:26.586+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 601 ms on localhost (executor driver) (1/1)
[2024-06-27T08:00:26.596+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-06-27T08:00:26.609+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 0.636 s
24/06/27 08:00:26 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
24/06/27 08:00:26 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.642193 s
24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:00:26.615+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 19
[2024-06-27T08:00:26.616+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:00:26.711+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:00:26.715+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:00:26.740+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/19 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.19.07992b49-42d6-4fde-b8d4-2a33b5dd880a.tmp
[2024-06-27T08:00:26.833+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.19.07992b49-42d6-4fde-b8d4-2a33b5dd880a.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/19
[2024-06-27T08:00:26.834+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:00:25.786Z",
  "batchId" : 19,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9560229445506692,
  "durationMs" : {
    "addBatch" : 849,
    "commitOffsets" : 118,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 15,
    "triggerExecution" : 1046,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9560229445506692,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:00:36.842+0000] {docker.py:436} INFO - 24/06/27 08:00:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:37.348+0000] {docker.py:436} INFO - 24/06/27 08:00:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:00:46.846+0000] {docker.py:436} INFO - 24/06/27 08:00:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:56.851+0000] {docker.py:436} INFO - 24/06/27 08:00:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:06.863+0000] {docker.py:436} INFO - 24/06/27 08:01:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:16.868+0000] {docker.py:436} INFO - 24/06/27 08:01:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:26.869+0000] {docker.py:436} INFO - 24/06/27 08:01:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:36.874+0000] {docker.py:436} INFO - 24/06/27 08:01:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:46.878+0000] {docker.py:436} INFO - 24/06/27 08:01:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:56.883+0000] {docker.py:436} INFO - 24/06/27 08:01:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:06.886+0000] {docker.py:436} INFO - 24/06/27 08:02:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:16.891+0000] {docker.py:436} INFO - 24/06/27 08:02:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:26.896+0000] {docker.py:436} INFO - 24/06/27 08:02:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:36.899+0000] {docker.py:436} INFO - 24/06/27 08:02:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:46.900+0000] {docker.py:436} INFO - 24/06/27 08:02:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:56.907+0000] {docker.py:436} INFO - 24/06/27 08:02:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:04.440+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/20 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.20.78899895-bf3a-423e-abbe-6c90eb28a3b6.tmp
[2024-06-27T08:03:04.491+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.20.78899895-bf3a-423e-abbe-6c90eb28a3b6.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/20
24/06/27 08:03:04 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1719475384425,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:03:04.506+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.516+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.533+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.537+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.558+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.559+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.568+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:03:04.570+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:03:04.572+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:03:04.572+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:03:04.573+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:03:04.574+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:03:04.576+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[62] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:03:04.579+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:03:04.582+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:03:04.583+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:03:04.584+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:03:04.585+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[62] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:03:04.586+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-06-27T08:03:04.587+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:03:04.601+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2024-06-27T08:03:04.609+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 45 for partition store_source_data-0
[2024-06-27T08:03:04.625+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:03:05.132+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:03:05.133+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:03:05.134+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:03:05.134+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2024-06-27T08:03:05.136+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2028 bytes result sent to driver
[2024-06-27T08:03:05.137+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 551 ms on localhost (executor driver) (1/1)
[2024-06-27T08:03:05.137+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-06-27T08:03:05.138+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
[2024-06-27T08:03:05.141+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:03:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-06-27T08:03:05.143+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.569881 s
[2024-06-27T08:03:05.144+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:03:05.144+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 20
-------------------------------------------
[2024-06-27T08:03:05.175+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:03:05.176+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:03:05.226+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/20 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.20.4de0ea4f-dcfa-4cd1-a867-9246fcfabd6f.tmp
[2024-06-27T08:03:05.333+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.20.4de0ea4f-dcfa-4cd1-a867-9246fcfabd6f.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/20
[2024-06-27T08:03:05.336+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:03:04.423Z",
  "batchId" : 20,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.0989010989010988,
  "durationMs" : {
    "addBatch" : 652,
    "commitOffsets" : 157,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 34,
    "triggerExecution" : 910,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.0989010989010988,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:03:06.351+0000] {docker.py:436} INFO - 24/06/27 08:03:06 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:03:15.339+0000] {docker.py:436} INFO - 24/06/27 08:03:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:25.346+0000] {docker.py:436} INFO - 24/06/27 08:03:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:35.351+0000] {docker.py:436} INFO - 24/06/27 08:03:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:45.355+0000] {docker.py:436} INFO - 24/06/27 08:03:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:55.358+0000] {docker.py:436} INFO - 24/06/27 08:03:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:05.366+0000] {docker.py:436} INFO - 24/06/27 08:04:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:15.374+0000] {docker.py:436} INFO - 24/06/27 08:04:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:25.375+0000] {docker.py:436} INFO - 24/06/27 08:04:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:33.009+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/21 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.21.ff411844-2349-4887-bdc7-c3fb10e94c71.tmp
[2024-06-27T08:04:33.057+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.21.ff411844-2349-4887-bdc7-c3fb10e94c71.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/21
24/06/27 08:04:33 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1719475472977,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:04:33.076+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.082+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.101+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.104+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.118+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.120+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.141+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:04:33.142+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:04:33.143+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:04:33.143+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:04:33.144+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:04:33.144+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:04:33.145+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:04:33.152+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:04:33.152+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:04:33.154+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:04:33.160+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:04:33.161+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:04:33.166+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
24/06/27 08:04:33 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:04:33.167+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2024-06-27T08:04:33.194+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 46 for partition store_source_data-0
[2024-06-27T08:04:33.199+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:04:33.703+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:04:33.704+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:04:33.705+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=47, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:04:33.707+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:04:33 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)
[2024-06-27T08:04:33.709+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2028 bytes result sent to driver
[2024-06-27T08:04:33.710+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 547 ms on localhost (executor driver) (1/1)
[2024-06-27T08:04:33.711+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-06-27T08:04:33.713+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 0.564 s
24/06/27 08:04:33 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-06-27T08:04:33.713+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.570617 s
[2024-06-27T08:04:33.714+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:04:33.715+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 21
-------------------------------------------
[2024-06-27T08:04:33.757+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:04:33.757+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:04:33.775+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/21 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.21.a5e933d9-9074-424c-91d0-e7b87e994b72.tmp
[2024-06-27T08:04:33.820+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.21.a5e933d9-9074-424c-91d0-e7b87e994b72.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/21
[2024-06-27T08:04:33.822+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:04:32.971Z",
  "batchId" : 21,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 55.55555555555556,
  "processedRowsPerSecond" : 1.1778563015312131,
  "durationMs" : {
    "addBatch" : 668,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 31,
    "triggerExecution" : 849,
    "walCommit" : 80
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 55.55555555555556,
    "processedRowsPerSecond" : 1.1778563015312131,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:04:42.885+0000] {docker.py:436} INFO - 24/06/27 08:04:42 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:04:43.823+0000] {docker.py:436} INFO - 24/06/27 08:04:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:53.822+0000] {docker.py:436} INFO - 24/06/27 08:04:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:03.830+0000] {docker.py:436} INFO - 24/06/27 08:05:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:13.833+0000] {docker.py:436} INFO - 24/06/27 08:05:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:23.837+0000] {docker.py:436} INFO - 24/06/27 08:05:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:33.842+0000] {docker.py:436} INFO - 24/06/27 08:05:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:43.843+0000] {docker.py:436} INFO - 24/06/27 08:05:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:53.845+0000] {docker.py:436} INFO - 24/06/27 08:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:03.855+0000] {docker.py:436} INFO - 24/06/27 08:06:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:13.863+0000] {docker.py:436} INFO - 24/06/27 08:06:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:23.868+0000] {docker.py:436} INFO - 24/06/27 08:06:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:33.879+0000] {docker.py:436} INFO - 24/06/27 08:06:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:43.889+0000] {docker.py:436} INFO - 24/06/27 08:06:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:53.892+0000] {docker.py:436} INFO - 24/06/27 08:06:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:03.897+0000] {docker.py:436} INFO - 24/06/27 08:07:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:13.896+0000] {docker.py:436} INFO - 24/06/27 08:07:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:16.661+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/22 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.22.89d6de73-1935-4c5c-a38b-222409c17b5d.tmp
[2024-06-27T08:07:16.730+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.22.89d6de73-1935-4c5c-a38b-222409c17b5d.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/22
24/06/27 08:07:16 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1719475636649,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:07:16.741+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.753+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.766+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.769+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.782+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.790+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.808+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:07:16.809+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:07:16.816+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:07:16 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:07:16 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:07:16 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:07:16.817+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:07:16.819+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:07:16.821+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:07:16.822+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:07:16.824+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:07:16.826+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:07:16.827+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-06-27T08:07:16.833+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:07:16.839+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2024-06-27T08:07:16.879+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 47 for partition store_source_data-0
[2024-06-27T08:07:16.882+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:07:17.384+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:07:17.386+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:07:17.387+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:07:17 INFO DataWritingSparkTask: Committed partition 0 (task 22, attempt 0, stage 22.0)
[2024-06-27T08:07:17.388+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2028 bytes result sent to driver
[2024-06-27T08:07:17.389+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 557 ms on localhost (executor driver) (1/1)
[2024-06-27T08:07:17.390+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/06/27 08:07:17 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 0.573 s
[2024-06-27T08:07:17.391+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:07:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
24/06/27 08:07:17 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.581815 s
24/06/27 08:07:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:07:17.392+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 22
-------------------------------------------
[2024-06-27T08:07:17.416+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:07:17.416+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:07:17.428+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/22 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.22.e01d5fb4-44a5-487d-9600-8b58710dd4e7.tmp
[2024-06-27T08:07:17.503+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.22.e01d5fb4-44a5-487d-9600-8b58710dd4e7.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/22
[2024-06-27T08:07:17.504+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:07:16.647Z",
  "batchId" : 22,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.17096018735363,
  "durationMs" : {
    "addBatch" : 657,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 33,
    "triggerExecution" : 854,
    "walCommit" : 76
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.17096018735363,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:07:23.049+0000] {docker.py:436} INFO - 24/06/27 08:07:23 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:07:27.508+0000] {docker.py:436} INFO - 24/06/27 08:07:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:37.518+0000] {docker.py:436} INFO - 24/06/27 08:07:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:47.528+0000] {docker.py:436} INFO - 24/06/27 08:07:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:57.532+0000] {docker.py:436} INFO - 24/06/27 08:07:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:07.536+0000] {docker.py:436} INFO - 24/06/27 08:08:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:17.545+0000] {docker.py:436} INFO - 24/06/27 08:08:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:27.545+0000] {docker.py:436} INFO - 24/06/27 08:08:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:37.554+0000] {docker.py:436} INFO - 24/06/27 08:08:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:47.557+0000] {docker.py:436} INFO - 24/06/27 08:08:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:57.558+0000] {docker.py:436} INFO - 24/06/27 08:08:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:58.592+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/23 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.23.366f4eb2-c311-426f-8ed3-70e47df39fee.tmp
[2024-06-27T08:08:58.751+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.23.366f4eb2-c311-426f-8ed3-70e47df39fee.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/23
[2024-06-27T08:08:58.752+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1719475738573,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:08:58.773+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.786+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.787+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.803+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.818+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.833+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:08:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:08:58 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:08:58 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:08:58 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:08:58 INFO DAGScheduler: Missing parents: List()
24/06/27 08:08:58 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:08:58.835+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:08:58.836+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:08:58.841+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:08:58.842+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:08:58.843+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:08:58.843+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2024-06-27T08:08:58.858+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:08:58.878+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2024-06-27T08:08:58.916+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 48 for partition store_source_data-0
[2024-06-27T08:08:58.920+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:08:59.422+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:08:59.423+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:08:59.423+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=49, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:08:59.424+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:08:59 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)
[2024-06-27T08:08:59.426+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2028 bytes result sent to driver
[2024-06-27T08:08:59.428+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 569 ms on localhost (executor driver) (1/1)
[2024-06-27T08:08:59.429+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-06-27T08:08:59.429+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.596 s
[2024-06-27T08:08:59.430+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:08:59.430+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2024-06-27T08:08:59.430+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.598344 s
[2024-06-27T08:08:59.431+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:08:59.431+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 23
-------------------------------------------
[2024-06-27T08:08:59.468+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:08:59.470+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:08:59.495+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/23 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.23.a8691c4b-f967-4b9e-b88b-199b2b723f9d.tmp
[2024-06-27T08:08:59.708+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.23.a8691c4b-f967-4b9e-b88b-199b2b723f9d.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/23
24/06/27 08:08:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:08:58.570Z",
  "batchId" : 23,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8818342151675486,
  "durationMs" : {
    "addBatch" : 699,
    "commitOffsets" : 232,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 22,
    "triggerExecution" : 1134,
    "walCommit" : 178
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8818342151675486,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:09:09.712+0000] {docker.py:436} INFO - 24/06/27 08:09:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:11.942+0000] {docker.py:436} INFO - 24/06/27 08:09:11 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:09:19.720+0000] {docker.py:436} INFO - 24/06/27 08:09:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:29.722+0000] {docker.py:436} INFO - 24/06/27 08:09:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:39.728+0000] {docker.py:436} INFO - 24/06/27 08:09:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:49.729+0000] {docker.py:436} INFO - 24/06/27 08:09:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:59.734+0000] {docker.py:436} INFO - 24/06/27 08:09:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:09.735+0000] {docker.py:436} INFO - 24/06/27 08:10:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:19.735+0000] {docker.py:436} INFO - 24/06/27 08:10:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:29.736+0000] {docker.py:436} INFO - 24/06/27 08:10:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:39.738+0000] {docker.py:436} INFO - 24/06/27 08:10:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:49.744+0000] {docker.py:436} INFO - 24/06/27 08:10:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:58.376+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/24 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.24.aa56ef0a-3b33-460e-94e5-48d7109d4fe1.tmp
[2024-06-27T08:10:58.439+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.24.aa56ef0a-3b33-460e-94e5-48d7109d4fe1.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/24
[2024-06-27T08:10:58.442+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1719475858358,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:10:58.460+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.460+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.475+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.486+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.564+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.565+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.583+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:10:58.584+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:10:58.591+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:10:58.591+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:10:58.592+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:10:58.593+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:10:58.595+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[74] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:10:58.606+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:10:58.612+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:10:58.622+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:10:58.629+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:10:58.629+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[74] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:10:58.631+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-06-27T08:10:58.641+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:10:58.641+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2024-06-27T08:10:58.676+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 49 for partition store_source_data-0
[2024-06-27T08:10:58.681+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:10:59.184+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:10:59.185+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:10:59.190+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=50, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:10:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:10:59 INFO DataWritingSparkTask: Committed partition 0 (task 24, attempt 0, stage 24.0)
[2024-06-27T08:10:59.191+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2028 bytes result sent to driver
[2024-06-27T08:10:59.193+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 560 ms on localhost (executor driver) (1/1)
[2024-06-27T08:10:59.194+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-06-27T08:10:59.195+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.595 s
[2024-06-27T08:10:59.197+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:10:59.198+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-06-27T08:10:59.200+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.615318 s
[2024-06-27T08:10:59.201+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:10:59.202+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:10:59.203+0000] {docker.py:436} INFO - Batch: 24
[2024-06-27T08:10:59.204+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:10:59.327+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:10:59.327+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:10:59.377+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/24 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.24.f636e37f-08bf-4b90-b41b-aa9dbc08a94e.tmp
[2024-06-27T08:10:59.413+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:10:59.459+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.24.f636e37f-08bf-4b90-b41b-aa9dbc08a94e.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/24
[2024-06-27T08:10:59.462+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:10:58.356Z",
  "batchId" : 24,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9066183136899365,
  "durationMs" : {
    "addBatch" : 872,
    "commitOffsets" : 132,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 12,
    "triggerExecution" : 1103,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9066183136899365,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:11:09.468+0000] {docker.py:436} INFO - 24/06/27 08:11:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:19.467+0000] {docker.py:436} INFO - 24/06/27 08:11:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:29.470+0000] {docker.py:436} INFO - 24/06/27 08:11:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:39.471+0000] {docker.py:436} INFO - 24/06/27 08:11:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:49.478+0000] {docker.py:436} INFO - 24/06/27 08:11:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:59.495+0000] {docker.py:436} INFO - 24/06/27 08:11:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:09.501+0000] {docker.py:436} INFO - 24/06/27 08:12:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:19.511+0000] {docker.py:436} INFO - 24/06/27 08:12:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:29.513+0000] {docker.py:436} INFO - 24/06/27 08:12:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:39.516+0000] {docker.py:436} INFO - 24/06/27 08:12:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:49.520+0000] {docker.py:436} INFO - 24/06/27 08:12:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:59.529+0000] {docker.py:436} INFO - 24/06/27 08:12:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:09.540+0000] {docker.py:436} INFO - 24/06/27 08:13:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:13.154+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/25 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.25.6850826d-9270-4c58-b4fd-13d7956596ed.tmp
[2024-06-27T08:13:13.305+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.25.6850826d-9270-4c58-b4fd-13d7956596ed.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/25
[2024-06-27T08:13:13.306+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1719475993133,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:13:13.327+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.350+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.351+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.396+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.495+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.511+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.552+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:13:13.553+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:13:13.630+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:13:13 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:13:13 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:13:13 INFO DAGScheduler: Missing parents: List()
24/06/27 08:13:13 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[77] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:13:13.631+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:13:13.632+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:13:13.637+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:13:13.638+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:13:13.639+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[77] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:13:13.641+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-06-27T08:13:13.644+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:13:13.693+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
[2024-06-27T08:13:13.840+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 50 for partition store_source_data-0
[2024-06-27T08:13:13.846+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:13:14.347+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:13:14.348+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:13:14.349+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:13:14.351+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:13:14 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 25.0)
[2024-06-27T08:13:14.354+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 2028 bytes result sent to driver
[2024-06-27T08:13:14.354+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 711 ms on localhost (executor driver) (1/1)
[2024-06-27T08:13:14.355+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-06-27T08:13:14.356+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 0.728 s
[2024-06-27T08:13:14.358+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:13:14.359+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-06-27T08:13:14.360+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.804419 s
[2024-06-27T08:13:14.360+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:13:14.361+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 25
-------------------------------------------
[2024-06-27T08:13:14.386+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:13:14.387+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:13:14.396+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/25 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.25.908ab415-c3e8-4ac7-8aa4-f681a00fd717.tmp
[2024-06-27T08:13:14.428+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.25.908ab415-c3e8-4ac7-8aa4-f681a00fd717.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/25
[2024-06-27T08:13:14.430+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:13:13.131Z",
  "batchId" : 25,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.7710100231303008,
  "durationMs" : {
    "addBatch" : 1043,
    "commitOffsets" : 41,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 38,
    "triggerExecution" : 1297,
    "walCommit" : 171
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.7710100231303008,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:13:18.570+0000] {docker.py:436} INFO - 24/06/27 08:13:18 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:13:24.441+0000] {docker.py:436} INFO - 24/06/27 08:13:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:34.442+0000] {docker.py:436} INFO - 24/06/27 08:13:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:44.447+0000] {docker.py:436} INFO - 24/06/27 08:13:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:54.450+0000] {docker.py:436} INFO - 24/06/27 08:13:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:04.458+0000] {docker.py:436} INFO - 24/06/27 08:14:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:14.466+0000] {docker.py:436} INFO - 24/06/27 08:14:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:24.472+0000] {docker.py:436} INFO - 24/06/27 08:14:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:34.475+0000] {docker.py:436} INFO - 24/06/27 08:14:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:44.477+0000] {docker.py:436} INFO - 24/06/27 08:14:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:54.481+0000] {docker.py:436} INFO - 24/06/27 08:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:04.486+0000] {docker.py:436} INFO - 24/06/27 08:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:14.486+0000] {docker.py:436} INFO - 24/06/27 08:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:24.494+0000] {docker.py:436} INFO - 24/06/27 08:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:34.503+0000] {docker.py:436} INFO - 24/06/27 08:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:44.512+0000] {docker.py:436} INFO - 24/06/27 08:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:54.514+0000] {docker.py:436} INFO - 24/06/27 08:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:03.437+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/26 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.26.694cf443-8e32-45c0-b4c1-cba4646672b8.tmp
[2024-06-27T08:16:03.514+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.26.694cf443-8e32-45c0-b4c1-cba4646672b8.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/26
[2024-06-27T08:16:03.515+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1719476163416,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:16:03.575+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.582+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.616+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.620+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.650+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.663+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:16:03.665+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:16:03.680+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:16:03.681+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:16:03.681+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:16:03.682+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:16:03.682+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:16:03.687+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:16:03.689+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:16:03.697+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:16:03.699+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
24/06/27 08:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:16:03.700+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2024-06-27T08:16:03.703+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:16:03.714+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
[2024-06-27T08:16:03.749+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 51 for partition store_source_data-0
[2024-06-27T08:16:03.767+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:16:04.269+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:16:04.270+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:16:04.271+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:16:04.273+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:16:04 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 26.0)
[2024-06-27T08:16:04.275+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 2028 bytes result sent to driver
[2024-06-27T08:16:04.277+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 576 ms on localhost (executor driver) (1/1)
[2024-06-27T08:16:04.277+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-06-27T08:16:04.278+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.595 s
[2024-06-27T08:16:04.279+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2024-06-27T08:16:04.279+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.613198 s
[2024-06-27T08:16:04.280+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:16:04.280+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 26
-------------------------------------------
[2024-06-27T08:16:04.300+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:16:04.301+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:16:04.324+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/26 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.26.02c80e15-0c5e-4ebd-9352-f096c5a9e5f4.tmp
[2024-06-27T08:16:04.446+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.26.02c80e15-0c5e-4ebd-9352-f096c5a9e5f4.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/26
[2024-06-27T08:16:04.449+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:16:03.411Z",
  "batchId" : 26,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.9661835748792271,
  "durationMs" : {
    "addBatch" : 709,
    "commitOffsets" : 146,
    "getBatch" : 1,
    "latestOffset" : 5,
    "queryPlanning" : 74,
    "triggerExecution" : 1035,
    "walCommit" : 99
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.9661835748792271,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:16:07.949+0000] {docker.py:436} INFO - 24/06/27 08:16:07 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:16:14.453+0000] {docker.py:436} INFO - 24/06/27 08:16:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:24.458+0000] {docker.py:436} INFO - 24/06/27 08:16:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:34.462+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:44.462+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:54.463+0000] {docker.py:436} INFO - 24/06/27 08:16:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:04.474+0000] {docker.py:436} INFO - 24/06/27 08:17:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:14.485+0000] {docker.py:436} INFO - 24/06/27 08:17:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:22.960+0000] {docker.py:436} INFO - 24/06/27 08:17:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/27 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.27.abafa957-71fa-42c6-a3d3-99797049dade.tmp
[2024-06-27T08:17:23.023+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.27.abafa957-71fa-42c6-a3d3-99797049dade.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/27
24/06/27 08:17:23 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1719476242943,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:17:23.047+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.063+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.109+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.135+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.143+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.159+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:17:23.162+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:17:23.167+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:17:23 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:17:23 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:17:23 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:17:23.176+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[83] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:17:23.191+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.194+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.202+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:17:23.207+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:17:23.207+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[83] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:17:23 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-06-27T08:17:23.208+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:17:23.208+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
[2024-06-27T08:17:23.223+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 52 for partition store_source_data-0
[2024-06-27T08:17:23.229+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:17:23.734+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:17:23.751+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=53, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:17:23.752+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:17:23.756+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DataWritingSparkTask: Committed partition 0 (task 27, attempt 0, stage 27.0)
[2024-06-27T08:17:23.760+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 2028 bytes result sent to driver
[2024-06-27T08:17:23.762+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 559 ms on localhost (executor driver) (1/1)
[2024-06-27T08:17:23.763+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-06-27T08:17:23.764+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.582 s
[2024-06-27T08:17:23.764+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-06-27T08:17:23.765+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.600920 s
[2024-06-27T08:17:23.770+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:17:23.782+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:17:23.783+0000] {docker.py:436} INFO - Batch: 27
[2024-06-27T08:17:23.783+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:17:23.828+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:17:23.828+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:17:23.846+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/27 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.27.12c1d1ab-d41d-45d1-a1e9-1673ba276fee.tmp
[2024-06-27T08:17:23.885+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.27.12c1d1ab-d41d-45d1-a1e9-1673ba276fee.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/27
[2024-06-27T08:17:23.886+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:17:22.942Z",
  "batchId" : 27,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.0604453870625663,
  "durationMs" : {
    "addBatch" : 772,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 34,
    "triggerExecution" : 943,
    "walCommit" : 80
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.0604453870625663,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:17:33.901+0000] {docker.py:436} INFO - 24/06/27 08:17:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:35.403+0000] {docker.py:436} INFO - 24/06/27 08:17:35 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:17:43.904+0000] {docker.py:436} INFO - 24/06/27 08:17:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:53.907+0000] {docker.py:436} INFO - 24/06/27 08:17:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:03.910+0000] {docker.py:436} INFO - 24/06/27 08:18:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:13.913+0000] {docker.py:436} INFO - 24/06/27 08:18:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:22.001+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/28 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.28.d42dffd7-f45c-4625-b321-b7056e264757.tmp
[2024-06-27T08:18:22.074+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.28.d42dffd7-f45c-4625-b321-b7056e264757.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/28
24/06/27 08:18:22 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1719476301980,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:18:22.112+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.126+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.152+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.157+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.185+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.191+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.209+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:18:22.209+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:18:22.210+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:18:22.210+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Final stage: ResultStage 28 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:18:22.211+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:18:22.211+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:18:22.212+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:18:22.218+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
24/06/27 08:18:22 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
24/06/27 08:18:22 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:18:22.219+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:18:22.219+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:18:22.220+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2024-06-27T08:18:22.221+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:18:22.221+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
[2024-06-27T08:18:22.245+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 53 for partition store_source_data-0
[2024-06-27T08:18:22.253+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:18:22.755+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:18:22.755+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:18:22.756+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=54, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:18:22.758+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:18:22 INFO DataWritingSparkTask: Committed partition 0 (task 28, attempt 0, stage 28.0)
[2024-06-27T08:18:22.759+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 2028 bytes result sent to driver
[2024-06-27T08:18:22.762+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 541 ms on localhost (executor driver) (1/1)
24/06/27 08:18:22 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-06-27T08:18:22.765+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: ResultStage 28 (start at NativeMethodAccessorImpl.java:0) finished in 0.552 s
[2024-06-27T08:18:22.765+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:18:22.767+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2024-06-27T08:18:22.768+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.559217 s
[2024-06-27T08:18:22.770+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:18:22.771+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 28
-------------------------------------------
[2024-06-27T08:18:22.861+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:18:22.862+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:18:22.883+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/28 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.28.5db7b43d-5868-42dc-91f4-abff4dea8439.tmp
[2024-06-27T08:18:22.994+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.28.5db7b43d-5868-42dc-91f4-abff4dea8439.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/28
[2024-06-27T08:18:22.995+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:18:21.979Z",
  "batchId" : 28,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9881422924901185,
  "durationMs" : {
    "addBatch" : 732,
    "commitOffsets" : 127,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 58,
    "triggerExecution" : 1012,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9881422924901185,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:18:30.869+0000] {docker.py:436} INFO - 24/06/27 08:18:30 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:18:32.994+0000] {docker.py:436} INFO - 24/06/27 08:18:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:42.994+0000] {docker.py:436} INFO - 24/06/27 08:18:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:52.999+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:03.006+0000] {docker.py:436} INFO - 24/06/27 08:19:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:13.016+0000] {docker.py:436} INFO - 24/06/27 08:19:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:23.018+0000] {docker.py:436} INFO - 24/06/27 08:19:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:33.025+0000] {docker.py:436} INFO - 24/06/27 08:19:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:43.028+0000] {docker.py:436} INFO - 24/06/27 08:19:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:53.030+0000] {docker.py:436} INFO - 24/06/27 08:19:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:58.018+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/29 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.29.bd69caf5-d0cf-40f4-a235-3c8541c7f398.tmp
[2024-06-27T08:19:58.189+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.29.bd69caf5-d0cf-40f4-a235-3c8541c7f398.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/29
24/06/27 08:19:58 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1719476397987,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:19:58.232+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.252+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.261+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.301+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:19:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:19:58.302+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:19:58.303+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:19:58.303+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:19:58.304+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:19:58.305+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[89] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:19:58.309+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.309+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.312+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:19:58.313+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:19:58.325+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[89] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:19:58 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
24/06/27 08:19:58 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes) 
24/06/27 08:19:58 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
[2024-06-27T08:19:58.333+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 54 for partition store_source_data-0
24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:19:58.856+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:19:58.874+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=55, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:19:58.894+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:19:58 INFO DataWritingSparkTask: Committed partition 0 (task 29, attempt 0, stage 29.0)
24/06/27 08:19:58 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 2028 bytes result sent to driver
24/06/27 08:19:58 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 574 ms on localhost (executor driver) (1/1)
24/06/27 08:19:58 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2024-06-27T08:19:58.895+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.590 s
[2024-06-27T08:19:58.896+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:19:58.897+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2024-06-27T08:19:58.898+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.596297 s
[2024-06-27T08:19:58.905+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:19:58.906+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:19:58.906+0000] {docker.py:436} INFO - Batch: 29
[2024-06-27T08:19:58.906+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:19:59.044+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:19:59.044+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:19:59.062+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/29 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.29.d6d1a6a0-6d96-45a5-a30a-91bfcf01d177.tmp
[2024-06-27T08:19:59.195+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.29.d6d1a6a0-6d96-45a5-a30a-91bfcf01d177.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/29
[2024-06-27T08:19:59.199+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:19:57.978Z",
  "batchId" : 29,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8203445447087776,
  "durationMs" : {
    "addBatch" : 809,
    "commitOffsets" : 155,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 44,
    "triggerExecution" : 1219,
    "walCommit" : 197
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8203445447087776,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:20:09.205+0000] {docker.py:436} INFO - 24/06/27 08:20:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:09.483+0000] {docker.py:436} INFO - 24/06/27 08:20:09 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:20:19.210+0000] {docker.py:436} INFO - 24/06/27 08:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:29.213+0000] {docker.py:436} INFO - 24/06/27 08:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:39.217+0000] {docker.py:436} INFO - 24/06/27 08:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:49.226+0000] {docker.py:436} INFO - 24/06/27 08:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:59.229+0000] {docker.py:436} INFO - 24/06/27 08:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:09.228+0000] {docker.py:436} INFO - 24/06/27 08:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:19.228+0000] {docker.py:436} INFO - 24/06/27 08:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:29.228+0000] {docker.py:436} INFO - 24/06/27 08:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:39.235+0000] {docker.py:436} INFO - 24/06/27 08:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:49.245+0000] {docker.py:436} INFO - 24/06/27 08:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:59.248+0000] {docker.py:436} INFO - 24/06/27 08:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:09.258+0000] {docker.py:436} INFO - 24/06/27 08:22:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:19.257+0000] {docker.py:436} INFO - 24/06/27 08:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:29.264+0000] {docker.py:436} INFO - 24/06/27 08:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:39.265+0000] {docker.py:436} INFO - 24/06/27 08:22:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:49.272+0000] {docker.py:436} INFO - 24/06/27 08:22:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:59.283+0000] {docker.py:436} INFO - 24/06/27 08:22:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:09.293+0000] {docker.py:436} INFO - 24/06/27 08:23:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:19.301+0000] {docker.py:436} INFO - 24/06/27 08:23:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:29.304+0000] {docker.py:436} INFO - 24/06/27 08:23:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:39.304+0000] {docker.py:436} INFO - 24/06/27 08:23:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:49.317+0000] {docker.py:436} INFO - 24/06/27 08:23:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:52.998+0000] {docker.py:436} INFO - 24/06/27 08:23:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/30 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.30.7f90b63a-8e11-4c79-afa7-75fa93ca815f.tmp
[2024-06-27T08:23:53.224+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.30.7f90b63a-8e11-4c79-afa7-75fa93ca815f.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/30
[2024-06-27T08:23:53.225+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1719476632974,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:23:53.254+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.265+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.315+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.318+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.392+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.393+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.415+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:23:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:23:53 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:23:53 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:23:53 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:23:53 INFO DAGScheduler: Missing parents: List()
24/06/27 08:23:53 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:23:53.416+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.418+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.424+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:23:53.425+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:23:53.426+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:23:53.426+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2024-06-27T08:23:53.427+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:23:53.427+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
[2024-06-27T08:23:53.455+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 55 for partition store_source_data-0
[2024-06-27T08:23:53.470+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:23:53.966+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:23:53.968+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=56, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:23:53.977+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:23:53.978+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DataWritingSparkTask: Committed partition 0 (task 30, attempt 0, stage 30.0)
[2024-06-27T08:23:53.983+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 2028 bytes result sent to driver
[2024-06-27T08:23:53.984+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 562 ms on localhost (executor driver) (1/1)
[2024-06-27T08:23:53.985+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2024-06-27T08:23:53.985+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.572 s
[2024-06-27T08:23:53.987+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:23:53.987+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2024-06-27T08:23:53.988+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.576608 s
[2024-06-27T08:23:53.989+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:23:53.990+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:53.990+0000] {docker.py:436} INFO - Batch: 30
[2024-06-27T08:23:53.990+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.078+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:23:54.078+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:23:54.118+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/30 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.30.eb00a59f-c085-4482-89f9-029a3921162b.tmp
[2024-06-27T08:23:54.201+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.30.eb00a59f-c085-4482-89f9-029a3921162b.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/30
[2024-06-27T08:23:54.203+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:23:52.964Z",
  "batchId" : 30,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.8084074373484236,
  "durationMs" : {
    "addBatch" : 804,
    "commitOffsets" : 122,
    "getBatch" : 1,
    "latestOffset" : 9,
    "queryPlanning" : 49,
    "triggerExecution" : 1237,
    "walCommit" : 250
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.8084074373484236,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:24:04.207+0000] {docker.py:436} INFO - 24/06/27 08:24:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:08.397+0000] {docker.py:436} INFO - 24/06/27 08:24:08 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:24:14.208+0000] {docker.py:436} INFO - 24/06/27 08:24:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:24.220+0000] {docker.py:436} INFO - 24/06/27 08:24:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:34.224+0000] {docker.py:436} INFO - 24/06/27 08:24:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:44.229+0000] {docker.py:436} INFO - 24/06/27 08:24:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:54.238+0000] {docker.py:436} INFO - 24/06/27 08:24:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:04.244+0000] {docker.py:436} INFO - 24/06/27 08:25:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:14.252+0000] {docker.py:436} INFO - 24/06/27 08:25:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:24.255+0000] {docker.py:436} INFO - 24/06/27 08:25:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:34.263+0000] {docker.py:436} INFO - 24/06/27 08:25:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:44.267+0000] {docker.py:436} INFO - 24/06/27 08:25:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:54.278+0000] {docker.py:436} INFO - 24/06/27 08:25:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:04.284+0000] {docker.py:436} INFO - 24/06/27 08:26:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:14.295+0000] {docker.py:436} INFO - 24/06/27 08:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:24.304+0000] {docker.py:436} INFO - 24/06/27 08:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:34.314+0000] {docker.py:436} INFO - 24/06/27 08:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:44.319+0000] {docker.py:436} INFO - 24/06/27 08:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:54.327+0000] {docker.py:436} INFO - 24/06/27 08:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:04.331+0000] {docker.py:436} INFO - 24/06/27 08:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:14.334+0000] {docker.py:436} INFO - 24/06/27 08:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:24.342+0000] {docker.py:436} INFO - 24/06/27 08:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:34.352+0000] {docker.py:436} INFO - 24/06/27 08:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:44.361+0000] {docker.py:436} INFO - 24/06/27 08:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:54.361+0000] {docker.py:436} INFO - 24/06/27 08:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:04.362+0000] {docker.py:436} INFO - 24/06/27 08:28:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:14.364+0000] {docker.py:436} INFO - 24/06/27 08:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:24.373+0000] {docker.py:436} INFO - 24/06/27 08:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:34.377+0000] {docker.py:436} INFO - 24/06/27 08:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:44.383+0000] {docker.py:436} INFO - 24/06/27 08:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:54.392+0000] {docker.py:436} INFO - 24/06/27 08:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:01.905+0000] {docker.py:436} INFO - 24/06/27 08:29:01 INFO Metrics: Metrics scheduler closed
24/06/27 08:29:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/27 08:29:01 INFO Metrics: Metrics reporters closed
[2024-06-27T08:29:01.912+0000] {docker.py:436} INFO - 24/06/27 08:29:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-2 unregistered
[2024-06-27T08:29:04.396+0000] {docker.py:436} INFO - 24/06/27 08:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:14.398+0000] {docker.py:436} INFO - 24/06/27 08:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:24.399+0000] {docker.py:436} INFO - 24/06/27 08:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:34.402+0000] {docker.py:436} INFO - 24/06/27 08:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:44.401+0000] {docker.py:436} INFO - 24/06/27 08:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:54.410+0000] {docker.py:436} INFO - 24/06/27 08:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:04.420+0000] {docker.py:436} INFO - 24/06/27 08:30:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:14.430+0000] {docker.py:436} INFO - 24/06/27 08:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:23.699+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/31 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.31.682f3e82-08bd-4652-9bf5-08f3f576a5ee.tmp
[2024-06-27T08:30:23.830+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.31.682f3e82-08bd-4652-9bf5-08f3f576a5ee.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/31
24/06/27 08:30:23 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1719477023660,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:30:23.881+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.898+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.905+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.928+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.939+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.957+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:30:23.959+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:30:23.960+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:30:23.961+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Final stage: ResultStage 31 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:30:23.962+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:30:23.962+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:30:23.963+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[95] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:30:23.965+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:30:23.970+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:30:23.972+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:30:23.973+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:30:23.977+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[95] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:30:23.978+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2024-06-27T08:30:23.979+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:30:23.981+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
[2024-06-27T08:30:24.005+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:30:24.039+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:30:24.043+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:30:24.043+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka startTimeMs: 1719477024034
[2024-06-27T08:30:24.044+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:30:24.049+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 56 for partition store_source_data-0
[2024-06-27T08:30:24.140+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:30:24.289+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:30:24.780+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:30:24.792+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=57, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:30:24.797+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:30:24.798+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DataWritingSparkTask: Committed partition 0 (task 31, attempt 0, stage 31.0)
[2024-06-27T08:30:24.832+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 2028 bytes result sent to driver
[2024-06-27T08:30:24.849+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 854 ms on localhost (executor driver) (1/1)
[2024-06-27T08:30:24.850+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2024-06-27T08:30:24.851+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: ResultStage 31 (start at NativeMethodAccessorImpl.java:0) finished in 0.886 s
24/06/27 08:30:24 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:30:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
[2024-06-27T08:30:24.853+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.891780 s
24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:30:24.854+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 31
-------------------------------------------
[2024-06-27T08:30:24.922+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:30:24.922+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:30:25.002+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/31 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.31.3a3bdd7b-138c-4a23-81b9-a3e5490cf04a.tmp
[2024-06-27T08:30:25.140+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.31.3a3bdd7b-138c-4a23-81b9-a3e5490cf04a.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/31
[2024-06-27T08:30:25.141+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:30:23.652Z",
  "batchId" : 31,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6734006734006733,
  "durationMs" : {
    "addBatch" : 1052,
    "commitOffsets" : 214,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 42,
    "triggerExecution" : 1485,
    "walCommit" : 169
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6734006734006733,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:30:35.151+0000] {docker.py:436} INFO - 24/06/27 08:30:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:35.155+0000] {docker.py:436} INFO - 24/06/27 08:30:35 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:30:45.147+0000] {docker.py:436} INFO - 24/06/27 08:30:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:55.148+0000] {docker.py:436} INFO - 24/06/27 08:30:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:05.149+0000] {docker.py:436} INFO - 24/06/27 08:31:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:15.155+0000] {docker.py:436} INFO - 24/06/27 08:31:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:23.829+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/32 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.32.24012d18-4ad0-493f-a277-7902aef7b411.tmp
[2024-06-27T08:31:23.922+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.32.24012d18-4ad0-493f-a277-7902aef7b411.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/32
24/06/27 08:31:23 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1719477083809,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:31:23.971+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:23.981+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.003+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.008+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.042+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.060+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.083+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:31:24.084+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:31:24.085+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:31:24.088+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:31:24.089+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:31:24.090+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:31:24.092+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:31:24.098+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.100+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.102+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:31:24.103+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:31:24.104+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:31:24 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2024-06-27T08:31:24.106+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:31:24.122+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
[2024-06-27T08:31:24.222+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 57 for partition store_source_data-0
[2024-06-27T08:31:24.233+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:31:24.744+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:31:24.756+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=58, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:31:24.765+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:31:24.768+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 32
-------------------------------------------
[2024-06-27T08:31:24.769+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DataWritingSparkTask: Committed partition 0 (task 32, attempt 0, stage 32.0)
24/06/27 08:31:24 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 2028 bytes result sent to driver
24/06/27 08:31:24 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 660 ms on localhost (executor driver) (1/1)
24/06/27 08:31:24 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/06/27 08:31:24 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.674 s
24/06/27 08:31:24 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:31:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
24/06/27 08:31:24 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.683445 s
24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:31:24.807+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:31:24.807+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:31:24.850+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/32 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.32.5dc4f28c-c8ed-41c6-9288-f405b42fadf1.tmp
[2024-06-27T08:31:24.913+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.32.5dc4f28c-c8ed-41c6-9288-f405b42fadf1.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/32
24/06/27 08:31:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:31:23.802Z",
  "batchId" : 32,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.903342366757001,
  "durationMs" : {
    "addBatch" : 825,
    "commitOffsets" : 102,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 57,
    "triggerExecution" : 1107,
    "walCommit" : 116
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.903342366757001,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:31:34.170+0000] {docker.py:436} INFO - 24/06/27 08:31:34 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:31:34.926+0000] {docker.py:436} INFO - 24/06/27 08:31:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:44.924+0000] {docker.py:436} INFO - 24/06/27 08:31:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:54.930+0000] {docker.py:436} INFO - 24/06/27 08:31:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:04.932+0000] {docker.py:436} INFO - 24/06/27 08:32:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:14.940+0000] {docker.py:436} INFO - 24/06/27 08:32:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:24.945+0000] {docker.py:436} INFO - 24/06/27 08:32:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:34.946+0000] {docker.py:436} INFO - 24/06/27 08:32:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:44.949+0000] {docker.py:436} INFO - 24/06/27 08:32:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:54.955+0000] {docker.py:436} INFO - 24/06/27 08:32:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:04.960+0000] {docker.py:436} INFO - 24/06/27 08:33:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:14.964+0000] {docker.py:436} INFO - 24/06/27 08:33:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:24.973+0000] {docker.py:436} INFO - 24/06/27 08:33:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:34.975+0000] {docker.py:436} INFO - 24/06/27 08:33:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:44.979+0000] {docker.py:436} INFO - 24/06/27 08:33:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:54.986+0000] {docker.py:436} INFO - 24/06/27 08:33:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:04.988+0000] {docker.py:436} INFO - 24/06/27 08:34:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:15.000+0000] {docker.py:436} INFO - 24/06/27 08:34:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:25.010+0000] {docker.py:436} INFO - 24/06/27 08:34:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:35.015+0000] {docker.py:436} INFO - 24/06/27 08:34:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:45.022+0000] {docker.py:436} INFO - 24/06/27 08:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:55.028+0000] {docker.py:436} INFO - 24/06/27 08:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:05.035+0000] {docker.py:436} INFO - 24/06/27 08:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:15.038+0000] {docker.py:436} INFO - 24/06/27 08:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:25.042+0000] {docker.py:436} INFO - 24/06/27 08:35:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:35.045+0000] {docker.py:436} INFO - 24/06/27 08:35:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:45.047+0000] {docker.py:436} INFO - 24/06/27 08:35:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:55.053+0000] {docker.py:436} INFO - 24/06/27 08:35:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:05.058+0000] {docker.py:436} INFO - 24/06/27 08:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:15.061+0000] {docker.py:436} INFO - 24/06/27 08:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:25.070+0000] {docker.py:436} INFO - 24/06/27 08:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:35.072+0000] {docker.py:436} INFO - 24/06/27 08:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:45.078+0000] {docker.py:436} INFO - 24/06/27 08:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:50.178+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/33 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.33.679292dd-fcfb-4645-be56-d82a1f77d631.tmp
[2024-06-27T08:36:50.300+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.33.679292dd-fcfb-4645-be56-d82a1f77d631.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/33
24/06/27 08:36:50 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1719477410148,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:36:50.357+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.369+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.437+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.448+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.521+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.537+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.569+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:36:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:36:50.573+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:36:50.573+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:36:50.575+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:36:50.576+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:36:50.578+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:36:50.586+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.596+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.609+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:36:50.609+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:36:50.610+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:36:50.611+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2024-06-27T08:36:50.621+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:36:50.655+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
[2024-06-27T08:36:50.689+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 58 for partition store_source_data-0
[2024-06-27T08:36:50.704+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:36:51.218+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:36:51.219+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=59, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:36:51.221+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:36:51 INFO DataWritingSparkTask: Committed partition 0 (task 33, attempt 0, stage 33.0)
[2024-06-27T08:36:51.271+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 2071 bytes result sent to driver
[2024-06-27T08:36:51.271+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 649 ms on localhost (executor driver) (1/1)
24/06/27 08:36:51 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2024-06-27T08:36:51.286+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.698 s
[2024-06-27T08:36:51.286+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2024-06-27T08:36:51.287+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.718337 s
[2024-06-27T08:36:51.287+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 33
-------------------------------------------
[2024-06-27T08:36:51.287+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:36:51.341+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:36:51.346+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:36:51.400+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/33 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.33.4075ad29-6011-43ff-9b84-b8d1b04216ff.tmp
[2024-06-27T08:36:51.527+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.33.4075ad29-6011-43ff-9b84-b8d1b04216ff.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/33
24/06/27 08:36:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:36:50.143Z",
  "batchId" : 33,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7246376811594204,
  "durationMs" : {
    "addBatch" : 971,
    "commitOffsets" : 181,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 73,
    "triggerExecution" : 1380,
    "walCommit" : 150
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7246376811594204,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:36:52.396+0000] {docker.py:436} INFO - 24/06/27 08:36:52 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:37:01.534+0000] {docker.py:436} INFO - 24/06/27 08:37:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:11.535+0000] {docker.py:436} INFO - 24/06/27 08:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:21.547+0000] {docker.py:436} INFO - 24/06/27 08:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:31.549+0000] {docker.py:436} INFO - 24/06/27 08:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:41.555+0000] {docker.py:436} INFO - 24/06/27 08:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:51.556+0000] {docker.py:436} INFO - 24/06/27 08:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:01.559+0000] {docker.py:436} INFO - 24/06/27 08:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:11.568+0000] {docker.py:436} INFO - 24/06/27 08:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:21.570+0000] {docker.py:436} INFO - 24/06/27 08:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:31.573+0000] {docker.py:436} INFO - 24/06/27 08:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:41.583+0000] {docker.py:436} INFO - 24/06/27 08:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:51.589+0000] {docker.py:436} INFO - 24/06/27 08:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:01.599+0000] {docker.py:436} INFO - 24/06/27 08:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:11.600+0000] {docker.py:436} INFO - 24/06/27 08:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:21.605+0000] {docker.py:436} INFO - 24/06/27 08:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:31.612+0000] {docker.py:436} INFO - 24/06/27 08:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:41.614+0000] {docker.py:436} INFO - 24/06/27 08:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:51.618+0000] {docker.py:436} INFO - 24/06/27 08:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:55.671+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/34 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.34.92266c77-0d7e-4049-b174-e91d9f21df31.tmp
[2024-06-27T08:39:55.807+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.34.92266c77-0d7e-4049-b174-e91d9f21df31.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/34
24/06/27 08:39:55 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1719477595646,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:39:55.826+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.837+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.849+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.867+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.883+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.894+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.940+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:39:56.005+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:39:56.016+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:39:56 INFO DAGScheduler: Final stage: ResultStage 34 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:39:56 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:39:56.017+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Missing parents: List()
24/06/27 08:39:56 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:39:56.034+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:39:56.077+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:39:56.078+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:39:56.079+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:39:56.112+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:39:56.115+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2024-06-27T08:39:56.128+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes) 
24/06/27 08:39:56 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
[2024-06-27T08:39:56.205+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 59 for partition store_source_data-0
[2024-06-27T08:39:56.205+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:39:56.707+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:39:56.716+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:39:56.725+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:39:56 INFO DataWritingSparkTask: Committed partition 0 (task 34, attempt 0, stage 34.0)
[2024-06-27T08:39:56.725+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 2028 bytes result sent to driver
[2024-06-27T08:39:56.728+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 606 ms on localhost (executor driver) (1/1)
[2024-06-27T08:39:56.728+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-06-27T08:39:56.730+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: ResultStage 34 (start at NativeMethodAccessorImpl.java:0) finished in 0.711 s
[2024-06-27T08:39:56.731+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:39:56.735+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
24/06/27 08:39:56 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.725909 s
[2024-06-27T08:39:56.736+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:39:56.737+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 34
-------------------------------------------
[2024-06-27T08:39:56.811+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:39:56.812+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:39:56.839+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/34 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.34.051e43f2-a335-4168-9756-ffc9e84050c7.tmp
[2024-06-27T08:39:57.094+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.34.051e43f2-a335-4168-9756-ffc9e84050c7.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/34
[2024-06-27T08:39:57.096+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:39:55.642Z",
  "batchId" : 34,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6887052341597797,
  "durationMs" : {
    "addBatch" : 977,
    "commitOffsets" : 276,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 35,
    "triggerExecution" : 1452,
    "walCommit" : 160
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6887052341597797,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:40:07.106+0000] {docker.py:436} INFO - 24/06/27 08:40:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:09.227+0000] {docker.py:436} INFO - 24/06/27 08:40:09 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:40:17.116+0000] {docker.py:436} INFO - 24/06/27 08:40:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:27.116+0000] {docker.py:436} INFO - 24/06/27 08:40:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:37.130+0000] {docker.py:436} INFO - 24/06/27 08:40:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:47.132+0000] {docker.py:436} INFO - 24/06/27 08:40:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:57.138+0000] {docker.py:436} INFO - 24/06/27 08:40:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:07.138+0000] {docker.py:436} INFO - 24/06/27 08:41:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:17.137+0000] {docker.py:436} INFO - 24/06/27 08:41:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:27.145+0000] {docker.py:436} INFO - 24/06/27 08:41:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:37.149+0000] {docker.py:436} INFO - 24/06/27 08:41:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:47.149+0000] {docker.py:436} INFO - 24/06/27 08:41:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:57.151+0000] {docker.py:436} INFO - 24/06/27 08:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:07.154+0000] {docker.py:436} INFO - 24/06/27 08:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:17.156+0000] {docker.py:436} INFO - 24/06/27 08:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:19.525+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/35 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.35.242270a4-15c3-4ad5-9944-9357c08a64f5.tmp
[2024-06-27T08:42:19.642+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.35.242270a4-15c3-4ad5-9944-9357c08a64f5.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/35
[2024-06-27T08:42:19.643+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1719477739494,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:19.662+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.674+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.701+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.711+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.779+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.792+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:19.793+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:19.793+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:42:19.795+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:42:19 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:19.813+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:19.813+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:19.817+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:42:19.818+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:42:19.818+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
24/06/27 08:42:19 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:19.825+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:42:19 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2024-06-27T08:42:19.835+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:42:19.849+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
[2024-06-27T08:42:19.889+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 60 for partition store_source_data-0
[2024-06-27T08:42:19.903+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:20.408+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:20.408+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=61, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:42:20.420+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:20 INFO DataWritingSparkTask: Committed partition 0 (task 35, attempt 0, stage 35.0)
24/06/27 08:42:20 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 2028 bytes result sent to driver
[2024-06-27T08:42:20.426+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 604 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:20.427+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-06-27T08:42:20.429+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.626 s
[2024-06-27T08:42:20.442+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2024-06-27T08:42:20.442+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:20.443+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.653669 s
24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:20.453+0000] {docker.py:436} INFO - Batch: 35
-------------------------------------------
[2024-06-27T08:42:20.504+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:42:20.504+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:20.524+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/35 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.35.1de7dd84-eccb-49f4-889f-de7700762e73.tmp
[2024-06-27T08:42:20.727+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.35.1de7dd84-eccb-49f4-889f-de7700762e73.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/35
[2024-06-27T08:42:20.729+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:19.490Z",
  "batchId" : 35,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8090614886731392,
  "durationMs" : {
    "addBatch" : 821,
    "commitOffsets" : 224,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 40,
    "triggerExecution" : 1236,
    "walCommit" : 147
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8090614886731392,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:23.054+0000] {docker.py:436} INFO - 24/06/27 08:42:23 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:42:30.724+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/36 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.36.061f4422-f874-4e59-9f6d-fe8dbc328496.tmp
[2024-06-27T08:42:30.915+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.36.061f4422-f874-4e59-9f6d-fe8dbc328496.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/36
24/06/27 08:42:30 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1719477750684,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:30.949+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.977+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.024+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.063+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.067+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.108+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:31.109+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:31.110+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:42:31.110+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:42:31.111+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:31.111+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:31.112+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[110] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:31.119+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.127+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.128+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:42:31.128+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:31.129+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[110] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:42:31.140+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
24/06/27 08:42:31 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:42:31.141+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
[2024-06-27T08:42:31.237+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 61 for partition store_source_data-0
[2024-06-27T08:42:31.237+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:31.757+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:31.757+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=62, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:31 INFO DataWritingSparkTask: Committed partition 0 (task 36, attempt 0, stage 36.0)
[2024-06-27T08:42:31.758+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 2028 bytes result sent to driver
[2024-06-27T08:42:31.758+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 617 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:31.758+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.642 s
[2024-06-27T08:42:31.759+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:31.759+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 36
-------------------------------------------
[2024-06-27T08:42:31.760+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/06/27 08:42:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
24/06/27 08:42:31 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.647264 s
24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:31.920+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:42:31.921+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:31.977+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/36 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.36.e30ccfa9-267b-49ed-ba20-56e85366ec2a.tmp
[2024-06-27T08:42:32.131+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.36.e30ccfa9-267b-49ed-ba20-56e85366ec2a.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/36
24/06/27 08:42:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:30.680Z",
  "batchId" : 36,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 52.631578947368425,
  "processedRowsPerSecond" : 0.6901311249137336,
  "durationMs" : {
    "addBatch" : 927,
    "commitOffsets" : 220,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 68,
    "triggerExecution" : 1449,
    "walCommit" : 229
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 52.631578947368425,
    "processedRowsPerSecond" : 0.6901311249137336,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:41.024+0000] {docker.py:436} INFO - 24/06/27 08:42:41 INFO BlockManagerInfo: Removed broadcast_36_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:42:42.131+0000] {docker.py:436} INFO - 24/06/27 08:42:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:52.136+0000] {docker.py:436} INFO - 24/06/27 08:42:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:02.151+0000] {docker.py:436} INFO - 24/06/27 08:43:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:12.157+0000] {docker.py:436} INFO - 24/06/27 08:43:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:22.164+0000] {docker.py:436} INFO - 24/06/27 08:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:32.172+0000] {docker.py:436} INFO - 24/06/27 08:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:42.182+0000] {docker.py:436} INFO - 24/06/27 08:43:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:52.189+0000] {docker.py:436} INFO - 24/06/27 08:43:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:02.195+0000] {docker.py:436} INFO - 24/06/27 08:44:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:12.204+0000] {docker.py:436} INFO - 24/06/27 08:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:22.213+0000] {docker.py:436} INFO - 24/06/27 08:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:32.223+0000] {docker.py:436} INFO - 24/06/27 08:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:42.228+0000] {docker.py:436} INFO - 24/06/27 08:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:52.232+0000] {docker.py:436} INFO - 24/06/27 08:44:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:02.233+0000] {docker.py:436} INFO - 24/06/27 08:45:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:12.243+0000] {docker.py:436} INFO - 24/06/27 08:45:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:22.263+0000] {docker.py:436} INFO - 24/06/27 08:45:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:32.263+0000] {docker.py:436} INFO - 24/06/27 08:45:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:42.272+0000] {docker.py:436} INFO - 24/06/27 08:45:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:48.167+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/37 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.37.250705ed-2633-48aa-933c-7d4609dad6ab.tmp
[2024-06-27T08:45:48.344+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.37.250705ed-2633-48aa-933c-7d4609dad6ab.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/37
[2024-06-27T08:45:48.345+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1719477948147,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:45:48.370+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.371+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.403+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.412+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.584+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.596+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.631+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:45:48.642+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:45:48 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:45:48 INFO DAGScheduler: Final stage: ResultStage 37 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:45:48 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:45:48 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:45:48.642+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[113] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:45:48.643+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:45:48.653+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:45:48.653+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:45:48.669+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:45:48.681+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[113] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:45:48.682+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2024-06-27T08:45:48.701+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:45:48.729+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
[2024-06-27T08:45:48.838+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 62 for partition store_source_data-0
[2024-06-27T08:45:48.883+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:45:49.379+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.379+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:45:49.380+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=63, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.383+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:45:49.396+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Committed partition 0 (task 37, attempt 0, stage 37.0)
[2024-06-27T08:45:49.397+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 2028 bytes result sent to driver
[2024-06-27T08:45:49.398+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 708 ms on localhost (executor driver) (1/1)
[2024-06-27T08:45:49.399+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
24/06/27 08:45:49 INFO DAGScheduler: ResultStage 37 (start at NativeMethodAccessorImpl.java:0) finished in 0.757 s
24/06/27 08:45:49 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
[2024-06-27T08:45:49.401+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.761978 s
[2024-06-27T08:45:49.401+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:45:49.403+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 37
-------------------------------------------
[2024-06-27T08:45:49.508+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:45:49.511+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:45:49.534+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/37 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.37.a52d97c6-0b43-458c-9b98-4f4d2c8911c3.tmp
[2024-06-27T08:45:49.661+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.37.a52d97c6-0b43-458c-9b98-4f4d2c8911c3.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/37
24/06/27 08:45:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:45:48.146Z",
  "batchId" : 37,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6622516556291391,
  "durationMs" : {
    "addBatch" : 1132,
    "commitOffsets" : 143,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 43,
    "triggerExecution" : 1510,
    "walCommit" : 191
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6622516556291391,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:45:59.681+0000] {docker.py:436} INFO - 24/06/27 08:45:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:00.750+0000] {docker.py:436} INFO - 24/06/27 08:46:00 INFO BlockManagerInfo: Removed broadcast_37_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:46:09.681+0000] {docker.py:436} INFO - 24/06/27 08:46:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:19.681+0000] {docker.py:436} INFO - 24/06/27 08:46:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:29.687+0000] {docker.py:436} INFO - 24/06/27 08:46:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:39.695+0000] {docker.py:436} INFO - 24/06/27 08:46:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:49.696+0000] {docker.py:436} INFO - 24/06/27 08:46:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:59.696+0000] {docker.py:436} INFO - 24/06/27 08:46:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:09.697+0000] {docker.py:436} INFO - 24/06/27 08:47:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:19.705+0000] {docker.py:436} INFO - 24/06/27 08:47:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:29.706+0000] {docker.py:436} INFO - 24/06/27 08:47:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:39.713+0000] {docker.py:436} INFO - 24/06/27 08:47:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:49.713+0000] {docker.py:436} INFO - 24/06/27 08:47:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:59.716+0000] {docker.py:436} INFO - 24/06/27 08:47:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:09.720+0000] {docker.py:436} INFO - 24/06/27 08:48:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:19.733+0000] {docker.py:436} INFO - 24/06/27 08:48:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:29.735+0000] {docker.py:436} INFO - 24/06/27 08:48:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:39.736+0000] {docker.py:436} INFO - 24/06/27 08:48:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:49.744+0000] {docker.py:436} INFO - 24/06/27 08:48:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:59.752+0000] {docker.py:436} INFO - 24/06/27 08:48:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:09.755+0000] {docker.py:436} INFO - 24/06/27 08:49:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:19.761+0000] {docker.py:436} INFO - 24/06/27 08:49:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:29.769+0000] {docker.py:436} INFO - 24/06/27 08:49:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:39.781+0000] {docker.py:436} INFO - 24/06/27 08:49:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:49.784+0000] {docker.py:436} INFO - 24/06/27 08:49:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:59.794+0000] {docker.py:436} INFO - 24/06/27 08:49:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:09.801+0000] {docker.py:436} INFO - 24/06/27 08:50:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:19.806+0000] {docker.py:436} INFO - 24/06/27 08:50:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:29.816+0000] {docker.py:436} INFO - 24/06/27 08:50:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:39.820+0000] {docker.py:436} INFO - 24/06/27 08:50:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:49.827+0000] {docker.py:436} INFO - 24/06/27 08:50:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:59.833+0000] {docker.py:436} INFO - 24/06/27 08:50:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:01.929+0000] {docker.py:436} INFO - 24/06/27 08:51:01 INFO Metrics: Metrics scheduler closed
24/06/27 08:51:01 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:51:01.930+0000] {docker.py:436} INFO - 24/06/27 08:51:01 INFO Metrics: Metrics reporters closed
[2024-06-27T08:51:01.931+0000] {docker.py:436} INFO - 24/06/27 08:51:01 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-3 unregistered
[2024-06-27T08:51:08.361+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/38 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.38.6882a268-dbe5-41e8-972d-1b5601e32bb0.tmp
[2024-06-27T08:51:08.491+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/.38.6882a268-dbe5-41e8-972d-1b5601e32bb0.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/offsets/38
[2024-06-27T08:51:08.492+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1719478268290,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:51:08.520+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.520+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.544+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.554+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.578+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.621+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:51:08.646+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:51:08 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:51:08 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:51:08 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:51:08 INFO DAGScheduler: Missing parents: List()
24/06/27 08:51:08 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[116] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:51:08.655+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 18.6 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.665+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.678+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on localhost:35905 (size: 8.9 KiB, free: 434.4 MiB)
24/06/27 08:51:08 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:51:08.687+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[116] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:51:08.698+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
[2024-06-27T08:51:08.701+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:51:08.756+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)
[2024-06-27T08:51:08.869+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:51:08.936+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:51:08.950+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 08:51:08 INFO AppInfoParser: Kafka startTimeMs: 1719478268931
[2024-06-27T08:51:08.951+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:51:08.952+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to offset 63 for partition store_source_data-0
[2024-06-27T08:51:08.970+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:51:08.997+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:51:09.503+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:51:09.524+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor-4, groupId=spark-kafka-source-79ea5dd9-a423-47f1-9d0a-e42e48e11bb1-1603252308-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=64, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:51:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:51:09 INFO DataWritingSparkTask: Committed partition 0 (task 38, attempt 0, stage 38.0)
24/06/27 08:51:09 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 2028 bytes result sent to driver
[2024-06-27T08:51:09.537+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 829 ms on localhost (executor driver) (1/1)
24/06/27 08:51:09 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
24/06/27 08:51:09 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.895 s
24/06/27 08:51:09 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:51:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
[2024-06-27T08:51:09.537+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.903400 s
[2024-06-27T08:51:09.538+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:51:09.539+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:51:09.539+0000] {docker.py:436} INFO - Batch: 38
-------------------------------------------
[2024-06-27T08:51:09.708+0000] {docker.py:436} INFO - +--------------------+
|                data|
+--------------------+
|{NULL, NULL, NULL...|
+--------------------+
[2024-06-27T08:51:09.714+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:51:09.808+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/38 using temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.38.05b334a7-0f94-4e9a-973c-703105403dde.tmp
[2024-06-27T08:51:10.020+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/.38.05b334a7-0f94-4e9a-973c-703105403dde.tmp to file:/tmp/temporary-27547587-df45-4b00-a2c6-3bf80f031521/commits/38
24/06/27 08:51:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fa3aaf6a-54af-4cdb-832e-202d5e6b7860",
  "runId" : "48e2cb0b-ede9-41ce-ab8b-ffefd71989ef",
  "name" : null,
  "timestamp" : "2024-06-27T08:51:08.284Z",
  "batchId" : 38,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.578368999421631,
  "durationMs" : {
    "addBatch" : 1182,
    "commitOffsets" : 296,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 44,
    "triggerExecution" : 1729,
    "walCommit" : 200
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.578368999421631,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@11952cae",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:51:17.821+0000] {docker.py:436} INFO - 24/06/27 08:51:17 INFO BlockManagerInfo: Removed broadcast_38_piece0 on localhost:35905 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2024-06-27T08:51:20.018+0000] {docker.py:436} INFO - 24/06/27 08:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:30.017+0000] {docker.py:436} INFO - 24/06/27 08:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:40.024+0000] {docker.py:436} INFO - 24/06/27 08:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:50.035+0000] {docker.py:436} INFO - 24/06/27 08:51:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:00.050+0000] {docker.py:436} INFO - 24/06/27 08:52:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:10.050+0000] {docker.py:436} INFO - 24/06/27 08:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:20.056+0000] {docker.py:436} INFO - 24/06/27 08:52:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:30.079+0000] {docker.py:436} INFO - 24/06/27 08:52:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:40.088+0000] {docker.py:436} INFO - 24/06/27 08:52:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:50.092+0000] {docker.py:436} INFO - 24/06/27 08:52:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:00.100+0000] {docker.py:436} INFO - 24/06/27 08:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:10.110+0000] {docker.py:436} INFO - 24/06/27 08:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:20.118+0000] {docker.py:436} INFO - 24/06/27 08:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:30.128+0000] {docker.py:436} INFO - 24/06/27 08:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:36.535+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:36.716+0000] {process_utils.py:132} INFO - Sending 15 to group 344. PIDs of all processes in the group: [344]
[2024-06-27T08:53:36.741+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 344
[2024-06-27T08:53:36.882+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-06-27T08:53:36.888+0000] {docker.py:528} INFO - Stopping docker container
[2024-06-27T08:53:40.132+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:40.139+0000] {process_utils.py:132} INFO - Sending 15 to group 344. PIDs of all processes in the group: [344]
[2024-06-27T08:53:40.139+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 344
