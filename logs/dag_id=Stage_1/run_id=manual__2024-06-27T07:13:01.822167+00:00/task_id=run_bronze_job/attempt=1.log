[2024-06-27T07:13:07.982+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T07:13:08.022+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:13:01.822167+00:00 [queued]>
[2024-06-27T07:13:08.034+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:13:01.822167+00:00 [queued]>
[2024-06-27T07:13:08.035+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-27T07:13:08.050+0000] {taskinstance.py:2330} INFO - Executing <Task(DockerOperator): run_bronze_job> on 2024-06-27 07:13:01.822167+00:00
[2024-06-27T07:13:08.059+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=369) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-27T07:13:08.061+0000] {standard_task_runner.py:63} INFO - Started process 370 to run task
[2024-06-27T07:13:08.059+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Stage_1', 'run_bronze_job', 'manual__2024-06-27T07:13:01.822167+00:00', '--job-id', '131', '--raw', '--subdir', 'DAGS_FOLDER/kafka_dag.py', '--cfg-path', '/tmp/tmpwvsu56j0']
[2024-06-27T07:13:08.062+0000] {standard_task_runner.py:91} INFO - Job 131: Subtask run_bronze_job
[2024-06-27T07:13:08.078+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-06-27T07:13:08.113+0000] {task_command.py:426} INFO - Running <TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:13:01.822167+00:00 [running]> on host 08bfa8b73cac
[2024-06-27T07:13:08.212+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Stage_1' AIRFLOW_CTX_TASK_ID='run_bronze_job' AIRFLOW_CTX_EXECUTION_DATE='2024-06-27T07:13:01.822167+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-27T07:13:01.822167+00:00'
[2024-06-27T07:13:08.213+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T07:13:08.247+0000] {docker.py:366} INFO - Starting docker container from image bitnami/spark:latest
[2024-06-27T07:13:08.251+0000] {docker.py:374} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-06-27T07:13:08.755+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:13:08.75 [0m[38;5;2mINFO [0m ==>
[2024-06-27T07:13:08.756+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:13:08.75 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-27T07:13:08.758+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:13:08.75 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-27T07:13:08.760+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:13:08.75 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-27T07:13:08.764+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:13:08.76 [0m[38;5;2mINFO [0m ==> Upgrade to Tanzu Application Catalog for production environments to access custom-configured and pre-packaged software components. Gain enhanced features, including Software Bill of Materials (SBOM), CVE scan result reports, and VEX documents. To learn more, visit [1mhttps://bitnami.com/enterprise[0m
[2024-06-27T07:13:08.765+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:13:08.76 [0m[38;5;2mINFO [0m ==>
[2024-06-27T07:13:08.776+0000] {docker.py:436} INFO - 
[2024-06-27T07:13:11.138+0000] {docker.py:436} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-27T07:13:11.238+0000] {docker.py:436} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-27T07:13:11.246+0000] {docker.py:436} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-06-27T07:13:11.247+0000] {docker.py:436} INFO - org.postgresql#postgresql added as a dependency
[2024-06-27T07:13:11.250+0000] {docker.py:436} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-971b5e62-cbba-4499-89b8-5a26c99dfa2c;1.0
[2024-06-27T07:13:11.251+0000] {docker.py:436} INFO - confs: [default]
[2024-06-27T07:13:13.461+0000] {docker.py:436} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T07:13:13.866+0000] {docker.py:436} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T07:13:13.944+0000] {docker.py:436} INFO - found org.apache.kafka#kafka-clients;2.8.1 in central
[2024-06-27T07:13:14.063+0000] {docker.py:436} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-27T07:13:14.153+0000] {docker.py:436} INFO - found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2024-06-27T07:13:14.301+0000] {docker.py:436} INFO - found org.slf4j#slf4j-api;1.7.32 in central
[2024-06-27T07:13:15.819+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2024-06-27T07:13:15.964+0000] {docker.py:436} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2024-06-27T07:13:16.080+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2024-06-27T07:13:16.297+0000] {docker.py:436} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-27T07:13:16.340+0000] {docker.py:436} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-27T07:13:16.745+0000] {docker.py:436} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-27T07:13:18.371+0000] {docker.py:436} INFO - found org.postgresql#postgresql;42.2.2 in central
[2024-06-27T07:13:18.395+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T07:13:18.455+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (68ms)
[2024-06-27T07:13:18.474+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar ...
[2024-06-27T07:13:18.573+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.2.2!postgresql.jar(bundle) (116ms)
[2024-06-27T07:13:18.583+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T07:13:18.601+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (26ms)
[2024-06-27T07:13:18.611+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...
[2024-06-27T07:13:19.221+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (618ms)
[2024-06-27T07:13:19.240+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-27T07:13:19.258+0000] {docker.py:436} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (28ms)
[2024-06-27T07:13:19.269+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-27T07:13:19.299+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (39ms)
[2024-06-27T07:13:19.308+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[2024-06-27T07:13:19.317+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (18ms)
[2024-06-27T07:13:19.328+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...
[2024-06-27T07:13:23.148+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (3829ms)
[2024-06-27T07:13:23.162+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-27T07:13:23.243+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (93ms)
[2024-06-27T07:13:23.259+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...
[2024-06-27T07:13:23.497+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (251ms)
[2024-06-27T07:13:23.506+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...
[2024-06-27T07:13:23.527+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (29ms)
[2024-06-27T07:13:23.536+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...
[2024-06-27T07:13:25.924+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (2396ms)
[2024-06-27T07:13:25.935+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-27T07:13:25.956+0000] {docker.py:436} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (31ms)
[2024-06-27T07:13:25.957+0000] {docker.py:436} INFO - :: resolution report :: resolve 7136ms :: artifacts dl 7571ms
[2024-06-27T07:13:25.957+0000] {docker.py:436} INFO - :: modules in use:
[2024-06-27T07:13:25.958+0000] {docker.py:436} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
[2024-06-27T07:13:25.958+0000] {docker.py:436} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.2.2 from central in [default]
	org.slf4j#slf4j-api;1.7.32 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
[2024-06-27T07:13:25.959+0000] {docker.py:436} INFO - ---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2024-06-27T07:13:25.959+0000] {docker.py:436} INFO - |      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-27T07:13:25.965+0000] {docker.py:436} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-971b5e62-cbba-4499-89b8-5a26c99dfa2c
[2024-06-27T07:13:25.966+0000] {docker.py:436} INFO - confs: [default]
[2024-06-27T07:13:26.068+0000] {docker.py:436} INFO - 13 artifacts copied, 0 already retrieved (57403kB/103ms)
[2024-06-27T07:13:26.396+0000] {docker.py:436} INFO - 24/06/27 07:13:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-27T07:13:27.882+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO SparkContext: Running Spark version 3.5.1
[2024-06-27T07:13:27.883+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO SparkContext: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T07:13:27.885+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO SparkContext: Java version 17.0.11
[2024-06-27T07:13:27.921+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO ResourceUtils: ==============================================================
[2024-06-27T07:13:27.923+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-27T07:13:27.925+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO ResourceUtils: ==============================================================
[2024-06-27T07:13:27.926+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-27T07:13:27.957+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-27T07:13:27.981+0000] {docker.py:436} INFO - 24/06/27 07:13:27 INFO ResourceProfile: Limiting resource is cpu
24/06/27 07:13:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-27T07:13:28.056+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SecurityManager: Changing view acls to: spark
[2024-06-27T07:13:28.057+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SecurityManager: Changing modify acls to: spark
[2024-06-27T07:13:28.058+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SecurityManager: Changing view acls groups to:
[2024-06-27T07:13:28.059+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SecurityManager: Changing modify acls groups to:
[2024-06-27T07:13:28.060+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-27T07:13:28.352+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Successfully started service 'sparkDriver' on port 36985.
[2024-06-27T07:13:28.382+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkEnv: Registering MapOutputTracker
[2024-06-27T07:13:28.416+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-27T07:13:28.443+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-27T07:13:28.445+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-27T07:13:28.451+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-27T07:13:28.482+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-39a154df-12fd-4d32-b7fb-bfeab182c677
[2024-06-27T07:13:28.498+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-27T07:13:28.516+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-27T07:13:28.684+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-27T07:13:28.777+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-27T07:13:28.835+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://localhost:36985/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at spark://localhost:36985/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://localhost:36985/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://localhost:36985/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472407874
[2024-06-27T07:13:28.836+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:36985/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:36985/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://localhost:36985/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://localhost:36985/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:36985/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://localhost:36985/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472407874
[2024-06-27T07:13:28.837+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://localhost:36985/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://localhost:36985/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472407874
24/06/27 07:13:28 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:36985/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472407874
[2024-06-27T07:13:28.841+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
[2024-06-27T07:13:28.843+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:13:28.853+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472407874
[2024-06-27T07:13:28.854+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:13:28.861+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
[2024-06-27T07:13:28.862+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:13:28.874+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472407874
[2024-06-27T07:13:28.876+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:13:28.886+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472407874
[2024-06-27T07:13:28.888+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:13:28.896+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472407874
[2024-06-27T07:13:28.898+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:13:28.903+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472407874
[2024-06-27T07:13:28.904+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:13:28.910+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472407874
[2024-06-27T07:13:28.911+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:13:28.949+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472407874
[2024-06-27T07:13:28.953+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:13:28.959+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472407874
[2024-06-27T07:13:28.960+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:13:28.965+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472407874
[2024-06-27T07:13:28.966+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:13:28.972+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472407874
[2024-06-27T07:13:28.973+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:13:28.994+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472407874
[2024-06-27T07:13:28.995+0000] {docker.py:436} INFO - 24/06/27 07:13:28 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:13:29.096+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Starting executor ID driver on host localhost
[2024-06-27T07:13:29.110+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T07:13:29.112+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Java version 17.0.11
[2024-06-27T07:13:29.125+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-27T07:13:29.126+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@79c6809b for default.
[2024-06-27T07:13:29.139+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472407874
[2024-06-27T07:13:29.159+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:13:29.162+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472407874
[2024-06-27T07:13:29.188+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:13:29.193+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472407874
[2024-06-27T07:13:29.197+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:13:29.203+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472407874
[2024-06-27T07:13:29.245+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:13:29.252+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472407874
[2024-06-27T07:13:29.254+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:13:29.269+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:13:29.273+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472407874
[2024-06-27T07:13:29.274+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:13:29.281+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.287+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:13:29.292+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.293+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:13:29.301+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.305+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:13:29.314+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472407874
[2024-06-27T07:13:29.317+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:13:29.332+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.333+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:13:29.349+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472407874
[2024-06-27T07:13:29.356+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:13:29.370+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.425+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:36985 after 39 ms (0 ms spent in bootstraps)
[2024-06-27T07:13:29.438+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp11788173092458640483.tmp
[2024-06-27T07:13:29.473+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp11788173092458640483.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:13:29.478+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T07:13:29.479+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472407874
[2024-06-27T07:13:29.479+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp12031245475303186523.tmp
[2024-06-27T07:13:29.487+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp12031245475303186523.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:13:29.491+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.postgresql_postgresql-42.2.2.jar to class loader default
[2024-06-27T07:13:29.492+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.494+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp437751295141724816.tmp
[2024-06-27T07:13:29.499+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp437751295141724816.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:13:29.504+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T07:13:29.506+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472407874
[2024-06-27T07:13:29.508+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp6437316017354347079.tmp
[2024-06-27T07:13:29.510+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp6437316017354347079.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:13:29.517+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/commons-logging_commons-logging-1.1.3.jar to class loader default
[2024-06-27T07:13:29.517+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472407874
[2024-06-27T07:13:29.526+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp18435272314821479218.tmp
[2024-06-27T07:13:29.691+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp18435272314821479218.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:13:29.697+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader default
[2024-06-27T07:13:29.697+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472407874
[2024-06-27T07:13:29.698+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp3734351275713641271.tmp
[2024-06-27T07:13:29.899+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp3734351275713641271.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:13:29.907+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader default
[2024-06-27T07:13:29.908+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472407874
24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp9483159331491154548.tmp
[2024-06-27T07:13:29.912+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp9483159331491154548.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:13:29.917+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-27T07:13:29.918+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.920+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp3341995562206463343.tmp
[2024-06-27T07:13:29.921+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp3341995562206463343.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:13:29.925+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.spark-project.spark_unused-1.0.0.jar to class loader default
[2024-06-27T07:13:29.926+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472407874
[2024-06-27T07:13:29.926+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp17542921946547082866.tmp
[2024-06-27T07:13:29.928+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp17542921946547082866.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:13:29.932+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.slf4j_slf4j-api-1.7.32.jar to class loader default
[2024-06-27T07:13:29.933+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472407874
[2024-06-27T07:13:29.933+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp16050968919760351229.tmp
[2024-06-27T07:13:29.934+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp16050968919760351229.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:13:29.938+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2024-06-27T07:13:29.939+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472407874
[2024-06-27T07:13:29.939+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp7478478075729658012.tmp
[2024-06-27T07:13:29.941+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp7478478075729658012.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:13:29.945+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-06-27T07:13:29.945+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472407874
[2024-06-27T07:13:29.946+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp6373314207469354788.tmp
[2024-06-27T07:13:29.964+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp6373314207469354788.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:13:29.971+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.apache.kafka_kafka-clients-2.8.1.jar to class loader default
[2024-06-27T07:13:29.972+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Fetching spark://localhost:36985/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472407874
[2024-06-27T07:13:29.973+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Fetching spark://localhost:36985/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp4193534242510268233.tmp
[2024-06-27T07:13:29.982+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/fetchFileTemp4193534242510268233.tmp has been previously copied to /tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:13:29.987+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Executor: Adding file:/tmp/spark-e7e27477-d342-445d-8c4b-0b7fed58288c/userFiles-c8ba27fc-48d9-4dfe-90ab-9e1bf7aa53a5/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader default
[2024-06-27T07:13:30.004+0000] {docker.py:436} INFO - 24/06/27 07:13:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41305.
24/06/27 07:13:29 INFO NettyBlockTransferService: Server created on localhost:41305
24/06/27 07:13:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-27T07:13:30.013+0000] {docker.py:436} INFO - 24/06/27 07:13:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 41305, None)
[2024-06-27T07:13:30.019+0000] {docker.py:436} INFO - 24/06/27 07:13:30 INFO BlockManagerMasterEndpoint: Registering block manager localhost:41305 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 41305, None)
[2024-06-27T07:13:30.021+0000] {docker.py:436} INFO - 24/06/27 07:13:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 41305, None)
[2024-06-27T07:13:30.023+0000] {docker.py:436} INFO - 24/06/27 07:13:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 41305, None)
[2024-06-27T07:13:30.651+0000] {docker.py:436} INFO - 2024-06-27 07:13:30,651:create_spark_session:INFO:Spark session created successfully
[2024-06-27T07:13:30.663+0000] {docker.py:436} INFO - 24/06/27 07:13:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-27T07:13:30.667+0000] {docker.py:436} INFO - 24/06/27 07:13:30 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-27T07:13:32.388+0000] {docker.py:436} INFO - 2024-06-27 07:13:32,388:create_initial_dataframe:INFO:Initial dataframe created successfully:
[2024-06-27T07:13:32.558+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-27T07:13:32.591+0000] {docker.py:436} INFO - 24/06/27 07:13:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-27T07:13:32.613+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a resolved to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a.
[2024-06-27T07:13:32.614+0000] {docker.py:436} INFO - 24/06/27 07:13:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-27T07:13:32.717+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/metadata using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/.metadata.d2d4a125-a1d4-430e-8313-5cb75076f79b.tmp
[2024-06-27T07:13:32.862+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/.metadata.d2d4a125-a1d4-430e-8313-5cb75076f79b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/metadata
[2024-06-27T07:13:32.898+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO MicroBatchExecution: Starting [id = ac8a30bd-12a6-4ea1-80f8-19bc81412578, runId = e01ff820-d4de-4106-96c3-8bb34506fc33]. Use file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a to store the query checkpoint.
[2024-06-27T07:13:32.910+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@41469e7d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@6dfd3f9]
[2024-06-27T07:13:32.968+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T07:13:32.977+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T07:13:32.978+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-27T07:13:32.985+0000] {docker.py:436} INFO - 24/06/27 07:13:32 INFO MicroBatchExecution: Stream started from {}
[2024-06-27T07:13:33.500+0000] {docker.py:436} INFO - 24/06/27 07:13:33 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-27T07:13:33.753+0000] {docker.py:436} INFO - 24/06/27 07:13:33 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
[2024-06-27T07:13:33.753+0000] {docker.py:436} INFO - 24/06/27 07:13:33 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
[2024-06-27T07:13:33.754+0000] {docker.py:436} INFO - 24/06/27 07:13:33 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
[2024-06-27T07:13:33.754+0000] {docker.py:436} INFO - 24/06/27 07:13:33 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
[2024-06-27T07:13:33.755+0000] {docker.py:436} INFO - 24/06/27 07:13:33 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
[2024-06-27T07:13:33.766+0000] {docker.py:436} INFO - 24/06/27 07:13:33 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:13:33.770+0000] {docker.py:436} INFO - 24/06/27 07:13:33 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T07:13:33.771+0000] {docker.py:436} INFO - 24/06/27 07:13:33 INFO AppInfoParser: Kafka startTimeMs: 1719472413754
[2024-06-27T07:13:34.220+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/sources/0/0 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/sources/0/.0.334258c1-61ec-4905-95d5-c3b66c737a35.tmp
[2024-06-27T07:13:34.258+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/sources/0/.0.334258c1-61ec-4905-95d5-c3b66c737a35.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/sources/0/0
[2024-06-27T07:13:34.259+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO KafkaMicroBatchStream: Initial offsets: {"store_source_data":{"0":8}}
[2024-06-27T07:13:34.288+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/0 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.0.642f50f5-bdd7-4dee-af7e-dc7d6eacb7a5.tmp
[2024-06-27T07:13:34.382+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.0.642f50f5-bdd7-4dee-af7e-dc7d6eacb7a5.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/0
[2024-06-27T07:13:34.383+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719472414274,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:13:34.914+0000] {docker.py:436} INFO - 24/06/27 07:13:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:35.020+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:35.078+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:35.080+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:35.151+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:35.155+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:13:35.537+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO CodeGenerator: Code generated in 206.659449 ms
[2024-06-27T07:13:35.628+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:13:35.646+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:13:35.676+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:13:35.676+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:13:35.677+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:13:35.679+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:13:35.687+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:13:35.839+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:13:35.873+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.4 MiB)
[2024-06-27T07:13:35.876+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:41305 (size: 4.4 KiB, free: 434.4 MiB)
[2024-06-27T07:13:35.881+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:13:35.902+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:13:35.904+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-27T07:13:35.988+0000] {docker.py:436} INFO - 24/06/27 07:13:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:13:36.007+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-06-27T07:13:36.134+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO CodeGenerator: Code generated in 31.05535 ms
[2024-06-27T07:13:36.236+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO CodeGenerator: Code generated in 46.638633 ms
[2024-06-27T07:13:36.316+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO CodeGenerator: Code generated in 47.035103 ms
[2024-06-27T07:13:36.344+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T07:13:36.428+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:13:36.430+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T07:13:36.431+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO AppInfoParser: Kafka startTimeMs: 1719472416427
[2024-06-27T07:13:36.433+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T07:13:36.448+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 8 for partition store_source_data-0
[2024-06-27T07:13:36.466+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T07:13:36.553+0000] {docker.py:436} INFO - 24/06/27 07:13:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:13:37.054+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:13:37.055+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:13:37.056+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=27, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:13:37.132+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:13:37.135+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2024-06-27T07:13:37.181+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 100308 bytes result sent to driver
[2024-06-27T07:13:37.201+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1239 ms on localhost (executor driver) (1/1)
[2024-06-27T07:13:37.203+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-27T07:13:37.225+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.516 s
[2024-06-27T07:13:37.238+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:13:37.239+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-06-27T07:13:37.243+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 1.596795 s
[2024-06-27T07:13:37.247+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:13:37.253+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:13:37.254+0000] {docker.py:436} INFO - Batch: 0
[2024-06-27T07:13:37.254+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:13:37.382+0000] {docker.py:436} INFO - 24/06/27 07:13:37 INFO CodeGenerator: Code generated in 11.578689 ms
[2024-06-27T07:13:38.572+0000] {docker.py:436} INFO - 24/06/27 07:13:38 INFO CodeGenerator: Code generated in 15.933696 ms
[2024-06-27T07:13:38.593+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:13:38.594+0000] {docker.py:436} INFO - 24/06/27 07:13:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:13:38.609+0000] {docker.py:436} INFO - 24/06/27 07:13:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/0 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.0.d8c7b672-b273-4aed-9120-ecc5dc3a9e7d.tmp
[2024-06-27T07:13:38.643+0000] {docker.py:436} INFO - 24/06/27 07:13:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.0.d8c7b672-b273-4aed-9120-ecc5dc3a9e7d.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/0
[2024-06-27T07:13:38.692+0000] {docker.py:436} INFO - 24/06/27 07:13:38 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:13:32.957Z",
  "batchId" : 0,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 3.340953050817654,
  "durationMs" : {
    "addBatch" : 3543,
    "commitOffsets" : 46,
    "getBatch" : 35,
    "latestOffset" : 1284,
    "queryPlanning" : 624,
    "triggerExecution" : 5685,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : null,
    "endOffset" : {
      "store_source_data" : {
        "0" : 27
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 27
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 3.340953050817654,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 19
  }
}
[2024-06-27T07:13:46.767+0000] {docker.py:436} INFO - 24/06/27 07:13:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:41305 in memory (size: 4.4 KiB, free: 434.4 MiB)
[2024-06-27T07:13:48.675+0000] {docker.py:436} INFO - 24/06/27 07:13:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:13:58.685+0000] {docker.py:436} INFO - 24/06/27 07:13:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:08.695+0000] {docker.py:436} INFO - 24/06/27 07:14:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:18.695+0000] {docker.py:436} INFO - 24/06/27 07:14:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:22.615+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/1 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.1.22582ac5-6e91-4378-9cae-e33b67be55ef.tmp
[2024-06-27T07:14:22.659+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.1.22582ac5-6e91-4378-9cae-e33b67be55ef.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/1
[2024-06-27T07:14:22.659+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719472462601,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:14:22.723+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.733+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.791+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.795+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.824+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.826+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:22.883+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:14:22.884+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:14:22.887+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:14:22.889+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:14:22 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:14:22.890+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Missing parents: List()
24/06/27 07:14:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:14:22.896+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:14:22.897+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/27 07:14:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:14:22.899+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:14:22.901+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:14:22.902+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-27T07:14:22.904+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:14:22.906+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-27T07:14:22.930+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 27 for partition store_source_data-0
[2024-06-27T07:14:22.947+0000] {docker.py:436} INFO - 24/06/27 07:14:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:14:23.449+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:14:23.450+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:14:23.452+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:14:23.453+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:14:23.453+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2024-06-27T07:14:23.456+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 7180 bytes result sent to driver
[2024-06-27T07:14:23.458+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 555 ms on localhost (executor driver) (1/1)
[2024-06-27T07:14:23.458+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-27T07:14:23.463+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.567 s
24/06/27 07:14:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:14:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/27 07:14:23 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.576125 s
24/06/27 07:14:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:14:23.464+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 1
-------------------------------------------
[2024-06-27T07:14:23.536+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:14:23.539+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:14:23.594+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/1 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.1.e2231a79-cf5b-4bc1-b32f-1e1252f08bf1.tmp
[2024-06-27T07:14:23.741+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.1.e2231a79-cf5b-4bc1-b32f-1e1252f08bf1.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/1
[2024-06-27T07:14:23.757+0000] {docker.py:436} INFO - 24/06/27 07:14:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:14:22.599Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8748906386701663,
  "durationMs" : {
    "addBatch" : 789,
    "commitOffsets" : 212,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 74,
    "triggerExecution" : 1143,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 27
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8748906386701663,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:14:32.188+0000] {docker.py:436} INFO - 24/06/27 07:14:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:14:33.744+0000] {docker.py:436} INFO - 24/06/27 07:14:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:43.754+0000] {docker.py:436} INFO - 24/06/27 07:14:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:14:53.764+0000] {docker.py:436} INFO - 24/06/27 07:14:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:03.770+0000] {docker.py:436} INFO - 24/06/27 07:15:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:13.778+0000] {docker.py:436} INFO - 24/06/27 07:15:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:23.788+0000] {docker.py:436} INFO - 24/06/27 07:15:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:33.794+0000] {docker.py:436} INFO - 24/06/27 07:15:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:43.801+0000] {docker.py:436} INFO - 24/06/27 07:15:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:53.805+0000] {docker.py:436} INFO - 24/06/27 07:15:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:03.806+0000] {docker.py:436} INFO - 24/06/27 07:16:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:13.809+0000] {docker.py:436} INFO - 24/06/27 07:16:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:23.810+0000] {docker.py:436} INFO - 24/06/27 07:16:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:33.818+0000] {docker.py:436} INFO - 24/06/27 07:16:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:43.827+0000] {docker.py:436} INFO - 24/06/27 07:16:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:53.833+0000] {docker.py:436} INFO - 24/06/27 07:16:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:03.840+0000] {docker.py:436} INFO - 24/06/27 07:17:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:13.844+0000] {docker.py:436} INFO - 24/06/27 07:17:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:23.852+0000] {docker.py:436} INFO - 24/06/27 07:17:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:33.858+0000] {docker.py:436} INFO - 24/06/27 07:17:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:43.867+0000] {docker.py:436} INFO - 24/06/27 07:17:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:53.870+0000] {docker.py:436} INFO - 24/06/27 07:17:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:03.878+0000] {docker.py:436} INFO - 24/06/27 07:18:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:13.880+0000] {docker.py:436} INFO - 24/06/27 07:18:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:19.971+0000] {docker.py:436} INFO - 24/06/27 07:18:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/2 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.2.17c13a1c-b7cf-4e03-856d-4a15461109bf.tmp
[2024-06-27T07:18:20.026+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.2.17c13a1c-b7cf-4e03-856d-4a15461109bf.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/2
[2024-06-27T07:18:20.027+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719472699957,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:18:20.121+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.134+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.194+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.202+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.262+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.274+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.393+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:18:20.395+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:18:20.403+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:18:20.409+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:18:20 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:18:20.412+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:18:20.418+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:18:20.423+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:18:20.449+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:18:20.449+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:18:20.452+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:18:20.466+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:18:20.469+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-06-27T07:18:20.475+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:18:20.494+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-06-27T07:18:20.608+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 28 for partition store_source_data-0
[2024-06-27T07:18:20.610+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:18:21.125+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 07:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:18:21 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:18:21 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2024-06-27T07:18:21.177+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 7266 bytes result sent to driver
[2024-06-27T07:18:21.192+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 713 ms on localhost (executor driver) (1/1)
24/06/27 07:18:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-27T07:18:21.212+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.775 s
[2024-06-27T07:18:21.212+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:18:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-06-27T07:18:21.226+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.826127 s
[2024-06-27T07:18:21.227+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:18:21.244+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:18:21.269+0000] {docker.py:436} INFO - Batch: 2
-------------------------------------------
[2024-06-27T07:18:21.452+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:18:21.452+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:18:21.518+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/2 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.2.5c987830-83ed-4c29-ad0f-9dfaa9e11174.tmp
[2024-06-27T07:18:21.696+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.2.5c987830-83ed-4c29-ad0f-9dfaa9e11174.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/2
[2024-06-27T07:18:21.697+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:18:19.955Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.5757052389176741,
  "durationMs" : {
    "addBatch" : 1291,
    "commitOffsets" : 243,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 116,
    "triggerExecution" : 1737,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.5757052389176741,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:18:31.699+0000] {docker.py:436} INFO - 24/06/27 07:18:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:33.168+0000] {docker.py:436} INFO - 24/06/27 07:18:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:18:41.707+0000] {docker.py:436} INFO - 24/06/27 07:18:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:51.714+0000] {docker.py:436} INFO - 24/06/27 07:18:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:01.718+0000] {docker.py:436} INFO - 24/06/27 07:19:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:11.727+0000] {docker.py:436} INFO - 24/06/27 07:19:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:21.161+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/3 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.3.bd157b3e-5a72-4caa-86bd-1d8dd16899d8.tmp
[2024-06-27T07:19:21.211+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.3.bd157b3e-5a72-4caa-86bd-1d8dd16899d8.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/3
[2024-06-27T07:19:21.213+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719472761116,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:19:21.230+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.244+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.302+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.314+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.346+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.349+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.416+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:19:21.425+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:19:21.427+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:19:21.428+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:19:21.428+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:19:21.430+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:19:21.431+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:19:21.439+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:19:21.487+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:19:21.487+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:19:21.488+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:19:21.503+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:19:21.504+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-06-27T07:19:21.518+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:19:21.521+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2024-06-27T07:19:21.727+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 29 for partition store_source_data-0
[2024-06-27T07:19:21.769+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:19:22.266+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:19:22.270+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:19:22.281+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:19:22.282+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2024-06-27T07:19:22.321+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 7266 bytes result sent to driver
[2024-06-27T07:19:22.330+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 832 ms on localhost (executor driver) (1/1)
[2024-06-27T07:19:22.331+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-06-27T07:19:22.345+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 0.902 s
[2024-06-27T07:19:22.364+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:19:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/27 07:19:22 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 0.933556 s
[2024-06-27T07:19:22.365+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:19:22.371+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 3
[2024-06-27T07:19:22.372+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:19:22.574+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:19:22.575+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:19:22.596+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/3 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.3.35dfa0e0-5374-4544-9c99-76c570b0dd68.tmp
[2024-06-27T07:19:22.643+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.3.35dfa0e0-5374-4544-9c99-76c570b0dd68.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/3
[2024-06-27T07:19:22.644+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:19:21.112Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.6531678641410843,
  "durationMs" : {
    "addBatch" : 1326,
    "commitOffsets" : 62,
    "getBatch" : 1,
    "latestOffset" : 4,
    "queryPlanning" : 38,
    "triggerExecution" : 1531,
    "walCommit" : 98
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.6531678641410843,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:19:32.649+0000] {docker.py:436} INFO - 24/06/27 07:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:34.163+0000] {docker.py:436} INFO - 24/06/27 07:19:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:19:42.659+0000] {docker.py:436} INFO - 24/06/27 07:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:52.664+0000] {docker.py:436} INFO - 24/06/27 07:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:02.664+0000] {docker.py:436} INFO - 24/06/27 07:20:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:12.668+0000] {docker.py:436} INFO - 24/06/27 07:20:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:22.669+0000] {docker.py:436} INFO - 24/06/27 07:20:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:32.676+0000] {docker.py:436} INFO - 24/06/27 07:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:42.677+0000] {docker.py:436} INFO - 24/06/27 07:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:52.684+0000] {docker.py:436} INFO - 24/06/27 07:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:02.688+0000] {docker.py:436} INFO - 24/06/27 07:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:08.900+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/4 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.4.43e14231-2965-4220-8cef-f803a1af6087.tmp
[2024-06-27T07:21:08.959+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.4.43e14231-2965-4220-8cef-f803a1af6087.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/4
24/06/27 07:21:08 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719472868882,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:21:09.013+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.018+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.099+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.101+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.175+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.180+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.289+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:21:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:21:09.317+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:21:09 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:21:09 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:21:09 INFO DAGScheduler: Missing parents: List()
24/06/27 07:21:09 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:21:09.352+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:21:09.422+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:21:09.422+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:21:09.430+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:21:09.442+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:21:09.444+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-06-27T07:21:09.453+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:21:09.471+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-06-27T07:21:09.843+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 30 for partition store_source_data-0
[2024-06-27T07:21:09.947+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:21:10.450+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:21:10.452+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=31, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:21:10.452+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:21:10.453+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2024-06-27T07:21:10.463+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 7266 bytes result sent to driver
[2024-06-27T07:21:10.467+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1015 ms on localhost (executor driver) (1/1)
[2024-06-27T07:21:10.470+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 1.133 s
24/06/27 07:21:10 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:21:10.471+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-06-27T07:21:10.472+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-06-27T07:21:10.473+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 1.193269 s
[2024-06-27T07:21:10.474+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:21:10.475+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:21:10.477+0000] {docker.py:436} INFO - Batch: 4
[2024-06-27T07:21:10.478+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:21:10.542+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:21:10.543+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:21:10.554+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/4 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.4.ddfcb41f-8fa6-41a7-ae99-94380ac31b6e.tmp
[2024-06-27T07:21:10.600+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.4.ddfcb41f-8fa6-41a7-ae99-94380ac31b6e.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/4
[2024-06-27T07:21:10.602+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:21:08.878Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.5807200929152149,
  "durationMs" : {
    "addBatch" : 1500,
    "commitOffsets" : 59,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 85,
    "triggerExecution" : 1722,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.5807200929152149,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:21:20.608+0000] {docker.py:436} INFO - 24/06/27 07:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:21.715+0000] {docker.py:436} INFO - 24/06/27 07:21:21 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:21:30.608+0000] {docker.py:436} INFO - 24/06/27 07:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:40.615+0000] {docker.py:436} INFO - 24/06/27 07:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:50.620+0000] {docker.py:436} INFO - 24/06/27 07:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:00.622+0000] {docker.py:436} INFO - 24/06/27 07:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:09.705+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/5 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.5.dabdd64d-f15a-46e9-9689-11eae9e7cede.tmp
[2024-06-27T07:22:09.972+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.5.dabdd64d-f15a-46e9-9689-11eae9e7cede.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/5
[2024-06-27T07:22:09.976+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1719472929664,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:22:10.090+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.104+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.153+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.154+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.236+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.259+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.380+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:22:10.403+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:22:10.405+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:22:10.424+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:22:10 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:22:10 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:22:10.428+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:22:10.466+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:22:10.534+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:22:10.540+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:22:10.591+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:22:10.598+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:22:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-06-27T07:22:10.608+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:22:10.618+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-06-27T07:22:10.702+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 31 for partition store_source_data-0
[2024-06-27T07:22:10.710+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:22:11.214+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:22:11.215+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:22:11.219+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=32, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:22:11.222+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:22:11.223+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2024-06-27T07:22:11.244+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 7266 bytes result sent to driver
[2024-06-27T07:22:11.248+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 642 ms on localhost (executor driver) (1/1)
[2024-06-27T07:22:11.250+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.821 s
[2024-06-27T07:22:11.254+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:22:11.256+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-06-27T07:22:11.257+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-06-27T07:22:11.260+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.858000 s
[2024-06-27T07:22:11.262+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:22:11.263+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:22:11.264+0000] {docker.py:436} INFO - Batch: 5
[2024-06-27T07:22:11.266+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:22:11.373+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:22:11.374+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:22:11.460+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/5 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.5.704954bb-c71b-4b51-a2d1-1d18b167d154.tmp
[2024-06-27T07:22:11.619+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.5.704954bb-c71b-4b51-a2d1-1d18b167d154.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/5
[2024-06-27T07:22:11.621+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:22:09.634Z",
  "batchId" : 5,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 47.61904761904761,
  "processedRowsPerSecond" : 0.5037783375314862,
  "durationMs" : {
    "addBatch" : 1261,
    "commitOffsets" : 240,
    "getBatch" : 0,
    "latestOffset" : 30,
    "queryPlanning" : 145,
    "triggerExecution" : 1985,
    "walCommit" : 308
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 47.61904761904761,
    "processedRowsPerSecond" : 0.5037783375314862,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:22:21.619+0000] {docker.py:436} INFO - 24/06/27 07:22:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:22.762+0000] {docker.py:436} INFO - 24/06/27 07:22:22 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:22:31.629+0000] {docker.py:436} INFO - 24/06/27 07:22:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:41.628+0000] {docker.py:436} INFO - 24/06/27 07:22:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:51.631+0000] {docker.py:436} INFO - 24/06/27 07:22:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:01.634+0000] {docker.py:436} INFO - 24/06/27 07:23:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:06.328+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/6 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.6.87dcf8ae-3e37-41f0-8481-bd3f9ea526f6.tmp
[2024-06-27T07:23:06.431+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.6.87dcf8ae-3e37-41f0-8481-bd3f9ea526f6.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/6
[2024-06-27T07:23:06.432+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1719472986286,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:23:06.523+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.539+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.608+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.629+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.705+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.709+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.757+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:23:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:23:06 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:23:06 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:23:06 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:23:06 INFO DAGScheduler: Missing parents: List()
24/06/27 07:23:06 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:23:06.761+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:23:06.851+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:23:06.853+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:23:06.854+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:23:06.855+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:23:06.860+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-06-27T07:23:06.861+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:23:06.873+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2024-06-27T07:23:06.906+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 32 for partition store_source_data-0
[2024-06-27T07:23:06.924+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:23:07.428+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:23:07.429+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:23:07.430+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=33, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:23:07.433+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:23:07.435+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2024-06-27T07:23:07.456+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 7266 bytes result sent to driver
[2024-06-27T07:23:07.461+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 597 ms on localhost (executor driver) (1/1)
24/06/27 07:23:07 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-06-27T07:23:07.461+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 6
-------------------------------------------
[2024-06-27T07:23:07.462+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.701 s
24/06/27 07:23:07 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/27 07:23:07 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.704276 s
24/06/27 07:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:23:07.507+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:23:07.507+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:23:07.546+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/6 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.6.b20dd255-9164-45f0-bbb1-03ab8ee4d909.tmp
[2024-06-27T07:23:07.717+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.6.b20dd255-9164-45f0-bbb1-03ab8ee4d909.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/6
[2024-06-27T07:23:07.720+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:23:06.281Z",
  "batchId" : 6,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 55.55555555555556,
  "processedRowsPerSecond" : 0.6963788300835655,
  "durationMs" : {
    "addBatch" : 960,
    "commitOffsets" : 210,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 111,
    "triggerExecution" : 1436,
    "walCommit" : 146
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 55.55555555555556,
    "processedRowsPerSecond" : 0.6963788300835655,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:23:17.719+0000] {docker.py:436} INFO - 24/06/27 07:23:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:19.181+0000] {docker.py:436} INFO - 24/06/27 07:23:19 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:23:27.722+0000] {docker.py:436} INFO - 24/06/27 07:23:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:37.724+0000] {docker.py:436} INFO - 24/06/27 07:23:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:47.731+0000] {docker.py:436} INFO - 24/06/27 07:23:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:57.750+0000] {docker.py:436} INFO - 24/06/27 07:23:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:59.603+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/7 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.7.41fba91c-9e2c-493d-9e89-59f8d9cc1612.tmp
[2024-06-27T07:23:59.746+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.7.41fba91c-9e2c-493d-9e89-59f8d9cc1612.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/7
24/06/27 07:23:59 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1719473039560,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:23:59.768+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.780+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.835+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.838+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.868+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.900+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.949+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:23:59.965+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:23:59.975+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:23:59.975+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:23:59.976+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:23:59.976+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:23:59.986+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:23:59.996+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:24:00.065+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:24:00.067+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:24:00.069+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:24:00.074+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:24:00.074+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-06-27T07:24:00.086+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:24:00.092+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-06-27T07:24:00.173+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 33 for partition store_source_data-0
[2024-06-27T07:24:00.183+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:24:00.680+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:24:00.684+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:24:00.692+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=34, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:24:00.694+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:24:00.696+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2024-06-27T07:24:00.744+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 7266 bytes result sent to driver
[2024-06-27T07:24:00.753+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 673 ms on localhost (executor driver) (1/1)
[2024-06-27T07:24:00.754+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-06-27T07:24:00.772+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.790 s
24/06/27 07:24:00 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:24:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-06-27T07:24:00.774+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.816057 s
[2024-06-27T07:24:00.775+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:24:00.776+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:24:00.777+0000] {docker.py:436} INFO - Batch: 7
[2024-06-27T07:24:00.779+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:24:01.039+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:24:01.041+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:24:01.088+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/7 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.7.320cf90c-59b4-44b8-affa-da5fa381e009.tmp
[2024-06-27T07:24:01.196+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.7.320cf90c-59b4-44b8-affa-da5fa381e009.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/7
[2024-06-27T07:24:01.198+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:23:59.557Z",
  "batchId" : 7,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 40.0,
  "processedRowsPerSecond" : 0.6105006105006106,
  "durationMs" : {
    "addBatch" : 1259,
    "commitOffsets" : 150,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 43,
    "triggerExecution" : 1638,
    "walCommit" : 182
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 40.0,
    "processedRowsPerSecond" : 0.6105006105006106,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:24:11.200+0000] {docker.py:436} INFO - 24/06/27 07:24:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:12.187+0000] {docker.py:436} INFO - 24/06/27 07:24:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:24:21.222+0000] {docker.py:436} INFO - 24/06/27 07:24:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:31.229+0000] {docker.py:436} INFO - 24/06/27 07:24:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:41.233+0000] {docker.py:436} INFO - 24/06/27 07:24:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:51.243+0000] {docker.py:436} INFO - 24/06/27 07:24:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:01.250+0000] {docker.py:436} INFO - 24/06/27 07:25:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:11.253+0000] {docker.py:436} INFO - 24/06/27 07:25:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:21.261+0000] {docker.py:436} INFO - 24/06/27 07:25:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:31.263+0000] {docker.py:436} INFO - 24/06/27 07:25:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:32.087+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/8 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.8.40576ecd-44a3-48b7-b836-4017f3eef55a.tmp
[2024-06-27T07:25:32.149+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.8.40576ecd-44a3-48b7-b836-4017f3eef55a.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/8
24/06/27 07:25:32 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1719473132070,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:25:32.185+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.186+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.205+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.215+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.231+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.233+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.261+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:25:32.263+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:25:32.266+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:25:32 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:25:32 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:25:32 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:25:32.274+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:25:32.279+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:25:32.316+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/27 07:25:32 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 07:25:32 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
24/06/27 07:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:25:32 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-06-27T07:25:32.318+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:25:32.322+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-06-27T07:25:32.379+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 34 for partition store_source_data-0
[2024-06-27T07:25:32.387+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:25:32.888+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:25:32.890+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=35, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:25:32.892+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:25:32.893+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2024-06-27T07:25:32.905+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 7266 bytes result sent to driver
[2024-06-27T07:25:32.908+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 591 ms on localhost (executor driver) (1/1)
[2024-06-27T07:25:32.910+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.633 s
[2024-06-27T07:25:32.911+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:25:32.912+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-06-27T07:25:32.913+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-06-27T07:25:32.913+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.649790 s
[2024-06-27T07:25:32.914+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:25:32.917+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:25:32.919+0000] {docker.py:436} INFO - Batch: 8
[2024-06-27T07:25:32.922+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:25:32.962+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:25:32.963+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:25:32.991+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/8 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.8.e9eb23c6-c950-48be-993d-7b5641054507.tmp
[2024-06-27T07:25:33.174+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.8.e9eb23c6-c950-48be-993d-7b5641054507.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/8
[2024-06-27T07:25:33.176+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:25:32.068Z",
  "batchId" : 8,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.9041591320072332,
  "durationMs" : {
    "addBatch" : 770,
    "commitOffsets" : 210,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 54,
    "triggerExecution" : 1106,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.9041591320072332,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:25:43.179+0000] {docker.py:436} INFO - 24/06/27 07:25:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:44.476+0000] {docker.py:436} INFO - 24/06/27 07:25:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:25:53.187+0000] {docker.py:436} INFO - 24/06/27 07:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:03.193+0000] {docker.py:436} INFO - 24/06/27 07:26:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:13.193+0000] {docker.py:436} INFO - 24/06/27 07:26:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:23.199+0000] {docker.py:436} INFO - 24/06/27 07:26:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:33.199+0000] {docker.py:436} INFO - 24/06/27 07:26:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:43.199+0000] {docker.py:436} INFO - 24/06/27 07:26:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:53.203+0000] {docker.py:436} INFO - 24/06/27 07:26:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:03.204+0000] {docker.py:436} INFO - 24/06/27 07:27:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:06.379+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/9 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.9.86f0b193-1233-4c52-be7e-ad584f10771d.tmp
[2024-06-27T07:27:06.454+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.9.86f0b193-1233-4c52-be7e-ad584f10771d.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/9
24/06/27 07:27:06 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1719473226361,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:27:06.470+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.474+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.498+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.508+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.553+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.557+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.589+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:27:06.590+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:27:06.590+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:27:06 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:27:06.591+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:27:06.592+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:27:06.595+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:27:06.596+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:27:06.611+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:27:06.612+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:27:06.615+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:27:06.616+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:27:06.617+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-06-27T07:27:06.619+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:27:06.627+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2024-06-27T07:27:06.669+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 35 for partition store_source_data-0
[2024-06-27T07:27:06.684+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:27:07.194+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:27:07.194+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:27:07.197+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:27:07.197+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:27:07.198+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2024-06-27T07:27:07.198+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 7180 bytes result sent to driver
[2024-06-27T07:27:07.206+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 588 ms on localhost (executor driver) (1/1)
[2024-06-27T07:27:07.207+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-06-27T07:27:07.213+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.617 s
[2024-06-27T07:27:07.213+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:27:07.217+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-06-27T07:27:07.237+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 9
-------------------------------------------
[2024-06-27T07:27:07.237+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.639988 s
24/06/27 07:27:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:27:07.389+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:27:07.402+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:27:07.505+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/9 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.9.f51cbd82-1b1c-4304-b47f-49541bc2c0d3.tmp
[2024-06-27T07:27:07.655+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.9.f51cbd82-1b1c-4304-b47f-49541bc2c0d3.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/9
[2024-06-27T07:27:07.672+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:27:06.359Z",
  "batchId" : 9,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.7722007722007722,
  "durationMs" : {
    "addBatch" : 923,
    "commitOffsets" : 248,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 33,
    "triggerExecution" : 1295,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.7722007722007722,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:27:17.666+0000] {docker.py:436} INFO - 24/06/27 07:27:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:19.769+0000] {docker.py:436} INFO - 24/06/27 07:27:19 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:27:27.673+0000] {docker.py:436} INFO - 24/06/27 07:27:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:37.683+0000] {docker.py:436} INFO - 24/06/27 07:27:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:47.686+0000] {docker.py:436} INFO - 24/06/27 07:27:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:57.689+0000] {docker.py:436} INFO - 24/06/27 07:27:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:07.691+0000] {docker.py:436} INFO - 24/06/27 07:28:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:17.699+0000] {docker.py:436} INFO - 24/06/27 07:28:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:27.699+0000] {docker.py:436} INFO - 24/06/27 07:28:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:37.704+0000] {docker.py:436} INFO - 24/06/27 07:28:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:47.713+0000] {docker.py:436} INFO - 24/06/27 07:28:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:57.724+0000] {docker.py:436} INFO - 24/06/27 07:28:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:07.735+0000] {docker.py:436} INFO - 24/06/27 07:29:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:17.738+0000] {docker.py:436} INFO - 24/06/27 07:29:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:27.748+0000] {docker.py:436} INFO - 24/06/27 07:29:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:37.753+0000] {docker.py:436} INFO - 24/06/27 07:29:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:47.758+0000] {docker.py:436} INFO - 24/06/27 07:29:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:57.759+0000] {docker.py:436} INFO - 24/06/27 07:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:07.764+0000] {docker.py:436} INFO - 24/06/27 07:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:17.766+0000] {docker.py:436} INFO - 24/06/27 07:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:27.769+0000] {docker.py:436} INFO - 24/06/27 07:30:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:37.783+0000] {docker.py:436} INFO - 24/06/27 07:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:47.789+0000] {docker.py:436} INFO - 24/06/27 07:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:57.791+0000] {docker.py:436} INFO - 24/06/27 07:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:07.796+0000] {docker.py:436} INFO - 24/06/27 07:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:17.797+0000] {docker.py:436} INFO - 24/06/27 07:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:27.804+0000] {docker.py:436} INFO - 24/06/27 07:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:37.810+0000] {docker.py:436} INFO - 24/06/27 07:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:47.813+0000] {docker.py:436} INFO - 24/06/27 07:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:57.823+0000] {docker.py:436} INFO - 24/06/27 07:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:07.833+0000] {docker.py:436} INFO - 24/06/27 07:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:17.837+0000] {docker.py:436} INFO - 24/06/27 07:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:27.845+0000] {docker.py:436} INFO - 24/06/27 07:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:35.483+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/10 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.10.f2589606-8402-4b5d-ba47-c59658731329.tmp
[2024-06-27T07:32:35.546+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.10.f2589606-8402-4b5d-ba47-c59658731329.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/10
[2024-06-27T07:32:35.546+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1719473555462,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:32:35.580+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.581+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.604+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.615+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.649+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.656+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.689+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:32:35.690+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:32:35.691+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:32:35 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:32:35.693+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:32:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:32:35.693+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:32:35.708+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:32:35.716+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:32:35.728+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:32:35.730+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:32:35.731+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:32:35.732+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-06-27T07:32:35.733+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:32:35.741+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2024-06-27T07:32:35.754+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 36 for partition store_source_data-0
[2024-06-27T07:32:35.760+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:32:36.262+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:32:36.262+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:32:36.263+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=37, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:32:36.264+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:32:36 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2024-06-27T07:32:36.266+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 7180 bytes result sent to driver
[2024-06-27T07:32:36.272+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 539 ms on localhost (executor driver) (1/1)
24/06/27 07:32:36 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-06-27T07:32:36.273+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.569 s
[2024-06-27T07:32:36.274+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:32:36.275+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-06-27T07:32:36.275+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.584749 s
[2024-06-27T07:32:36.276+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:32:36.276+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:32:36.277+0000] {docker.py:436} INFO - Batch: 10
[2024-06-27T07:32:36.278+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:32:36.319+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:32:36.320+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:32:36.344+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/10 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.10.2ec865c6-0431-47d7-9cab-b4acbb68e1d6.tmp
[2024-06-27T07:32:36.405+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.10.2ec865c6-0431-47d7-9cab-b4acbb68e1d6.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/10
[2024-06-27T07:32:36.408+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:32:35.460Z",
  "batchId" : 10,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 1.0593220338983051,
  "durationMs" : {
    "addBatch" : 728,
    "commitOffsets" : 83,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 46,
    "triggerExecution" : 944,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 1.0593220338983051,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:32:46.163+0000] {docker.py:436} INFO - 24/06/27 07:32:46 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:32:46.415+0000] {docker.py:436} INFO - 24/06/27 07:32:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:56.420+0000] {docker.py:436} INFO - 24/06/27 07:32:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:06.428+0000] {docker.py:436} INFO - 24/06/27 07:33:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:16.437+0000] {docker.py:436} INFO - 24/06/27 07:33:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:26.440+0000] {docker.py:436} INFO - 24/06/27 07:33:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:36.445+0000] {docker.py:436} INFO - 24/06/27 07:33:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:46.446+0000] {docker.py:436} INFO - 24/06/27 07:33:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:56.449+0000] {docker.py:436} INFO - 24/06/27 07:33:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:06.458+0000] {docker.py:436} INFO - 24/06/27 07:34:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:16.466+0000] {docker.py:436} INFO - 24/06/27 07:34:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:26.473+0000] {docker.py:436} INFO - 24/06/27 07:34:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:36.483+0000] {docker.py:436} INFO - 24/06/27 07:34:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:46.494+0000] {docker.py:436} INFO - 24/06/27 07:34:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:56.503+0000] {docker.py:436} INFO - 24/06/27 07:34:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:06.505+0000] {docker.py:436} INFO - 24/06/27 07:35:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:16.509+0000] {docker.py:436} INFO - 24/06/27 07:35:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:26.519+0000] {docker.py:436} INFO - 24/06/27 07:35:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:36.529+0000] {docker.py:436} INFO - 24/06/27 07:35:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:46.532+0000] {docker.py:436} INFO - 24/06/27 07:35:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:56.536+0000] {docker.py:436} INFO - 24/06/27 07:35:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:06.540+0000] {docker.py:436} INFO - 24/06/27 07:36:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:16.544+0000] {docker.py:436} INFO - 24/06/27 07:36:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:26.552+0000] {docker.py:436} INFO - 24/06/27 07:36:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:36.559+0000] {docker.py:436} INFO - 24/06/27 07:36:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:46.566+0000] {docker.py:436} INFO - 24/06/27 07:36:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:56.571+0000] {docker.py:436} INFO - 24/06/27 07:36:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:06.572+0000] {docker.py:436} INFO - 24/06/27 07:37:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:16.581+0000] {docker.py:436} INFO - 24/06/27 07:37:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:26.581+0000] {docker.py:436} INFO - 24/06/27 07:37:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:36.589+0000] {docker.py:436} INFO - 24/06/27 07:37:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:46.600+0000] {docker.py:436} INFO - 24/06/27 07:37:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:56.601+0000] {docker.py:436} INFO - 24/06/27 07:37:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:06.609+0000] {docker.py:436} INFO - 24/06/27 07:38:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:16.612+0000] {docker.py:436} INFO - 24/06/27 07:38:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:26.615+0000] {docker.py:436} INFO - 24/06/27 07:38:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:36.175+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO Metrics: Metrics scheduler closed
[2024-06-27T07:38:36.175+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T07:38:36.176+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO Metrics: Metrics reporters closed
[2024-06-27T07:38:36.182+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-1 unregistered
[2024-06-27T07:38:36.617+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:46.618+0000] {docker.py:436} INFO - 24/06/27 07:38:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:56.620+0000] {docker.py:436} INFO - 24/06/27 07:38:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:06.623+0000] {docker.py:436} INFO - 24/06/27 07:39:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:16.629+0000] {docker.py:436} INFO - 24/06/27 07:39:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:26.635+0000] {docker.py:436} INFO - 24/06/27 07:39:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:36.645+0000] {docker.py:436} INFO - 24/06/27 07:39:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:46.656+0000] {docker.py:436} INFO - 24/06/27 07:39:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:56.660+0000] {docker.py:436} INFO - 24/06/27 07:39:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:06.666+0000] {docker.py:436} INFO - 24/06/27 07:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:16.672+0000] {docker.py:436} INFO - 24/06/27 07:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:26.681+0000] {docker.py:436} INFO - 24/06/27 07:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:36.686+0000] {docker.py:436} INFO - 24/06/27 07:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:46.695+0000] {docker.py:436} INFO - 24/06/27 07:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:56.701+0000] {docker.py:436} INFO - 24/06/27 07:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:06.707+0000] {docker.py:436} INFO - 24/06/27 07:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:16.710+0000] {docker.py:436} INFO - 24/06/27 07:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:26.717+0000] {docker.py:436} INFO - 24/06/27 07:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:36.718+0000] {docker.py:436} INFO - 24/06/27 07:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:46.718+0000] {docker.py:436} INFO - 24/06/27 07:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:56.723+0000] {docker.py:436} INFO - 24/06/27 07:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:06.729+0000] {docker.py:436} INFO - 24/06/27 07:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:16.735+0000] {docker.py:436} INFO - 24/06/27 07:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:26.741+0000] {docker.py:436} INFO - 24/06/27 07:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:36.742+0000] {docker.py:436} INFO - 24/06/27 07:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:46.752+0000] {docker.py:436} INFO - 24/06/27 07:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:56.759+0000] {docker.py:436} INFO - 24/06/27 07:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:06.767+0000] {docker.py:436} INFO - 24/06/27 07:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:16.771+0000] {docker.py:436} INFO - 24/06/27 07:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:26.777+0000] {docker.py:436} INFO - 24/06/27 07:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:36.783+0000] {docker.py:436} INFO - 24/06/27 07:43:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:46.787+0000] {docker.py:436} INFO - 24/06/27 07:43:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:56.791+0000] {docker.py:436} INFO - 24/06/27 07:43:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:06.802+0000] {docker.py:436} INFO - 24/06/27 07:44:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:16.808+0000] {docker.py:436} INFO - 24/06/27 07:44:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:26.818+0000] {docker.py:436} INFO - 24/06/27 07:44:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:36.822+0000] {docker.py:436} INFO - 24/06/27 07:44:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:46.832+0000] {docker.py:436} INFO - 24/06/27 07:44:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:56.836+0000] {docker.py:436} INFO - 24/06/27 07:44:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:06.847+0000] {docker.py:436} INFO - 24/06/27 07:45:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:16.853+0000] {docker.py:436} INFO - 24/06/27 07:45:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:26.859+0000] {docker.py:436} INFO - 24/06/27 07:45:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:36.865+0000] {docker.py:436} INFO - 24/06/27 07:45:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:46.865+0000] {docker.py:436} INFO - 24/06/27 07:45:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:56.869+0000] {docker.py:436} INFO - 24/06/27 07:45:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:06.873+0000] {docker.py:436} INFO - 24/06/27 07:46:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:16.879+0000] {docker.py:436} INFO - 24/06/27 07:46:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:26.888+0000] {docker.py:436} INFO - 24/06/27 07:46:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:36.889+0000] {docker.py:436} INFO - 24/06/27 07:46:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:46.901+0000] {docker.py:436} INFO - 24/06/27 07:46:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:56.905+0000] {docker.py:436} INFO - 24/06/27 07:46:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:06.906+0000] {docker.py:436} INFO - 24/06/27 07:47:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:16.912+0000] {docker.py:436} INFO - 24/06/27 07:47:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:26.912+0000] {docker.py:436} INFO - 24/06/27 07:47:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:36.921+0000] {docker.py:436} INFO - 24/06/27 07:47:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:46.928+0000] {docker.py:436} INFO - 24/06/27 07:47:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:56.931+0000] {docker.py:436} INFO - 24/06/27 07:47:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:59.895+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/11 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.11.e2beba8a-658e-410e-a68a-45c38c89bfe2.tmp
[2024-06-27T07:47:59.941+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.11.e2beba8a-658e-410e-a68a-45c38c89bfe2.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/11
[2024-06-27T07:47:59.942+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1719474479883,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:47:59.972+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.979+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.999+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.009+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.028+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.029+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.056+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:48:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:48:00 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:48:00 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:48:00 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:48:00 INFO DAGScheduler: Missing parents: List()
24/06/27 07:48:00 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 07:48:00 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
24/06/27 07:48:00 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:48:00.057+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:48:00.061+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:48:00.062+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:48:00.065+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-06-27T07:48:00.071+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:48:00.089+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2024-06-27T07:48:00.125+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T07:48:00.155+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:48:00.157+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T07:48:00.158+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka startTimeMs: 1719474480153
24/06/27 07:48:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Subscribed to partition(s): store_source_data-0
24/06/27 07:48:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 37 for partition store_source_data-0
[2024-06-27T07:48:00.185+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T07:48:00.209+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:48:00.712+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:48:00.713+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:48:00.730+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:48:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:48:00 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
24/06/27 07:48:00 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 7180 bytes result sent to driver
[2024-06-27T07:48:00.739+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 671 ms on localhost (executor driver) (1/1)
[2024-06-27T07:48:00.740+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-06-27T07:48:00.742+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.696 s
[2024-06-27T07:48:00.745+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:48:00.747+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-06-27T07:48:00.749+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.703140 s
[2024-06-27T07:48:00.751+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:48:00.780+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 11
-------------------------------------------
[2024-06-27T07:48:00.966+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:48:00.971+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:48:01.035+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/11 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.11.66cd4c94-ade2-4863-b699-642760152a4b.tmp
[2024-06-27T07:48:01.456+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.11.66cd4c94-ade2-4863-b699-642760152a4b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/11
[2024-06-27T07:48:01.469+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:47:59.882Z",
  "batchId" : 11,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6353240152477764,
  "durationMs" : {
    "addBatch" : 998,
    "commitOffsets" : 475,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 41,
    "triggerExecution" : 1574,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6353240152477764,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:48:11.410+0000] {docker.py:436} INFO - 24/06/27 07:48:11 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:48:11.468+0000] {docker.py:436} INFO - 24/06/27 07:48:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:21.468+0000] {docker.py:436} INFO - 24/06/27 07:48:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:31.469+0000] {docker.py:436} INFO - 24/06/27 07:48:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:41.476+0000] {docker.py:436} INFO - 24/06/27 07:48:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:51.487+0000] {docker.py:436} INFO - 24/06/27 07:48:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:01.489+0000] {docker.py:436} INFO - 24/06/27 07:49:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:11.498+0000] {docker.py:436} INFO - 24/06/27 07:49:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:21.500+0000] {docker.py:436} INFO - 24/06/27 07:49:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:31.509+0000] {docker.py:436} INFO - 24/06/27 07:49:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:41.511+0000] {docker.py:436} INFO - 24/06/27 07:49:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:51.520+0000] {docker.py:436} INFO - 24/06/27 07:49:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:59.767+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/12 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.12.683567f1-3159-416a-b8c4-0deb39fc53e6.tmp
[2024-06-27T07:49:59.818+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.12.683567f1-3159-416a-b8c4-0deb39fc53e6.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/12
[2024-06-27T07:49:59.819+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1719474599741,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:49:59.874+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.877+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.909+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.931+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.962+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.968+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.990+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:49:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:49:59 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:49:59 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:49:59 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:49:59 INFO DAGScheduler: Missing parents: List()
24/06/27 07:49:59 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:49:59.990+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:49:59.992+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:49:59.994+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:49:59.995+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:49:59.995+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:49:59.996+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-06-27T07:49:59.997+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:50:00.004+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2024-06-27T07:50:00.021+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 38 for partition store_source_data-0
[2024-06-27T07:50:00.029+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:50:00.531+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:50:00.531+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:50:00.532+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:50:00.532+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2024-06-27T07:50:00.534+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 7180 bytes result sent to driver
[2024-06-27T07:50:00.536+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 540 ms on localhost (executor driver) (1/1)
[2024-06-27T07:50:00.536+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-06-27T07:50:00.537+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.550 s
[2024-06-27T07:50:00.538+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:50:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-06-27T07:50:00.538+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.553176 s
[2024-06-27T07:50:00.539+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:50:00.540+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:50:00.541+0000] {docker.py:436} INFO - Batch: 12
[2024-06-27T07:50:00.541+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:50:00.638+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:50:00.638+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:50:00.658+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/12 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.12.07da2766-cda6-4890-b731-b444cf889296.tmp
[2024-06-27T07:50:00.708+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.12.07da2766-cda6-4890-b731-b444cf889296.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/12
[2024-06-27T07:50:00.709+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:49:59.734Z",
  "batchId" : 12,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.027749229188078,
  "durationMs" : {
    "addBatch" : 745,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 66,
    "triggerExecution" : 973,
    "walCommit" : 77
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.027749229188078,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:50:10.711+0000] {docker.py:436} INFO - 24/06/27 07:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:13.608+0000] {docker.py:436} INFO - 24/06/27 07:50:13 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:50:20.717+0000] {docker.py:436} INFO - 24/06/27 07:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:30.720+0000] {docker.py:436} INFO - 24/06/27 07:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:40.722+0000] {docker.py:436} INFO - 24/06/27 07:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:50.729+0000] {docker.py:436} INFO - 24/06/27 07:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:00.730+0000] {docker.py:436} INFO - 24/06/27 07:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:10.735+0000] {docker.py:436} INFO - 24/06/27 07:51:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:20.740+0000] {docker.py:436} INFO - 24/06/27 07:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:30.745+0000] {docker.py:436} INFO - 24/06/27 07:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:40.752+0000] {docker.py:436} INFO - 24/06/27 07:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:42.113+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/13 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.13.6b360827-775c-4b5e-adc1-11ceb71901c1.tmp
[2024-06-27T07:51:42.294+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.13.6b360827-775c-4b5e-adc1-11ceb71901c1.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/13
[2024-06-27T07:51:42.305+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1719474702096,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:51:42.345+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.377+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.428+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.450+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.519+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.523+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.548+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:51:42.549+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:51:42.551+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:51:42 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:51:42.551+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:51:42 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:51:42.552+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:51:42.553+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:51:42.555+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:51:42.557+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:51:42.557+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:51:42.558+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:51:42.558+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-06-27T07:51:42.577+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:51:42.589+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2024-06-27T07:51:42.623+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 39 for partition store_source_data-0
[2024-06-27T07:51:42.698+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:51:43.203+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:51:43.214+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:51:43.214+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:51:43.215+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2024-06-27T07:51:43.215+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 7180 bytes result sent to driver
[2024-06-27T07:51:43.218+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 653 ms on localhost (executor driver) (1/1)
[2024-06-27T07:51:43.223+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-06-27T07:51:43.227+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 13
-------------------------------------------
[2024-06-27T07:51:43.228+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 0.671 s
24/06/27 07:51:43 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:51:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
24/06/27 07:51:43 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.676279 s
24/06/27 07:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:51:43.321+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:51:43.322+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:51:43.376+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/13 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.13.d6f9cca6-3580-4521-bc96-7f8fbc2ae55f.tmp
[2024-06-27T07:51:43.501+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.13.d6f9cca6-3580-4521-bc96-7f8fbc2ae55f.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/13
[2024-06-27T07:51:43.508+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:51:42.093Z",
  "batchId" : 13,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7102272727272728,
  "durationMs" : {
    "addBatch" : 927,
    "commitOffsets" : 175,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 90,
    "triggerExecution" : 1408,
    "walCommit" : 210
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7102272727272728,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:51:46.489+0000] {docker.py:436} INFO - 24/06/27 07:51:46 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:51:53.505+0000] {docker.py:436} INFO - 24/06/27 07:51:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:03.515+0000] {docker.py:436} INFO - 24/06/27 07:52:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:13.528+0000] {docker.py:436} INFO - 24/06/27 07:52:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:23.536+0000] {docker.py:436} INFO - 24/06/27 07:52:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:33.546+0000] {docker.py:436} INFO - 24/06/27 07:52:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:43.552+0000] {docker.py:436} INFO - 24/06/27 07:52:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:53.553+0000] {docker.py:436} INFO - 24/06/27 07:52:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:59.319+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/14 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.14.89be621b-4b67-409a-bd35-e77346168654.tmp
[2024-06-27T07:52:59.375+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.14.89be621b-4b67-409a-bd35-e77346168654.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/14
[2024-06-27T07:52:59.375+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1719474779306,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:52:59.388+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.389+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.400+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.420+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.431+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:52:59.433+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:52:59.434+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:52:59 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:52:59 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:52:59 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:52:59.435+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:52:59.456+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:52:59.460+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:52:59.464+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:52:59.465+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:52:59.466+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:52:59.475+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-06-27T07:52:59.476+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:52:59.480+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2024-06-27T07:52:59.559+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 40 for partition store_source_data-0
[2024-06-27T07:52:59.573+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:53:00.067+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:53:00.072+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=41, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:53:00.072+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:53:00.073+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
[2024-06-27T07:53:00.078+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 7223 bytes result sent to driver
[2024-06-27T07:53:00.079+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 605 ms on localhost (executor driver) (1/1)
[2024-06-27T07:53:00.082+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-06-27T07:53:00.091+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 14
-------------------------------------------
[2024-06-27T07:53:00.092+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 0.631 s
24/06/27 07:53:00 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:53:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/06/27 07:53:00 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.653396 s
24/06/27 07:53:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:53:00.167+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:53:00.168+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:53:00.181+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/14 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.14.a1e6c145-c304-480c-af3a-66075b5c85cf.tmp
[2024-06-27T07:53:00.240+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.14.a1e6c145-c304-480c-af3a-66075b5c85cf.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/14
[2024-06-27T07:53:00.241+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:52:59.305Z",
  "batchId" : 14,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.070663811563169,
  "durationMs" : {
    "addBatch" : 776,
    "commitOffsets" : 73,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 15,
    "triggerExecution" : 934,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.070663811563169,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:53:07.119+0000] {docker.py:436} INFO - 24/06/27 07:53:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:53:10.252+0000] {docker.py:436} INFO - 24/06/27 07:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:20.261+0000] {docker.py:436} INFO - 24/06/27 07:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:30.272+0000] {docker.py:436} INFO - 24/06/27 07:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:40.271+0000] {docker.py:436} INFO - 24/06/27 07:53:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:50.274+0000] {docker.py:436} INFO - 24/06/27 07:53:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:00.273+0000] {docker.py:436} INFO - 24/06/27 07:54:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:10.278+0000] {docker.py:436} INFO - 24/06/27 07:54:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:20.286+0000] {docker.py:436} INFO - 24/06/27 07:54:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:30.295+0000] {docker.py:436} INFO - 24/06/27 07:54:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:40.295+0000] {docker.py:436} INFO - 24/06/27 07:54:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:47.764+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/15 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.15.6b83f774-819a-42f6-b434-d37bf0b353fe.tmp
[2024-06-27T07:54:47.820+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.15.6b83f774-819a-42f6-b434-d37bf0b353fe.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/15
[2024-06-27T07:54:47.821+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1719474887751,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:54:47.842+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.851+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.881+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.903+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.920+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.953+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:54:47.966+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:54:47.967+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:54:47.967+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:54:47.968+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:54:47.968+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:54:47.969+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[47] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:54:47.974+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:54:47.976+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:54:47.977+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:54:47.978+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:54:47.979+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[47] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:54:47.980+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-06-27T07:54:47.982+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:54:47.984+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2024-06-27T07:54:48.004+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 41 for partition store_source_data-0
[2024-06-27T07:54:48.009+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:54:48.513+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:54:48.520+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:54:48 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:54:48 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2024-06-27T07:54:48.533+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 7223 bytes result sent to driver
24/06/27 07:54:48 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 550 ms on localhost (executor driver) (1/1)
24/06/27 07:54:48 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/06/27 07:54:48 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 0.563 s
24/06/27 07:54:48 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:54:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-06-27T07:54:48.542+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.576148 s
[2024-06-27T07:54:48.544+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 15
-------------------------------------------
[2024-06-27T07:54:48.544+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:54:48.593+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:54:48.597+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:54:48.616+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/15 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.15.2c0b520f-535a-4509-9615-76976975a118.tmp
[2024-06-27T07:54:48.682+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.15.2c0b520f-535a-4509-9615-76976975a118.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/15
[2024-06-27T07:54:48.684+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:54:47.748Z",
  "batchId" : 15,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.070663811563169,
  "durationMs" : {
    "addBatch" : 747,
    "commitOffsets" : 86,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 29,
    "triggerExecution" : 934,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.070663811563169,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:54:53.743+0000] {docker.py:436} INFO - 24/06/27 07:54:53 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:54:58.694+0000] {docker.py:436} INFO - 24/06/27 07:54:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:08.696+0000] {docker.py:436} INFO - 24/06/27 07:55:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:18.697+0000] {docker.py:436} INFO - 24/06/27 07:55:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:28.700+0000] {docker.py:436} INFO - 24/06/27 07:55:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:38.706+0000] {docker.py:436} INFO - 24/06/27 07:55:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:48.709+0000] {docker.py:436} INFO - 24/06/27 07:55:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:58.714+0000] {docker.py:436} INFO - 24/06/27 07:55:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:08.722+0000] {docker.py:436} INFO - 24/06/27 07:56:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:18.725+0000] {docker.py:436} INFO - 24/06/27 07:56:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:28.733+0000] {docker.py:436} INFO - 24/06/27 07:56:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:38.738+0000] {docker.py:436} INFO - 24/06/27 07:56:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:48.748+0000] {docker.py:436} INFO - 24/06/27 07:56:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:58.751+0000] {docker.py:436} INFO - 24/06/27 07:56:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:08.757+0000] {docker.py:436} INFO - 24/06/27 07:57:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:18.764+0000] {docker.py:436} INFO - 24/06/27 07:57:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:28.772+0000] {docker.py:436} INFO - 24/06/27 07:57:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:38.781+0000] {docker.py:436} INFO - 24/06/27 07:57:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:48.786+0000] {docker.py:436} INFO - 24/06/27 07:57:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:58.793+0000] {docker.py:436} INFO - 24/06/27 07:57:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:08.795+0000] {docker.py:436} INFO - 24/06/27 07:58:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:18.805+0000] {docker.py:436} INFO - 24/06/27 07:58:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:28.812+0000] {docker.py:436} INFO - 24/06/27 07:58:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:38.813+0000] {docker.py:436} INFO - 24/06/27 07:58:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:47.989+0000] {docker.py:436} INFO - 24/06/27 07:58:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/16 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.16.d8d5c8a2-4720-405c-9ed6-2c3da1d760b5.tmp
[2024-06-27T07:58:48.060+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.16.d8d5c8a2-4720-405c-9ed6-2c3da1d760b5.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/16
24/06/27 07:58:48 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1719475127970,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:58:48.068+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.083+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.107+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.116+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.139+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.142+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.164+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:58:48.166+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:58:48.167+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:58:48 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:58:48 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:58:48.168+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:58:48.170+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[50] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:58:48.173+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:58:48.177+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/27 07:58:48 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:58:48.184+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:58:48.196+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[50] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:58:48.197+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-06-27T07:58:48.202+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:58:48.204+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2024-06-27T07:58:48.221+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 42 for partition store_source_data-0
[2024-06-27T07:58:48.226+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:58:48.737+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:58:48.737+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:58:48.738+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=43, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:58:48.753+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:58:48.754+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2024-06-27T07:58:48.758+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 7180 bytes result sent to driver
[2024-06-27T07:58:48.762+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 558 ms on localhost (executor driver) (1/1)
[2024-06-27T07:58:48.763+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-06-27T07:58:48.766+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.594 s
[2024-06-27T07:58:48.766+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:58:48.768+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2024-06-27T07:58:48.777+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 16
-------------------------------------------
[2024-06-27T07:58:48.778+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.607054 s
24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:58:48.926+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:58:48.927+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:58:48.975+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/16 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.16.a53e3e0d-6299-42bb-bcd0-78cbc20ce21b.tmp
[2024-06-27T07:58:49.048+0000] {docker.py:436} INFO - 24/06/27 07:58:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.16.a53e3e0d-6299-42bb-bcd0-78cbc20ce21b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/16
[2024-06-27T07:58:49.049+0000] {docker.py:436} INFO - 24/06/27 07:58:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:58:47.969Z",
  "batchId" : 16,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9267840593141798,
  "durationMs" : {
    "addBatch" : 841,
    "commitOffsets" : 120,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 1079,
    "walCommit" : 87
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9267840593141798,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:58:49.650+0000] {docker.py:436} INFO - 24/06/27 07:58:49 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:58:59.062+0000] {docker.py:436} INFO - 24/06/27 07:58:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:09.066+0000] {docker.py:436} INFO - 24/06/27 07:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:19.069+0000] {docker.py:436} INFO - 24/06/27 07:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:29.079+0000] {docker.py:436} INFO - 24/06/27 07:59:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:34.862+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/17 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.17.8086722e-f4b1-4a85-b64f-992e11daf22c.tmp
[2024-06-27T07:59:34.906+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.17.8086722e-f4b1-4a85-b64f-992e11daf22c.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/17
[2024-06-27T07:59:34.906+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1719475174843,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:59:34.929+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.944+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.972+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.972+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:35.002+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:35.006+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:35.029+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:59:35.031+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:59:35.032+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:59:35.033+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:59:35.034+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:59:35.035+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:59:35.037+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[53] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:59:35.042+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T07:59:35.046+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T07:59:35.052+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 07:59:35 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:59:35.053+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[53] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:59:35.054+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-06-27T07:59:35.058+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T07:59:35.061+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2024-06-27T07:59:35.073+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 43 for partition store_source_data-0
[2024-06-27T07:59:35.081+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:59:35.608+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:59:35.613+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:59:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:59:35 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2024-06-27T07:59:35.614+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 7180 bytes result sent to driver
[2024-06-27T07:59:35.620+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 555 ms on localhost (executor driver) (1/1)
[2024-06-27T07:59:35.621+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-06-27T07:59:35.622+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
[2024-06-27T07:59:35.622+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:59:35.623+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-06-27T07:59:35.624+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.587487 s
[2024-06-27T07:59:35.624+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:59:35.636+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 17
-------------------------------------------
[2024-06-27T07:59:35.717+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T07:59:35.717+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:59:35.810+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/17 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.17.5f50e8a2-fb75-46c7-8f7f-a29814e4434e.tmp
[2024-06-27T07:59:35.959+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.17.5f50e8a2-fb75-46c7-8f7f-a29814e4434e.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/17
24/06/27 07:59:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T07:59:34.842Z",
  "batchId" : 17,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8960573476702508,
  "durationMs" : {
    "addBatch" : 775,
    "commitOffsets" : 228,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 47,
    "triggerExecution" : 1116,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8960573476702508,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:59:43.133+0000] {docker.py:436} INFO - 24/06/27 07:59:43 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T07:59:45.963+0000] {docker.py:436} INFO - 24/06/27 07:59:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:55.971+0000] {docker.py:436} INFO - 24/06/27 07:59:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:05.973+0000] {docker.py:436} INFO - 24/06/27 08:00:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:15.991+0000] {docker.py:436} INFO - 24/06/27 08:00:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:25.802+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/18 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.18.d3b1c34e-13f0-4642-b783-40ca92ad7fe4.tmp
[2024-06-27T08:00:25.846+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.18.d3b1c34e-13f0-4642-b783-40ca92ad7fe4.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/18
24/06/27 08:00:25 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1719475225787,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:00:25.872+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.878+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.897+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.900+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.933+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.939+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.966+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:00:25.971+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:00:25 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:00:25 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:00:25 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:00:25 INFO DAGScheduler: Missing parents: List()
24/06/27 08:00:25 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:00:25.974+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:00:25.976+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:00:25.977+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:00:25.978+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:00:25.979+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:00:25.980+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-06-27T08:00:25.984+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:00:25.985+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2024-06-27T08:00:26.004+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 44 for partition store_source_data-0
[2024-06-27T08:00:26.009+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:00:26.512+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:00:26.513+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:00:26.514+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:00:26.524+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2024-06-27T08:00:26.525+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 7180 bytes result sent to driver
[2024-06-27T08:00:26.526+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 545 ms on localhost (executor driver) (1/1)
24/06/27 08:00:26 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-06-27T08:00:26.526+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.558 s
[2024-06-27T08:00:26.527+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-06-27T08:00:26.539+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.570924 s
24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:00:26.539+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 18
-------------------------------------------
[2024-06-27T08:00:26.661+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:00:26.667+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:00:26.741+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/18 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.18.6c892dc8-db13-48b7-8df7-146499facef6.tmp
[2024-06-27T08:00:26.832+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.18.6c892dc8-db13-48b7-8df7-146499facef6.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/18
[2024-06-27T08:00:26.834+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:00:25.786Z",
  "batchId" : 18,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9560229445506692,
  "durationMs" : {
    "addBatch" : 807,
    "commitOffsets" : 149,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 29,
    "triggerExecution" : 1046,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9560229445506692,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:00:36.625+0000] {docker.py:436} INFO - 24/06/27 08:00:36 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:00:36.842+0000] {docker.py:436} INFO - 24/06/27 08:00:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:46.846+0000] {docker.py:436} INFO - 24/06/27 08:00:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:56.852+0000] {docker.py:436} INFO - 24/06/27 08:00:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:06.856+0000] {docker.py:436} INFO - 24/06/27 08:01:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:16.858+0000] {docker.py:436} INFO - 24/06/27 08:01:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:26.869+0000] {docker.py:436} INFO - 24/06/27 08:01:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:36.874+0000] {docker.py:436} INFO - 24/06/27 08:01:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:46.878+0000] {docker.py:436} INFO - 24/06/27 08:01:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:56.882+0000] {docker.py:436} INFO - 24/06/27 08:01:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:06.892+0000] {docker.py:436} INFO - 24/06/27 08:02:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:16.902+0000] {docker.py:436} INFO - 24/06/27 08:02:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:26.908+0000] {docker.py:436} INFO - 24/06/27 08:02:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:36.911+0000] {docker.py:436} INFO - 24/06/27 08:02:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:46.917+0000] {docker.py:436} INFO - 24/06/27 08:02:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:56.921+0000] {docker.py:436} INFO - 24/06/27 08:02:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:04.438+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/19 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.19.d601a496-2688-4433-b15e-065c73d9043f.tmp
[2024-06-27T08:03:04.491+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.19.d601a496-2688-4433-b15e-065c73d9043f.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/19
24/06/27 08:03:04 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1719475384425,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:03:04.509+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.517+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.541+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.551+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.572+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.582+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.622+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:03:04.633+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:03:04.633+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:03:04.634+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:03:04.634+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:03:04.634+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:03:04.635+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:03:04.642+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:03:04.646+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/27 08:03:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:03:04.647+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:03:04.648+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:03:04.649+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-06-27T08:03:04.651+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:03:04.654+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2024-06-27T08:03:04.669+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 45 for partition store_source_data-0
[2024-06-27T08:03:04.673+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:03:05.179+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:03:05.180+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:03:05.181+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:03:05.182+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2024-06-27T08:03:05.183+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 7180 bytes result sent to driver
[2024-06-27T08:03:05.191+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 541 ms on localhost (executor driver) (1/1)
[2024-06-27T08:03:05.192+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-06-27T08:03:05.194+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 0.554 s
[2024-06-27T08:03:05.203+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:03:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-06-27T08:03:05.203+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.574377 s
[2024-06-27T08:03:05.205+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:03:05.205+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:03:05.206+0000] {docker.py:436} INFO - Batch: 19
[2024-06-27T08:03:05.206+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:03:05.268+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:03:05.272+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:03:05.288+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/19 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.19.cf3660f4-bc17-485b-89fb-22274308fc95.tmp
[2024-06-27T08:03:05.363+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.19.cf3660f4-bc17-485b-89fb-22274308fc95.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/19
24/06/27 08:03:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:03:04.423Z",
  "batchId" : 19,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.0660980810234542,
  "durationMs" : {
    "addBatch" : 736,
    "commitOffsets" : 90,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 938,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.0660980810234542,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:03:13.649+0000] {docker.py:436} INFO - 24/06/27 08:03:13 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:03:15.382+0000] {docker.py:436} INFO - 24/06/27 08:03:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:25.377+0000] {docker.py:436} INFO - 24/06/27 08:03:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:35.376+0000] {docker.py:436} INFO - 24/06/27 08:03:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:45.378+0000] {docker.py:436} INFO - 24/06/27 08:03:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:55.381+0000] {docker.py:436} INFO - 24/06/27 08:03:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:05.389+0000] {docker.py:436} INFO - 24/06/27 08:04:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:15.394+0000] {docker.py:436} INFO - 24/06/27 08:04:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:25.399+0000] {docker.py:436} INFO - 24/06/27 08:04:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:32.986+0000] {docker.py:436} INFO - 24/06/27 08:04:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/20 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.20.aa50e1d3-f373-4d50-a7f5-58806df4d6e0.tmp
[2024-06-27T08:04:33.050+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.20.aa50e1d3-f373-4d50-a7f5-58806df4d6e0.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/20
[2024-06-27T08:04:33.051+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1719475472970,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:04:33.071+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.073+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.087+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.092+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.107+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.108+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.134+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:04:33.148+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:04:33.154+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:04:33 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:04:33 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:04:33 INFO DAGScheduler: Missing parents: List()
24/06/27 08:04:33 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[62] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:04:33 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:04:33.155+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:04:33.160+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 08:04:33 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
24/06/27 08:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[62] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:04:33 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-06-27T08:04:33.166+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:04:33.173+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2024-06-27T08:04:33.216+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 46 for partition store_source_data-0
[2024-06-27T08:04:33.225+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:04:33.719+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:04:33.720+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:04:33.721+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=47, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:04:33.723+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:04:33 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2024-06-27T08:04:33.723+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 7180 bytes result sent to driver
[2024-06-27T08:04:33.726+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 561 ms on localhost (executor driver) (1/1)
[2024-06-27T08:04:33.726+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-06-27T08:04:33.727+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s
[2024-06-27T08:04:33.728+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:04:33.729+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-06-27T08:04:33.729+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.580297 s
[2024-06-27T08:04:33.730+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:04:33.731+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:04:33.731+0000] {docker.py:436} INFO - Batch: 20
[2024-06-27T08:04:33.731+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:04:33.852+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:04:33.853+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:04:33.880+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/20 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.20.4829dc7a-89ba-4198-b6fa-e42c5e0e2d35.tmp
[2024-06-27T08:04:33.917+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.20.4829dc7a-89ba-4198-b6fa-e42c5e0e2d35.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/20
[2024-06-27T08:04:33.918+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:04:32.968Z",
  "batchId" : 20,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 1.053740779768177,
  "durationMs" : {
    "addBatch" : 779,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 24,
    "triggerExecution" : 949,
    "walCommit" : 80
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 1.053740779768177,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:04:43.926+0000] {docker.py:436} INFO - 24/06/27 08:04:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:45.548+0000] {docker.py:436} INFO - 24/06/27 08:04:45 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:04:53.936+0000] {docker.py:436} INFO - 24/06/27 08:04:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:03.944+0000] {docker.py:436} INFO - 24/06/27 08:05:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:13.947+0000] {docker.py:436} INFO - 24/06/27 08:05:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:23.957+0000] {docker.py:436} INFO - 24/06/27 08:05:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:33.957+0000] {docker.py:436} INFO - 24/06/27 08:05:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:43.958+0000] {docker.py:436} INFO - 24/06/27 08:05:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:53.960+0000] {docker.py:436} INFO - 24/06/27 08:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:03.969+0000] {docker.py:436} INFO - 24/06/27 08:06:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:13.977+0000] {docker.py:436} INFO - 24/06/27 08:06:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:23.983+0000] {docker.py:436} INFO - 24/06/27 08:06:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:33.983+0000] {docker.py:436} INFO - 24/06/27 08:06:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:43.988+0000] {docker.py:436} INFO - 24/06/27 08:06:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:53.994+0000] {docker.py:436} INFO - 24/06/27 08:06:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:03.999+0000] {docker.py:436} INFO - 24/06/27 08:07:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:14.006+0000] {docker.py:436} INFO - 24/06/27 08:07:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:16.662+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/21 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.21.31a2bcd2-4ed8-49c5-afed-a8dbab7ed8c2.tmp
[2024-06-27T08:07:16.720+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.21.31a2bcd2-4ed8-49c5-afed-a8dbab7ed8c2.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/21
[2024-06-27T08:07:16.721+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1719475636649,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:07:16.750+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.751+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.787+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.802+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.815+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.825+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.863+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:07:16.868+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:07:16.887+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:07:16.889+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:07:16.891+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:07:16.894+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:07:16.895+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:07:16.897+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:07:16.901+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:07:16.902+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:07:16.902+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:07:16.908+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:07:16.913+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-06-27T08:07:16.921+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:07:16.923+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2024-06-27T08:07:16.945+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 47 for partition store_source_data-0
[2024-06-27T08:07:16.949+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:07:17.451+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:07:17.452+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:07:17.453+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:07:17.454+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:07:17.455+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)
[2024-06-27T08:07:17.456+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 7180 bytes result sent to driver
[2024-06-27T08:07:17.457+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 538 ms on localhost (executor driver) (1/1)
[2024-06-27T08:07:17.463+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-06-27T08:07:17.465+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 0.570 s
24/06/27 08:07:17 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:07:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-06-27T08:07:17.466+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.596129 s
[2024-06-27T08:07:17.472+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:07:17.473+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 21
-------------------------------------------
[2024-06-27T08:07:17.530+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:07:17.530+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:07:17.544+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/21 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.21.a03a8f85-3c49-4550-976c-f08aeafd0d5b.tmp
[2024-06-27T08:07:17.587+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.21.a03a8f85-3c49-4550-976c-f08aeafd0d5b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/21
[2024-06-27T08:07:17.588+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:07:16.648Z",
  "batchId" : 21,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.0660980810234542,
  "durationMs" : {
    "addBatch" : 772,
    "commitOffsets" : 58,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 35,
    "triggerExecution" : 938,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.0660980810234542,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:07:18.784+0000] {docker.py:436} INFO - 24/06/27 08:07:18 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:07:27.588+0000] {docker.py:436} INFO - 24/06/27 08:07:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:37.590+0000] {docker.py:436} INFO - 24/06/27 08:07:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:47.593+0000] {docker.py:436} INFO - 24/06/27 08:07:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:57.600+0000] {docker.py:436} INFO - 24/06/27 08:07:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:07.610+0000] {docker.py:436} INFO - 24/06/27 08:08:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:17.618+0000] {docker.py:436} INFO - 24/06/27 08:08:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:27.626+0000] {docker.py:436} INFO - 24/06/27 08:08:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:37.635+0000] {docker.py:436} INFO - 24/06/27 08:08:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:47.639+0000] {docker.py:436} INFO - 24/06/27 08:08:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:57.645+0000] {docker.py:436} INFO - 24/06/27 08:08:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:58.587+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/22 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.22.9defbf9f-caf5-4f93-9f09-ae2aa93f4ae6.tmp
[2024-06-27T08:08:58.650+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.22.9defbf9f-caf5-4f93-9f09-ae2aa93f4ae6.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/22
[2024-06-27T08:08:58.651+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1719475738571,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:08:58.668+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.676+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.734+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.754+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.787+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.817+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.864+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:08:58.872+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:08:58 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:08:58 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:08:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:08:58.873+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:08:58.876+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:08:58.888+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:08:58.889+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:08:58.889+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 08:08:58 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:08:58.899+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:08:58.900+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-06-27T08:08:58.901+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:08:58.916+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2024-06-27T08:08:58.942+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 48 for partition store_source_data-0
[2024-06-27T08:08:58.943+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:08:59.438+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:08:59.438+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=49, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:08:59.439+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:08:59 INFO DataWritingSparkTask: Committed partition 0 (task 22, attempt 0, stage 22.0)
[2024-06-27T08:08:59.441+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 7180 bytes result sent to driver
[2024-06-27T08:08:59.445+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 554 ms on localhost (executor driver) (1/1)
[2024-06-27T08:08:59.446+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-06-27T08:08:59.447+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 0.571 s
[2024-06-27T08:08:59.450+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:08:59.464+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2024-06-27T08:08:59.473+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 22
-------------------------------------------
[2024-06-27T08:08:59.474+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.598262 s
24/06/27 08:08:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:08:59.533+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:08:59.533+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:08:59.561+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/22 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.22.dbae98ba-a630-4743-8d63-23ba5d821b50.tmp
[2024-06-27T08:08:59.723+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.22.dbae98ba-a630-4743-8d63-23ba5d821b50.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/22
[2024-06-27T08:08:59.724+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:08:58.570Z",
  "batchId" : 22,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8703220191470844,
  "durationMs" : {
    "addBatch" : 835,
    "commitOffsets" : 187,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 44,
    "triggerExecution" : 1149,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8703220191470844,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:09:02.586+0000] {docker.py:436} INFO - 24/06/27 08:09:02 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:09:09.724+0000] {docker.py:436} INFO - 24/06/27 08:09:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:19.732+0000] {docker.py:436} INFO - 24/06/27 08:09:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:29.733+0000] {docker.py:436} INFO - 24/06/27 08:09:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:39.739+0000] {docker.py:436} INFO - 24/06/27 08:09:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:49.740+0000] {docker.py:436} INFO - 24/06/27 08:09:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:59.740+0000] {docker.py:436} INFO - 24/06/27 08:09:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:09.747+0000] {docker.py:436} INFO - 24/06/27 08:10:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:19.749+0000] {docker.py:436} INFO - 24/06/27 08:10:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:29.758+0000] {docker.py:436} INFO - 24/06/27 08:10:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:39.763+0000] {docker.py:436} INFO - 24/06/27 08:10:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:49.767+0000] {docker.py:436} INFO - 24/06/27 08:10:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:58.377+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/23 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.23.f63ca84c-ba76-467a-8ca7-e7bf25a94121.tmp
[2024-06-27T08:10:58.478+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.23.f63ca84c-ba76-467a-8ca7-e7bf25a94121.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/23
24/06/27 08:10:58 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1719475858358,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:10:58.545+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.550+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.589+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.599+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.641+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.663+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.678+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:10:58.686+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:10:58.705+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:10:58 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:10:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:10:58.706+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Missing parents: List()
24/06/27 08:10:58 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:10:58.714+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:10:58.716+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:10:58.725+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:10:58.727+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:10:58.729+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:10:58.730+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2024-06-27T08:10:58.734+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:10:58.744+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2024-06-27T08:10:58.766+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 49 for partition store_source_data-0
[2024-06-27T08:10:58.781+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:10:59.320+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=50, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:10:59.321+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:10:59.322+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)
[2024-06-27T08:10:59.322+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 7223 bytes result sent to driver
[2024-06-27T08:10:59.322+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 581 ms on localhost (executor driver) (1/1)
[2024-06-27T08:10:59.323+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-06-27T08:10:59.323+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.610 s
[2024-06-27T08:10:59.324+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:10:59.327+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2024-06-27T08:10:59.328+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.634065 s
[2024-06-27T08:10:59.329+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:10:59.338+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 23
-------------------------------------------
[2024-06-27T08:10:59.370+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:10:59.371+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:10:59.391+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/23 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.23.c847af49-2932-46fa-80fe-bfdddb8a6d5b.tmp
[2024-06-27T08:10:59.456+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.23.c847af49-2932-46fa-80fe-bfdddb8a6d5b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/23
[2024-06-27T08:10:59.458+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:10:58.356Z",
  "batchId" : 23,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9099181073703367,
  "durationMs" : {
    "addBatch" : 805,
    "commitOffsets" : 84,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 91,
    "triggerExecution" : 1099,
    "walCommit" : 116
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9099181073703367,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:11:09.468+0000] {docker.py:436} INFO - 24/06/27 08:11:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:12.419+0000] {docker.py:436} INFO - 24/06/27 08:11:12 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:11:19.468+0000] {docker.py:436} INFO - 24/06/27 08:11:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:29.470+0000] {docker.py:436} INFO - 24/06/27 08:11:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:39.471+0000] {docker.py:436} INFO - 24/06/27 08:11:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:49.474+0000] {docker.py:436} INFO - 24/06/27 08:11:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:59.479+0000] {docker.py:436} INFO - 24/06/27 08:11:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:09.481+0000] {docker.py:436} INFO - 24/06/27 08:12:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:19.488+0000] {docker.py:436} INFO - 24/06/27 08:12:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:29.494+0000] {docker.py:436} INFO - 24/06/27 08:12:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:39.504+0000] {docker.py:436} INFO - 24/06/27 08:12:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:49.510+0000] {docker.py:436} INFO - 24/06/27 08:12:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:59.518+0000] {docker.py:436} INFO - 24/06/27 08:12:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:09.519+0000] {docker.py:436} INFO - 24/06/27 08:13:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:13.193+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/24 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.24.d7d7018f-ee18-49f8-b3ff-d185125087d8.tmp
[2024-06-27T08:13:13.318+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.24.d7d7018f-ee18-49f8-b3ff-d185125087d8.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/24
24/06/27 08:13:13 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1719475993148,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:13:13.396+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.427+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.443+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.506+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.507+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.548+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:13:13.549+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:13:13.550+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:13:13 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:13:13.550+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:13:13.551+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:13:13.551+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[74] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:13:13.551+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:13:13.552+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:13:13.569+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:13:13.569+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:13:13.570+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[74] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:13:13.570+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-06-27T08:13:13.570+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:13:13.571+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2024-06-27T08:13:13.588+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 50 for partition store_source_data-0
[2024-06-27T08:13:13.609+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:13:14.103+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:13:14.109+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:13:14.109+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:13:14.110+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DataWritingSparkTask: Committed partition 0 (task 24, attempt 0, stage 24.0)
[2024-06-27T08:13:14.110+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 7180 bytes result sent to driver
[2024-06-27T08:13:14.111+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 571 ms on localhost (executor driver) (1/1)
[2024-06-27T08:13:14.111+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-06-27T08:13:14.114+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
24/06/27 08:13:14 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-06-27T08:13:14.114+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.581712 s
[2024-06-27T08:13:14.115+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:13:14.115+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:13:14.116+0000] {docker.py:436} INFO - Batch: 24
[2024-06-27T08:13:14.116+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:13:14.143+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:13:14.143+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:13:14.157+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/24 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.24.fc62e63f-bc5a-4d38-ab23-1a0ce6ea067b.tmp
[2024-06-27T08:13:14.207+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.24.fc62e63f-bc5a-4d38-ab23-1a0ce6ea067b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/24
24/06/27 08:13:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:13:13.137Z",
  "batchId" : 24,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 0.9363295880149812,
  "durationMs" : {
    "addBatch" : 745,
    "commitOffsets" : 62,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 82,
    "triggerExecution" : 1068,
    "walCommit" : 166
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 0.9363295880149812,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:13:23.838+0000] {docker.py:436} INFO - 24/06/27 08:13:23 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:13:24.212+0000] {docker.py:436} INFO - 24/06/27 08:13:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:34.215+0000] {docker.py:436} INFO - 24/06/27 08:13:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:44.218+0000] {docker.py:436} INFO - 24/06/27 08:13:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:54.219+0000] {docker.py:436} INFO - 24/06/27 08:13:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:04.218+0000] {docker.py:436} INFO - 24/06/27 08:14:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:14.223+0000] {docker.py:436} INFO - 24/06/27 08:14:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:24.229+0000] {docker.py:436} INFO - 24/06/27 08:14:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:34.231+0000] {docker.py:436} INFO - 24/06/27 08:14:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:44.234+0000] {docker.py:436} INFO - 24/06/27 08:14:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:54.238+0000] {docker.py:436} INFO - 24/06/27 08:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:04.244+0000] {docker.py:436} INFO - 24/06/27 08:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:14.254+0000] {docker.py:436} INFO - 24/06/27 08:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:24.257+0000] {docker.py:436} INFO - 24/06/27 08:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:34.266+0000] {docker.py:436} INFO - 24/06/27 08:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:44.271+0000] {docker.py:436} INFO - 24/06/27 08:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:54.280+0000] {docker.py:436} INFO - 24/06/27 08:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:03.432+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/25 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.25.a842c0c0-773e-47be-8204-0727aa72ae78.tmp
[2024-06-27T08:16:03.498+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.25.a842c0c0-773e-47be-8204-0727aa72ae78.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/25
24/06/27 08:16:03 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1719476163412,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:16:03.517+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.530+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.553+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.575+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.621+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.638+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.685+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:16:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:16:03.712+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:16:03.713+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:16:03.713+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:16:03.714+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:16:03.714+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[77] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:16:03.729+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:16:03.733+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:16:03.733+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:16:03.749+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
24/06/27 08:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[77] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:16:03 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-06-27T08:16:03.773+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:16:03.783+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
[2024-06-27T08:16:03.807+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 51 for partition store_source_data-0
[2024-06-27T08:16:03.819+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:16:04.327+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:16:04.335+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:16:04.335+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:16:04.336+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 25.0)
[2024-06-27T08:16:04.336+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 7180 bytes result sent to driver
[2024-06-27T08:16:04.337+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 565 ms on localhost (executor driver) (1/1)
[2024-06-27T08:16:04.337+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
24/06/27 08:16:04 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 0.618 s
24/06/27 08:16:04 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-06-27T08:16:04.338+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.630444 s
[2024-06-27T08:16:04.338+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:16:04.339+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:16:04.339+0000] {docker.py:436} INFO - Batch: 25
-------------------------------------------
[2024-06-27T08:16:04.377+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:16:04.378+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:16:04.392+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/25 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.25.3d969b7d-1b20-47c1-8224-bf09c82156cd.tmp
[2024-06-27T08:16:04.554+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.25.3d969b7d-1b20-47c1-8224-bf09c82156cd.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/25
[2024-06-27T08:16:04.555+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:16:03.410Z",
  "batchId" : 25,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.8748906386701663,
  "durationMs" : {
    "addBatch" : 835,
    "commitOffsets" : 180,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 42,
    "triggerExecution" : 1143,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.8748906386701663,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:16:13.613+0000] {docker.py:436} INFO - 24/06/27 08:16:13 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:16:14.566+0000] {docker.py:436} INFO - 24/06/27 08:16:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:24.573+0000] {docker.py:436} INFO - 24/06/27 08:16:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:34.582+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:44.579+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:54.586+0000] {docker.py:436} INFO - 24/06/27 08:16:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:04.591+0000] {docker.py:436} INFO - 24/06/27 08:17:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:14.599+0000] {docker.py:436} INFO - 24/06/27 08:17:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:22.964+0000] {docker.py:436} INFO - 24/06/27 08:17:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/26 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.26.dbae7637-ba34-4c2e-b4ac-044e07a4b18e.tmp
[2024-06-27T08:17:23.016+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.26.dbae7637-ba34-4c2e-b4ac-044e07a4b18e.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/26
[2024-06-27T08:17:23.017+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1719476242943,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:17:23.039+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.048+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.049+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.078+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.119+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:17:23.120+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:17:23.124+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:17:23 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:17:23.127+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:17:23.130+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:17:23.132+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:17:23.149+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.154+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.159+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:17:23.159+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:17:23.160+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:17:23 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2024-06-27T08:17:23.164+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:17:23.178+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
[2024-06-27T08:17:23.228+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 52 for partition store_source_data-0
[2024-06-27T08:17:23.235+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:17:23.747+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:17:23.751+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=53, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:17:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:17:23 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 26.0)
[2024-06-27T08:17:23.752+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 7180 bytes result sent to driver
[2024-06-27T08:17:23.754+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 595 ms on localhost (executor driver) (1/1)
[2024-06-27T08:17:23.761+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-06-27T08:17:23.763+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 26
-------------------------------------------
[2024-06-27T08:17:23.764+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.612 s
24/06/27 08:17:23 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
24/06/27 08:17:23 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.637438 s
24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:17:23.807+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:17:23.808+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:17:23.834+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/26 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.26.ed35636c-3bae-4cf5-a6eb-e96287127e51.tmp
[2024-06-27T08:17:23.876+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.26.ed35636c-3bae-4cf5-a6eb-e96287127e51.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/26
[2024-06-27T08:17:23.878+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:17:22.941Z",
  "batchId" : 26,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.06951871657754,
  "durationMs" : {
    "addBatch" : 767,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 24,
    "triggerExecution" : 935,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.06951871657754,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:17:33.879+0000] {docker.py:436} INFO - 24/06/27 08:17:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:37.521+0000] {docker.py:436} INFO - 24/06/27 08:17:37 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:17:43.881+0000] {docker.py:436} INFO - 24/06/27 08:17:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:53.884+0000] {docker.py:436} INFO - 24/06/27 08:17:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:03.887+0000] {docker.py:436} INFO - 24/06/27 08:18:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:13.890+0000] {docker.py:436} INFO - 24/06/27 08:18:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:21.993+0000] {docker.py:436} INFO - 24/06/27 08:18:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/27 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.27.3ea27aa8-ff34-4f37-88f0-04f75f98da54.tmp
[2024-06-27T08:18:22.062+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.27.3ea27aa8-ff34-4f37-88f0-04f75f98da54.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/27
24/06/27 08:18:22 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1719476301980,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:18:22.079+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.086+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.087+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.103+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.109+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.118+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:18:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:18:22.118+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:18:22 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:18:22.120+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:18:22 INFO DAGScheduler: Missing parents: List()
24/06/27 08:18:22 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[83] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:18:22.122+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:18:22.127+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/27 08:18:22 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:18:22.130+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:18:22.131+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[83] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:18:22.131+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-06-27T08:18:22.133+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:18:22.134+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
[2024-06-27T08:18:22.146+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 53 for partition store_source_data-0
[2024-06-27T08:18:22.155+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:18:22.655+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:18:22.656+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=54, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:18:22.657+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:18:22 INFO DataWritingSparkTask: Committed partition 0 (task 27, attempt 0, stage 27.0)
[2024-06-27T08:18:22.658+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 7180 bytes result sent to driver
[2024-06-27T08:18:22.659+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 526 ms on localhost (executor driver) (1/1)
[2024-06-27T08:18:22.659+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-06-27T08:18:22.660+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s
[2024-06-27T08:18:22.661+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-06-27T08:18:22.661+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.542580 s
24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:18:22.662+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 27
-------------------------------------------
[2024-06-27T08:18:22.679+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:18:22.680+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:18:22.688+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/27 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.27.b7a20b90-09d9-4374-bf79-6ed2c6281471.tmp
[2024-06-27T08:18:22.722+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.27.b7a20b90-09d9-4374-bf79-6ed2c6281471.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/27
[2024-06-27T08:18:22.723+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:18:21.978Z",
  "batchId" : 27,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.3440860215053763,
  "durationMs" : {
    "addBatch" : 600,
    "commitOffsets" : 42,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 21,
    "triggerExecution" : 744,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.3440860215053763,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:18:30.156+0000] {docker.py:436} INFO - 24/06/27 08:18:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:18:32.732+0000] {docker.py:436} INFO - 24/06/27 08:18:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:42.738+0000] {docker.py:436} INFO - 24/06/27 08:18:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:52.741+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:02.747+0000] {docker.py:436} INFO - 24/06/27 08:19:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:12.756+0000] {docker.py:436} INFO - 24/06/27 08:19:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:22.755+0000] {docker.py:436} INFO - 24/06/27 08:19:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:32.762+0000] {docker.py:436} INFO - 24/06/27 08:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:42.772+0000] {docker.py:436} INFO - 24/06/27 08:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:52.777+0000] {docker.py:436} INFO - 24/06/27 08:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:57.992+0000] {docker.py:436} INFO - 24/06/27 08:19:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/28 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.28.f8c0377b-d676-455b-aedd-3d78c8c81d08.tmp
[2024-06-27T08:19:58.161+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.28.f8c0377b-d676-455b-aedd-3d78c8c81d08.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/28
24/06/27 08:19:58 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1719476397975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:19:58.188+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.209+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.235+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.247+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.281+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.303+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.348+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:19:58.349+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:19:58.359+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:19:58.359+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Final stage: ResultStage 28 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:19:58 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:19:58 INFO DAGScheduler: Missing parents: List()
24/06/27 08:19:58 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:19:58 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.361+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.368+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 08:19:58 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
24/06/27 08:19:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:19:58 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2024-06-27T08:19:58.385+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:19:58.389+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
[2024-06-27T08:19:58.464+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 54 for partition store_source_data-0
[2024-06-27T08:19:58.468+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:19:58.971+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:19:58.971+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=55, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:19:58.973+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:19:58.974+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DataWritingSparkTask: Committed partition 0 (task 28, attempt 0, stage 28.0)
[2024-06-27T08:19:58.975+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 7180 bytes result sent to driver
[2024-06-27T08:19:58.982+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 597 ms on localhost (executor driver) (1/1)
[2024-06-27T08:19:58.982+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-06-27T08:19:58.987+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: ResultStage 28 (start at NativeMethodAccessorImpl.java:0) finished in 0.635 s
[2024-06-27T08:19:58.994+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:19:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
24/06/27 08:19:58 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.645664 s
[2024-06-27T08:19:58.995+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:19:58.995+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 28
-------------------------------------------
[2024-06-27T08:19:59.040+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:19:59.043+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:19:59.149+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/28 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.28.77dd30e7-4807-4628-8c4a-a99a046aea14.tmp
[2024-06-27T08:19:59.297+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.28.77dd30e7-4807-4628-8c4a-a99a046aea14.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/28
[2024-06-27T08:19:59.299+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:19:57.974Z",
  "batchId" : 28,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7558578987150416,
  "durationMs" : {
    "addBatch" : 841,
    "commitOffsets" : 238,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 52,
    "triggerExecution" : 1323,
    "walCommit" : 182
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7558578987150416,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:20:04.908+0000] {docker.py:436} INFO - 24/06/27 08:20:04 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:20:09.309+0000] {docker.py:436} INFO - 24/06/27 08:20:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:19.315+0000] {docker.py:436} INFO - 24/06/27 08:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:29.323+0000] {docker.py:436} INFO - 24/06/27 08:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:39.332+0000] {docker.py:436} INFO - 24/06/27 08:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:49.346+0000] {docker.py:436} INFO - 24/06/27 08:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:59.347+0000] {docker.py:436} INFO - 24/06/27 08:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:09.353+0000] {docker.py:436} INFO - 24/06/27 08:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:19.356+0000] {docker.py:436} INFO - 24/06/27 08:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:29.358+0000] {docker.py:436} INFO - 24/06/27 08:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:39.365+0000] {docker.py:436} INFO - 24/06/27 08:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:49.368+0000] {docker.py:436} INFO - 24/06/27 08:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:59.370+0000] {docker.py:436} INFO - 24/06/27 08:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:09.377+0000] {docker.py:436} INFO - 24/06/27 08:22:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:19.387+0000] {docker.py:436} INFO - 24/06/27 08:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:29.387+0000] {docker.py:436} INFO - 24/06/27 08:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:39.391+0000] {docker.py:436} INFO - 24/06/27 08:22:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:49.398+0000] {docker.py:436} INFO - 24/06/27 08:22:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:59.401+0000] {docker.py:436} INFO - 24/06/27 08:22:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:09.411+0000] {docker.py:436} INFO - 24/06/27 08:23:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:19.420+0000] {docker.py:436} INFO - 24/06/27 08:23:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:29.423+0000] {docker.py:436} INFO - 24/06/27 08:23:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:39.432+0000] {docker.py:436} INFO - 24/06/27 08:23:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:49.442+0000] {docker.py:436} INFO - 24/06/27 08:23:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:53.068+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/29 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.29.33494022-0184-4364-bd2e-c00b3c41ccd7.tmp
[2024-06-27T08:23:53.315+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.29.33494022-0184-4364-bd2e-c00b3c41ccd7.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/29
[2024-06-27T08:23:53.316+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1719476632999,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:23:53.342+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.344+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.413+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.419+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.437+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.438+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.485+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:23:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:23:53.488+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:23:53 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:23:53 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:23:53 INFO DAGScheduler: Missing parents: List()
24/06/27 08:23:53 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[89] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:23:53 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.528+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.536+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:23:53.543+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:23:53.545+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[89] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:23:53.547+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2024-06-27T08:23:53.554+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:23:53.567+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
[2024-06-27T08:23:53.589+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 55 for partition store_source_data-0
[2024-06-27T08:23:53.599+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:23:54.113+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:23:54.119+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:23:54.121+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=56, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:23:54.122+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:23:54 INFO DataWritingSparkTask: Committed partition 0 (task 29, attempt 0, stage 29.0)
[2024-06-27T08:23:54.123+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 7180 bytes result sent to driver
[2024-06-27T08:23:54.128+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 571 ms on localhost (executor driver) (1/1)
[2024-06-27T08:23:54.128+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2024-06-27T08:23:54.129+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.639 s
[2024-06-27T08:23:54.130+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:23:54.131+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2024-06-27T08:23:54.132+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.647192 s
[2024-06-27T08:23:54.134+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:23:54.135+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.135+0000] {docker.py:436} INFO - Batch: 29
[2024-06-27T08:23:54.136+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.229+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:23:54.233+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:23:54.292+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/29 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.29.05188bd0-894a-45b1-aea3-4dc9ce309fe5.tmp
[2024-06-27T08:23:54.378+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.29.05188bd0-894a-45b1-aea3-4dc9ce309fe5.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/29
[2024-06-27T08:23:54.378+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:23:52.976Z",
  "batchId" : 29,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 58.8235294117647,
  "processedRowsPerSecond" : 0.7142857142857143,
  "durationMs" : {
    "addBatch" : 898,
    "commitOffsets" : 127,
    "getBatch" : 1,
    "latestOffset" : 23,
    "queryPlanning" : 34,
    "triggerExecution" : 1400,
    "walCommit" : 316
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 58.8235294117647,
    "processedRowsPerSecond" : 0.7142857142857143,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:23:58.083+0000] {docker.py:436} INFO - 24/06/27 08:23:58 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:24:04.385+0000] {docker.py:436} INFO - 24/06/27 08:24:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:14.390+0000] {docker.py:436} INFO - 24/06/27 08:24:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:24.401+0000] {docker.py:436} INFO - 24/06/27 08:24:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:34.402+0000] {docker.py:436} INFO - 24/06/27 08:24:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:44.412+0000] {docker.py:436} INFO - 24/06/27 08:24:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:54.414+0000] {docker.py:436} INFO - 24/06/27 08:24:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:04.427+0000] {docker.py:436} INFO - 24/06/27 08:25:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:14.436+0000] {docker.py:436} INFO - 24/06/27 08:25:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:24.445+0000] {docker.py:436} INFO - 24/06/27 08:25:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:34.455+0000] {docker.py:436} INFO - 24/06/27 08:25:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:44.463+0000] {docker.py:436} INFO - 24/06/27 08:25:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:54.464+0000] {docker.py:436} INFO - 24/06/27 08:25:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:04.475+0000] {docker.py:436} INFO - 24/06/27 08:26:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:14.478+0000] {docker.py:436} INFO - 24/06/27 08:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:24.479+0000] {docker.py:436} INFO - 24/06/27 08:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:34.488+0000] {docker.py:436} INFO - 24/06/27 08:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:44.494+0000] {docker.py:436} INFO - 24/06/27 08:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:54.497+0000] {docker.py:436} INFO - 24/06/27 08:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:04.496+0000] {docker.py:436} INFO - 24/06/27 08:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:14.502+0000] {docker.py:436} INFO - 24/06/27 08:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:24.512+0000] {docker.py:436} INFO - 24/06/27 08:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:34.521+0000] {docker.py:436} INFO - 24/06/27 08:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:44.527+0000] {docker.py:436} INFO - 24/06/27 08:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:54.533+0000] {docker.py:436} INFO - 24/06/27 08:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:04.538+0000] {docker.py:436} INFO - 24/06/27 08:28:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:14.539+0000] {docker.py:436} INFO - 24/06/27 08:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:24.545+0000] {docker.py:436} INFO - 24/06/27 08:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:34.556+0000] {docker.py:436} INFO - 24/06/27 08:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:44.558+0000] {docker.py:436} INFO - 24/06/27 08:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:54.559+0000] {docker.py:436} INFO - 24/06/27 08:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:04.561+0000] {docker.py:436} INFO - 24/06/27 08:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:14.566+0000] {docker.py:436} INFO - 24/06/27 08:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:24.568+0000] {docker.py:436} INFO - 24/06/27 08:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:34.569+0000] {docker.py:436} INFO - 24/06/27 08:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:36.211+0000] {docker.py:436} INFO - 24/06/27 08:29:36 INFO Metrics: Metrics scheduler closed
24/06/27 08:29:36 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/27 08:29:36 INFO Metrics: Metrics reporters closed
[2024-06-27T08:29:36.213+0000] {docker.py:436} INFO - 24/06/27 08:29:36 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-2 unregistered
[2024-06-27T08:29:44.573+0000] {docker.py:436} INFO - 24/06/27 08:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:54.574+0000] {docker.py:436} INFO - 24/06/27 08:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:04.576+0000] {docker.py:436} INFO - 24/06/27 08:30:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:14.585+0000] {docker.py:436} INFO - 24/06/27 08:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:23.721+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/30 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.30.45de456a-5901-4b1e-9fa8-47eb85bfb151.tmp
[2024-06-27T08:30:23.907+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.30.45de456a-5901-4b1e-9fa8-47eb85bfb151.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/30
[2024-06-27T08:30:23.908+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1719477023666,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:30:23.932+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.934+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.947+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.950+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.986+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.992+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.035+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:30:24.036+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:30:24.036+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:30:24.048+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:30:24.049+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:30:24.050+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:30:24.050+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:30:24.062+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:30:24.063+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:30:24.091+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 08:30:24 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
24/06/27 08:30:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:30:24.102+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2024-06-27T08:30:24.103+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:30:24.137+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
[2024-06-27T08:30:24.176+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:30:24.216+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:30:24.218+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:30:24.219+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka startTimeMs: 1719477024216
[2024-06-27T08:30:24.220+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:30:24.223+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 56 for partition store_source_data-0
[2024-06-27T08:30:24.257+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:30:24.301+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:30:24.818+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:30:24.820+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=57, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:30:24.827+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:30:24.828+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DataWritingSparkTask: Committed partition 0 (task 30, attempt 0, stage 30.0)
[2024-06-27T08:30:24.831+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 7180 bytes result sent to driver
[2024-06-27T08:30:24.842+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 741 ms on localhost (executor driver) (1/1)
[2024-06-27T08:30:24.845+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2024-06-27T08:30:24.858+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.819 s
[2024-06-27T08:30:24.859+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:30:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2024-06-27T08:30:24.861+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.828044 s
[2024-06-27T08:30:24.862+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 30
-------------------------------------------
[2024-06-27T08:30:24.863+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:30:25.018+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:30:25.026+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:30:25.156+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/30 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.30.bf909e3e-66e8-4267-9120-55a53b2f9f79.tmp
[2024-06-27T08:30:25.330+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.30.bf909e3e-66e8-4267-9120-55a53b2f9f79.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/30
[2024-06-27T08:30:25.339+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:30:23.655Z",
  "batchId" : 30,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.5970149253731343,
  "durationMs" : {
    "addBatch" : 1096,
    "commitOffsets" : 297,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 30,
    "triggerExecution" : 1675,
    "walCommit" : 241
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.5970149253731343,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:30:29.270+0000] {docker.py:436} INFO - 24/06/27 08:30:29 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:30:35.338+0000] {docker.py:436} INFO - 24/06/27 08:30:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:45.340+0000] {docker.py:436} INFO - 24/06/27 08:30:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:55.352+0000] {docker.py:436} INFO - 24/06/27 08:30:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:05.361+0000] {docker.py:436} INFO - 24/06/27 08:31:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:15.367+0000] {docker.py:436} INFO - 24/06/27 08:31:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:23.805+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/31 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.31.dc8e81c9-eeec-4ae9-9015-c28d34bcbbfd.tmp
[2024-06-27T08:31:23.912+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.31.dc8e81c9-eeec-4ae9-9015-c28d34bcbbfd.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/31
[2024-06-27T08:31:23.922+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1719477083791,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:31:23.934+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:23.935+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:23.972+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:23.982+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.047+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.060+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.213+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:31:24.214+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:31:24.222+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:31:24.227+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Final stage: ResultStage 31 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:31:24 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:31:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:31:24.227+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[95] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:31:24.254+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.312+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.312+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:31:24.313+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:31:24.316+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[95] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:31:24.323+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2024-06-27T08:31:24.331+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:31:24.339+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
[2024-06-27T08:31:24.430+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 57 for partition store_source_data-0
[2024-06-27T08:31:24.445+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:31:24.966+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:31:24.968+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=58, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:31:24.970+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:31:24.971+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DataWritingSparkTask: Committed partition 0 (task 31, attempt 0, stage 31.0)
[2024-06-27T08:31:24.978+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 7180 bytes result sent to driver
[2024-06-27T08:31:24.982+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 652 ms on localhost (executor driver) (1/1)
24/06/27 08:31:24 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2024-06-27T08:31:24.984+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: ResultStage 31 (start at NativeMethodAccessorImpl.java:0) finished in 0.750 s
[2024-06-27T08:31:24.986+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:31:24.988+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
[2024-06-27T08:31:24.989+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.774585 s
[2024-06-27T08:31:24.991+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:31:24.992+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:31:24.992+0000] {docker.py:436} INFO - Batch: 31
[2024-06-27T08:31:24.997+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:31:25.069+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:31:25.070+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:31:25.080+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/31 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.31.a28ebe82-a625-4254-8410-b3ed700ed407.tmp
[2024-06-27T08:31:25.157+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.31.a28ebe82-a625-4254-8410-b3ed700ed407.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/31
24/06/27 08:31:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:31:23.789Z",
  "batchId" : 31,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7347538574577517,
  "durationMs" : {
    "addBatch" : 1117,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 20,
    "triggerExecution" : 1361,
    "walCommit" : 131
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7347538574577517,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:31:26.222+0000] {docker.py:436} INFO - 24/06/27 08:31:26 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:31:35.153+0000] {docker.py:436} INFO - 24/06/27 08:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:45.164+0000] {docker.py:436} INFO - 24/06/27 08:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:55.167+0000] {docker.py:436} INFO - 24/06/27 08:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:05.169+0000] {docker.py:436} INFO - 24/06/27 08:32:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:15.177+0000] {docker.py:436} INFO - 24/06/27 08:32:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:25.185+0000] {docker.py:436} INFO - 24/06/27 08:32:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:35.193+0000] {docker.py:436} INFO - 24/06/27 08:32:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:45.202+0000] {docker.py:436} INFO - 24/06/27 08:32:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:55.210+0000] {docker.py:436} INFO - 24/06/27 08:32:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:05.217+0000] {docker.py:436} INFO - 24/06/27 08:33:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:15.220+0000] {docker.py:436} INFO - 24/06/27 08:33:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:25.225+0000] {docker.py:436} INFO - 24/06/27 08:33:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:35.229+0000] {docker.py:436} INFO - 24/06/27 08:33:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:45.234+0000] {docker.py:436} INFO - 24/06/27 08:33:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:55.237+0000] {docker.py:436} INFO - 24/06/27 08:33:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:05.243+0000] {docker.py:436} INFO - 24/06/27 08:34:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:15.252+0000] {docker.py:436} INFO - 24/06/27 08:34:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:25.255+0000] {docker.py:436} INFO - 24/06/27 08:34:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:35.265+0000] {docker.py:436} INFO - 24/06/27 08:34:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:45.267+0000] {docker.py:436} INFO - 24/06/27 08:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:55.274+0000] {docker.py:436} INFO - 24/06/27 08:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:05.278+0000] {docker.py:436} INFO - 24/06/27 08:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:15.288+0000] {docker.py:436} INFO - 24/06/27 08:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:25.289+0000] {docker.py:436} INFO - 24/06/27 08:35:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:35.295+0000] {docker.py:436} INFO - 24/06/27 08:35:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:45.297+0000] {docker.py:436} INFO - 24/06/27 08:35:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:55.302+0000] {docker.py:436} INFO - 24/06/27 08:35:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:05.304+0000] {docker.py:436} INFO - 24/06/27 08:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:15.305+0000] {docker.py:436} INFO - 24/06/27 08:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:25.306+0000] {docker.py:436} INFO - 24/06/27 08:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:35.311+0000] {docker.py:436} INFO - 24/06/27 08:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:36.218+0000] {docker.py:436} INFO - 24/06/27 08:36:36 INFO Metrics: Metrics scheduler closed
24/06/27 08:36:36 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:36:36.219+0000] {docker.py:436} INFO - 24/06/27 08:36:36 INFO Metrics: Metrics reporters closed
[2024-06-27T08:36:36.220+0000] {docker.py:436} INFO - 24/06/27 08:36:36 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-3 unregistered
[2024-06-27T08:36:45.312+0000] {docker.py:436} INFO - 24/06/27 08:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:50.174+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/32 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.32.85f70d5b-d9df-4d73-b23c-adf9122129fa.tmp
[2024-06-27T08:36:50.254+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.32.85f70d5b-d9df-4d73-b23c-adf9122129fa.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/32
[2024-06-27T08:36:50.255+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1719477410160,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:36:50.261+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.262+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.277+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.279+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.287+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.288+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.302+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:36:50.303+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:36:50.306+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:36:50 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:36:50 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:36:50 INFO DAGScheduler: Missing parents: List()
24/06/27 08:36:50 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:36:50.309+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.321+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.328+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:36:50.332+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:36:50.332+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:36:50.333+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2024-06-27T08:36:50.336+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:36:50.348+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
[2024-06-27T08:36:50.419+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:36:50.525+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:36:50.526+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:36:50.541+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO AppInfoParser: Kafka startTimeMs: 1719477410525
24/06/27 08:36:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Subscribed to partition(s): store_source_data-0
24/06/27 08:36:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 58 for partition store_source_data-0
[2024-06-27T08:36:50.569+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:36:50.589+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:36:51.103+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=59, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:36:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:36:51 INFO DataWritingSparkTask: Committed partition 0 (task 32, attempt 0, stage 32.0)
24/06/27 08:36:51 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 7180 bytes result sent to driver
[2024-06-27T08:36:51.113+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 771 ms on localhost (executor driver) (1/1)
24/06/27 08:36:51 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2024-06-27T08:36:51.114+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.799 s
[2024-06-27T08:36:51.114+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:36:51.115+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2024-06-27T08:36:51.115+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.806795 s
[2024-06-27T08:36:51.115+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:36:51.116+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:36:51.116+0000] {docker.py:436} INFO - Batch: 32
[2024-06-27T08:36:51.117+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:36:51.177+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:36:51.181+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:36:51.247+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/32 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.32.d0de2a62-2137-4b98-a48a-5719b70842a7.tmp
[2024-06-27T08:36:51.504+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.32.d0de2a62-2137-4b98-a48a-5719b70842a7.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/32
[2024-06-27T08:36:51.530+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:36:50.142Z",
  "batchId" : 32,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7342143906020557,
  "durationMs" : {
    "addBatch" : 913,
    "commitOffsets" : 322,
    "getBatch" : 0,
    "latestOffset" : 18,
    "queryPlanning" : 20,
    "triggerExecution" : 1362,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7342143906020557,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:37:00.390+0000] {docker.py:436} INFO - 24/06/27 08:37:00 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:37:01.510+0000] {docker.py:436} INFO - 24/06/27 08:37:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:11.515+0000] {docker.py:436} INFO - 24/06/27 08:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:21.521+0000] {docker.py:436} INFO - 24/06/27 08:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:31.524+0000] {docker.py:436} INFO - 24/06/27 08:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:41.531+0000] {docker.py:436} INFO - 24/06/27 08:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:51.532+0000] {docker.py:436} INFO - 24/06/27 08:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:01.535+0000] {docker.py:436} INFO - 24/06/27 08:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:11.545+0000] {docker.py:436} INFO - 24/06/27 08:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:21.556+0000] {docker.py:436} INFO - 24/06/27 08:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:31.564+0000] {docker.py:436} INFO - 24/06/27 08:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:41.570+0000] {docker.py:436} INFO - 24/06/27 08:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:51.581+0000] {docker.py:436} INFO - 24/06/27 08:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:01.583+0000] {docker.py:436} INFO - 24/06/27 08:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:11.589+0000] {docker.py:436} INFO - 24/06/27 08:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:21.603+0000] {docker.py:436} INFO - 24/06/27 08:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:31.613+0000] {docker.py:436} INFO - 24/06/27 08:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:41.615+0000] {docker.py:436} INFO - 24/06/27 08:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:51.619+0000] {docker.py:436} INFO - 24/06/27 08:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:55.685+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/33 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.33.d3a31202-e8bd-4743-a466-0babac5663c7.tmp
[2024-06-27T08:39:55.849+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.33.d3a31202-e8bd-4743-a466-0babac5663c7.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/33
24/06/27 08:39:55 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1719477595649,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:39:55.857+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.881+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.945+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.960+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.979+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:39:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:39:55 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:39:55 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:39:55 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:39:55 INFO DAGScheduler: Missing parents: List()
24/06/27 08:39:55 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:39:55 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
24/06/27 08:39:55 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/27 08:39:55 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:39:55.980+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:39:55.981+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:39:55 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2024-06-27T08:39:55.982+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:39:55.998+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
[2024-06-27T08:39:56.000+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 59 for partition store_source_data-0
[2024-06-27T08:39:56.012+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:39:56.519+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:39:56.520+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:39:56.523+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:39:56.524+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:39:56 INFO DataWritingSparkTask: Committed partition 0 (task 33, attempt 0, stage 33.0)
[2024-06-27T08:39:56.527+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 7223 bytes result sent to driver
24/06/27 08:39:56 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 545 ms on localhost (executor driver) (1/1)
24/06/27 08:39:56 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2024-06-27T08:39:56.529+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.556 s
[2024-06-27T08:39:56.530+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:39:56.530+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2024-06-27T08:39:56.531+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.558419 s
[2024-06-27T08:39:56.541+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:39:56.541+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:39:56.542+0000] {docker.py:436} INFO - Batch: 33
[2024-06-27T08:39:56.542+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:39:56.559+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:39:56.560+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:39:56.596+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/33 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.33.34b7fbf9-3f31-4beb-a5b0-79e63e521da4.tmp
[2024-06-27T08:39:56.719+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.33.34b7fbf9-3f31-4beb-a5b0-79e63e521da4.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/33
[2024-06-27T08:39:56.721+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:39:55.643Z",
  "batchId" : 33,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.929368029739777,
  "durationMs" : {
    "addBatch" : 654,
    "commitOffsets" : 153,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 52,
    "triggerExecution" : 1076,
    "walCommit" : 194
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.929368029739777,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:40:06.725+0000] {docker.py:436} INFO - 24/06/27 08:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:11.409+0000] {docker.py:436} INFO - 24/06/27 08:40:11 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:40:16.727+0000] {docker.py:436} INFO - 24/06/27 08:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:26.729+0000] {docker.py:436} INFO - 24/06/27 08:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:36.733+0000] {docker.py:436} INFO - 24/06/27 08:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:46.753+0000] {docker.py:436} INFO - 24/06/27 08:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:56.756+0000] {docker.py:436} INFO - 24/06/27 08:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:06.762+0000] {docker.py:436} INFO - 24/06/27 08:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:16.764+0000] {docker.py:436} INFO - 24/06/27 08:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:26.773+0000] {docker.py:436} INFO - 24/06/27 08:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:36.775+0000] {docker.py:436} INFO - 24/06/27 08:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:46.785+0000] {docker.py:436} INFO - 24/06/27 08:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:56.791+0000] {docker.py:436} INFO - 24/06/27 08:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:06.798+0000] {docker.py:436} INFO - 24/06/27 08:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:16.804+0000] {docker.py:436} INFO - 24/06/27 08:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:19.517+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/34 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.34.6231516d-2a98-401a-9b56-d02020a866bd.tmp
[2024-06-27T08:42:19.668+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.34.6231516d-2a98-401a-9b56-d02020a866bd.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/34
[2024-06-27T08:42:19.669+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1719477739492,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:19.688+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.696+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.727+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.754+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.796+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.817+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:42:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:19.825+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:42:19 INFO DAGScheduler: Final stage: ResultStage 34 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:42:19 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:42:19 INFO DAGScheduler: Missing parents: List()
24/06/27 08:42:19 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:19.834+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:42:19.858+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:42:19.859+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:42:19.859+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:19.876+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:42:19.877+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2024-06-27T08:42:19.878+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:42:19.878+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
[2024-06-27T08:42:19.927+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 60 for partition store_source_data-0
24/06/27 08:42:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:20.426+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:20.432+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=61, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:20 INFO DataWritingSparkTask: Committed partition 0 (task 34, attempt 0, stage 34.0)
24/06/27 08:42:20 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 7180 bytes result sent to driver
[2024-06-27T08:42:20.442+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 572 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:20.442+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: ResultStage 34 (start at NativeMethodAccessorImpl.java:0) finished in 0.623 s
[2024-06-27T08:42:20.443+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:20.443+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-06-27T08:42:20.444+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2024-06-27T08:42:20.444+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.625173 s
[2024-06-27T08:42:20.445+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:20.445+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 34
[2024-06-27T08:42:20.453+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:20.475+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:42:20.475+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:20.521+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/34 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.34.de60fe9c-2154-4884-adcd-88f996583827.tmp
[2024-06-27T08:42:20.697+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.34.de60fe9c-2154-4884-adcd-88f996583827.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/34
24/06/27 08:42:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:19.490Z",
  "batchId" : 34,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8417508417508418,
  "durationMs" : {
    "addBatch" : 759,
    "commitOffsets" : 203,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 1188,
    "walCommit" : 178
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8417508417508418,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:30.725+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/35 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.35.5b72a441-d421-4a74-8c42-48dde6be0755.tmp
[2024-06-27T08:42:30.862+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.35.5b72a441-d421-4a74-8c42-48dde6be0755.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/35
24/06/27 08:42:30 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1719477750690,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:30.865+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.885+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.929+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.943+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.991+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.025+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.160+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:31.166+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:31.168+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:42:31.168+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:42:31 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:31.169+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:31.173+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:31.182+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.235+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 08:42:31 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.236+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:42:31.249+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:31.264+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:42:31.279+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2024-06-27T08:42:31.291+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:42:31.329+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
[2024-06-27T08:42:31.428+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 61 for partition store_source_data-0
[2024-06-27T08:42:31.430+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:31.940+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:31.941+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=62, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:42:31.941+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:42:31.942+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DataWritingSparkTask: Committed partition 0 (task 35, attempt 0, stage 35.0)
[2024-06-27T08:42:31.964+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 7223 bytes result sent to driver
[2024-06-27T08:42:31.965+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 666 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:31.966+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-06-27T08:42:31.966+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.775 s
[2024-06-27T08:42:31.967+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:31.967+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2024-06-27T08:42:31.967+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.791115 s
[2024-06-27T08:42:31.968+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:31.968+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:31.978+0000] {docker.py:436} INFO - Batch: 35
[2024-06-27T08:42:31.978+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:42:32.010+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:42:32.012+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:32.053+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/35 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.35.28d876c2-2d97-4e36-a98b-467a1d484c37.tmp
[2024-06-27T08:42:32.181+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.35.28d876c2-2d97-4e36-a98b-467a1d484c37.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/35
24/06/27 08:42:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:30.675Z",
  "batchId" : 35,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.6648936170212766,
  "durationMs" : {
    "addBatch" : 1114,
    "commitOffsets" : 167,
    "getBatch" : 0,
    "latestOffset" : 15,
    "queryPlanning" : 52,
    "triggerExecution" : 1504,
    "walCommit" : 154
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.6648936170212766,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:42.186+0000] {docker.py:436} INFO - 24/06/27 08:42:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:49.149+0000] {docker.py:436} INFO - 24/06/27 08:42:49 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:42:52.190+0000] {docker.py:436} INFO - 24/06/27 08:42:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:02.200+0000] {docker.py:436} INFO - 24/06/27 08:43:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:12.205+0000] {docker.py:436} INFO - 24/06/27 08:43:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:22.212+0000] {docker.py:436} INFO - 24/06/27 08:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:32.225+0000] {docker.py:436} INFO - 24/06/27 08:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:42.227+0000] {docker.py:436} INFO - 24/06/27 08:43:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:52.236+0000] {docker.py:436} INFO - 24/06/27 08:43:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:02.244+0000] {docker.py:436} INFO - 24/06/27 08:44:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:12.252+0000] {docker.py:436} INFO - 24/06/27 08:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:22.252+0000] {docker.py:436} INFO - 24/06/27 08:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:32.252+0000] {docker.py:436} INFO - 24/06/27 08:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:42.254+0000] {docker.py:436} INFO - 24/06/27 08:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:52.260+0000] {docker.py:436} INFO - 24/06/27 08:44:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:02.262+0000] {docker.py:436} INFO - 24/06/27 08:45:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:12.267+0000] {docker.py:436} INFO - 24/06/27 08:45:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:22.275+0000] {docker.py:436} INFO - 24/06/27 08:45:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:32.277+0000] {docker.py:436} INFO - 24/06/27 08:45:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:42.280+0000] {docker.py:436} INFO - 24/06/27 08:45:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:48.187+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/36 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.36.76695008-a5cd-429a-8a5a-a0ac29d7518b.tmp
[2024-06-27T08:45:48.269+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.36.76695008-a5cd-429a-8a5a-a0ac29d7518b.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/36
24/06/27 08:45:48 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1719477948147,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:45:48.323+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.381+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.510+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.510+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.616+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.669+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.840+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:45:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:45:48.850+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:45:48 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:45:48 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:45:48 INFO DAGScheduler: Missing parents: List()
24/06/27 08:45:48 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[110] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:45:48.850+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:45:48.859+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:45:48.872+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:45:48.889+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:45:48.889+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[110] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:45:48.935+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2024-06-27T08:45:48.939+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:45:48.953+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
[2024-06-27T08:45:49.094+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 62 for partition store_source_data-0
[2024-06-27T08:45:49.110+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:45:49.605+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:45:49.606+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=63, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.606+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:45:49.614+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Committed partition 0 (task 36, attempt 0, stage 36.0)
[2024-06-27T08:45:49.619+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 7180 bytes result sent to driver
[2024-06-27T08:45:49.628+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 691 ms on localhost (executor driver) (1/1)
[2024-06-27T08:45:49.629+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2024-06-27T08:45:49.629+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.787 s
[2024-06-27T08:45:49.630+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:45:49.631+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2024-06-27T08:45:49.631+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.790723 s
[2024-06-27T08:45:49.632+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:45:49.632+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:45:49.637+0000] {docker.py:436} INFO - Batch: 36
[2024-06-27T08:45:49.639+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:45:49.723+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:45:49.724+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:45:49.848+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/36 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.36.e380a4d5-90e4-4f46-96f4-a08f7f289790.tmp
[2024-06-27T08:45:49.919+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.36.e380a4d5-90e4-4f46-96f4-a08f7f289790.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/36
[2024-06-27T08:45:49.929+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:45:48.146Z",
  "batchId" : 36,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.5640157924421884,
  "durationMs" : {
    "addBatch" : 1333,
    "commitOffsets" : 202,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 120,
    "triggerExecution" : 1773,
    "walCommit" : 116
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.5640157924421884,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:45:59.934+0000] {docker.py:436} INFO - 24/06/27 08:45:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:02.885+0000] {docker.py:436} INFO - 24/06/27 08:46:02 INFO BlockManagerInfo: Removed broadcast_36_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:46:09.935+0000] {docker.py:436} INFO - 24/06/27 08:46:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:19.940+0000] {docker.py:436} INFO - 24/06/27 08:46:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:29.950+0000] {docker.py:436} INFO - 24/06/27 08:46:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:39.952+0000] {docker.py:436} INFO - 24/06/27 08:46:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:49.955+0000] {docker.py:436} INFO - 24/06/27 08:46:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:59.966+0000] {docker.py:436} INFO - 24/06/27 08:46:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:09.975+0000] {docker.py:436} INFO - 24/06/27 08:47:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:19.978+0000] {docker.py:436} INFO - 24/06/27 08:47:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:29.978+0000] {docker.py:436} INFO - 24/06/27 08:47:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:39.977+0000] {docker.py:436} INFO - 24/06/27 08:47:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:49.984+0000] {docker.py:436} INFO - 24/06/27 08:47:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:59.987+0000] {docker.py:436} INFO - 24/06/27 08:47:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:09.997+0000] {docker.py:436} INFO - 24/06/27 08:48:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:20.005+0000] {docker.py:436} INFO - 24/06/27 08:48:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:30.010+0000] {docker.py:436} INFO - 24/06/27 08:48:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:40.017+0000] {docker.py:436} INFO - 24/06/27 08:48:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:50.026+0000] {docker.py:436} INFO - 24/06/27 08:48:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:00.026+0000] {docker.py:436} INFO - 24/06/27 08:49:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:10.027+0000] {docker.py:436} INFO - 24/06/27 08:49:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:20.027+0000] {docker.py:436} INFO - 24/06/27 08:49:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:30.031+0000] {docker.py:436} INFO - 24/06/27 08:49:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:40.036+0000] {docker.py:436} INFO - 24/06/27 08:49:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:50.038+0000] {docker.py:436} INFO - 24/06/27 08:49:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:00.040+0000] {docker.py:436} INFO - 24/06/27 08:50:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:10.041+0000] {docker.py:436} INFO - 24/06/27 08:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:20.043+0000] {docker.py:436} INFO - 24/06/27 08:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:30.049+0000] {docker.py:436} INFO - 24/06/27 08:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:40.050+0000] {docker.py:436} INFO - 24/06/27 08:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:50.062+0000] {docker.py:436} INFO - 24/06/27 08:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:00.071+0000] {docker.py:436} INFO - 24/06/27 08:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:08.372+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/37 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.37.c0ec7b46-a876-4927-b72b-58d29fc69a01.tmp
[2024-06-27T08:51:08.565+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/.37.c0ec7b46-a876-4927-b72b-58d29fc69a01.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/offsets/37
[2024-06-27T08:51:08.566+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1719478268292,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:51:08.592+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.608+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.644+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.659+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.772+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:51:08.774+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:51:08.776+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:51:08.777+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Final stage: ResultStage 37 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:51:08.777+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:51:08.778+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:51:08.778+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[113] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:51:08.793+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.807+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.822+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on localhost:41305 (size: 4.5 KiB, free: 434.4 MiB)
24/06/27 08:51:08 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
24/06/27 08:51:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[113] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:51:08 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
24/06/27 08:51:08 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11072 bytes)
[2024-06-27T08:51:08.822+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
[2024-06-27T08:51:08.871+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to offset 63 for partition store_source_data-0
[2024-06-27T08:51:08.884+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:51:09.404+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:51:09.429+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor-4, groupId=spark-kafka-source-b2b71e38-307e-4e1b-90cc-61cd1d53fe28--1123293477-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=64, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:51:09.430+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:51:09.431+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Committed partition 0 (task 37, attempt 0, stage 37.0)
[2024-06-27T08:51:09.433+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 7180 bytes result sent to driver
[2024-06-27T08:51:09.433+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 620 ms on localhost (executor driver) (1/1)
[2024-06-27T08:51:09.434+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2024-06-27T08:51:09.446+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: ResultStage 37 (start at NativeMethodAccessorImpl.java:0) finished in 0.655 s
[2024-06-27T08:51:09.446+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:51:09.447+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
[2024-06-27T08:51:09.448+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.671791 s
[2024-06-27T08:51:09.448+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:51:09.449+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:51:09.449+0000] {docker.py:436} INFO - Batch: 37
[2024-06-27T08:51:09.450+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:51:09.741+0000] {docker.py:436} INFO - +--------------------+
|               value|
+--------------------+
|{"purchases": [{"...|
+--------------------+
[2024-06-27T08:51:09.741+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:51:09.776+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/37 using temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.37.d2fdda4d-0f29-4393-9805-261e7b6e2d4e.tmp
[2024-06-27T08:51:09.983+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/.37.d2fdda4d-0f29-4393-9805-261e7b6e2d4e.tmp to file:/tmp/temporary-f48c0eb8-b178-4780-a1c5-bc2d5468006a/commits/37
[2024-06-27T08:51:09.986+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ac8a30bd-12a6-4ea1-80f8-19bc81412578",
  "runId" : "e01ff820-d4de-4106-96c3-8bb34506fc33",
  "name" : null,
  "timestamp" : "2024-06-27T08:51:08.288Z",
  "batchId" : 37,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 55.55555555555556,
  "processedRowsPerSecond" : 0.5903187721369539,
  "durationMs" : {
    "addBatch" : 1157,
    "commitOffsets" : 233,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 15,
    "triggerExecution" : 1694,
    "walCommit" : 279
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 55.55555555555556,
    "processedRowsPerSecond" : 0.5903187721369539,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@56a7d494",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:51:19.989+0000] {docker.py:436} INFO - 24/06/27 08:51:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:20.381+0000] {docker.py:436} INFO - 24/06/27 08:51:20 INFO BlockManagerInfo: Removed broadcast_37_piece0 on localhost:41305 in memory (size: 4.5 KiB, free: 434.4 MiB)
[2024-06-27T08:51:29.998+0000] {docker.py:436} INFO - 24/06/27 08:51:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:40.009+0000] {docker.py:436} INFO - 24/06/27 08:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:50.019+0000] {docker.py:436} INFO - 24/06/27 08:51:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:00.019+0000] {docker.py:436} INFO - 24/06/27 08:52:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:10.018+0000] {docker.py:436} INFO - 24/06/27 08:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:20.025+0000] {docker.py:436} INFO - 24/06/27 08:52:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:30.032+0000] {docker.py:436} INFO - 24/06/27 08:52:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:40.037+0000] {docker.py:436} INFO - 24/06/27 08:52:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:50.043+0000] {docker.py:436} INFO - 24/06/27 08:52:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:00.053+0000] {docker.py:436} INFO - 24/06/27 08:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:10.061+0000] {docker.py:436} INFO - 24/06/27 08:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:20.070+0000] {docker.py:436} INFO - 24/06/27 08:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:30.069+0000] {docker.py:436} INFO - 24/06/27 08:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:36.473+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:36.603+0000] {process_utils.py:132} INFO - Sending 15 to group 370. PIDs of all processes in the group: [370]
[2024-06-27T08:53:36.651+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 370
[2024-06-27T08:53:36.661+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-06-27T08:53:36.663+0000] {docker.py:528} INFO - Stopping docker container
[2024-06-27T08:53:36.702+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T08:53:36.703+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 265, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.45/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 371, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 398, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 439, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 456, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 271, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 267, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.45/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/airflowtmpx8dlbl2f")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c01f70>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c01f70>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 438, in _run_image_with_mounts
    result = self.cli.wait(self.container["Id"])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1346, in wait
    res = self._post(url, timeout=timeout, params=params)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c01f70>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c76360>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c76360>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 380, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 462, in _run_image_with_mounts
    self.cli.remove_container(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1033, in remove_container
    res = self._delete(
          ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 244, in _delete
    return self.delete(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 671, in delete
    return self.request("DELETE", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c76360>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c76840>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c76840>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 509, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 368, in _run_image
    with TemporaryDirectory(prefix="airflowtmp", dir=self.host_tmp_dir) as host_tmp_dir_generated:
  File "/usr/local/lib/python3.12/tempfile.py", line 946, in __exit__
    self.cleanup()
  File "/usr/local/lib/python3.12/tempfile.py", line 950, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/usr/local/lib/python3.12/tempfile.py", line 930, in _rmtree
    _shutil.rmtree(name, onexc=onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 796, in rmtree
    onexc(os.rmdir, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 794, in rmtree
    os.rmdir(path, dir_fd=dir_fd)
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2612, in signal_handler
    self.task.on_kill()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 532, in on_kill
    self.cli.stop(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1210, in stop
    res = self._post(url, params=params, timeout=conn_timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c76840>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:37.171+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=Stage_1, task_id=run_bronze_job, run_id=manual__2024-06-27T07:13:01.822167+00:00, execution_date=20240627T071301, start_date=20240627T071308, end_date=20240627T085337
[2024-06-27T08:53:37.342+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 131 for task run_bronze_job (HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/438e8dced755f844bc78ca9c818643d4ea22e1dbae7f817a7b34c18d8dc1c7d5/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c76840>: Failed to establish a new connection: [Errno 111] Connection refused')); 370)
[2024-06-27T08:53:37.403+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=370, status='terminated', exitcode=1, started='07:13:07') (370) terminated with exit code 1
[2024-06-27T08:53:37.410+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 143
[2024-06-27T08:53:37.515+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T08:53:40.132+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:40.134+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 143
