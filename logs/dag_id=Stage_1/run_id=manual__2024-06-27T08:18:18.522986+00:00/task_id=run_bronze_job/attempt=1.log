[2024-06-27T08:18:25.259+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T08:18:25.298+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T08:18:18.522986+00:00 [queued]>
[2024-06-27T08:18:25.310+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T08:18:18.522986+00:00 [queued]>
[2024-06-27T08:18:25.311+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-27T08:18:25.327+0000] {taskinstance.py:2330} INFO - Executing <Task(DockerOperator): run_bronze_job> on 2024-06-27 08:18:18.522986+00:00
[2024-06-27T08:18:25.334+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=1316) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-27T08:18:25.335+0000] {standard_task_runner.py:63} INFO - Started process 1317 to run task
[2024-06-27T08:18:25.334+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Stage_1', 'run_bronze_job', 'manual__2024-06-27T08:18:18.522986+00:00', '--job-id', '266', '--raw', '--subdir', 'DAGS_FOLDER/kafka_dag.py', '--cfg-path', '/tmp/tmpu0l4pjpf']
[2024-06-27T08:18:25.337+0000] {standard_task_runner.py:91} INFO - Job 266: Subtask run_bronze_job
[2024-06-27T08:18:25.351+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-06-27T08:18:25.390+0000] {task_command.py:426} INFO - Running <TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T08:18:18.522986+00:00 [running]> on host 08bfa8b73cac
[2024-06-27T08:18:25.495+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Stage_1' AIRFLOW_CTX_TASK_ID='run_bronze_job' AIRFLOW_CTX_EXECUTION_DATE='2024-06-27T08:18:18.522986+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-27T08:18:18.522986+00:00'
[2024-06-27T08:18:25.496+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T08:18:25.529+0000] {docker.py:366} INFO - Starting docker container from image bitnami/spark:latest
[2024-06-27T08:18:25.532+0000] {docker.py:374} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-06-27T08:18:26.167+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:18:26.16 [0m[38;5;2mINFO [0m ==>
[2024-06-27T08:18:26.173+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:18:26.16 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-27T08:18:26.176+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:18:26.17 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-27T08:18:26.177+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:18:26.17 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-27T08:18:26.181+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:18:26.18 [0m[38;5;2mINFO [0m ==> Upgrade to Tanzu Application Catalog for production environments to access custom-configured and pre-packaged software components. Gain enhanced features, including Software Bill of Materials (SBOM), CVE scan result reports, and VEX documents. To learn more, visit [1mhttps://bitnami.com/enterprise[0m
[2024-06-27T08:18:26.184+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m08:18:26.18 [0m[38;5;2mINFO [0m ==>
[2024-06-27T08:18:26.195+0000] {docker.py:436} INFO - 
[2024-06-27T08:18:29.005+0000] {docker.py:436} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-27T08:18:29.091+0000] {docker.py:436} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-27T08:18:29.099+0000] {docker.py:436} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-06-27T08:18:29.100+0000] {docker.py:436} INFO - org.postgresql#postgresql added as a dependency
[2024-06-27T08:18:29.101+0000] {docker.py:436} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-dd4a4e94-92ec-4b1a-a3d0-584865db4c3e;1.0
[2024-06-27T08:18:29.102+0000] {docker.py:436} INFO - confs: [default]
[2024-06-27T08:18:31.136+0000] {docker.py:436} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T08:18:31.593+0000] {docker.py:436} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T08:18:31.748+0000] {docker.py:436} INFO - found org.apache.kafka#kafka-clients;2.8.1 in central
[2024-06-27T08:18:31.910+0000] {docker.py:436} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-27T08:18:32.086+0000] {docker.py:436} INFO - found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2024-06-27T08:18:32.313+0000] {docker.py:436} INFO - found org.slf4j#slf4j-api;1.7.32 in central
[2024-06-27T08:18:32.608+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2024-06-27T08:18:32.729+0000] {docker.py:436} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2024-06-27T08:18:32.876+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2024-06-27T08:18:33.166+0000] {docker.py:436} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-27T08:18:33.211+0000] {docker.py:436} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-27T08:18:33.669+0000] {docker.py:436} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-27T08:18:35.539+0000] {docker.py:436} INFO - found org.postgresql#postgresql;42.2.2 in central
[2024-06-27T08:18:35.561+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T08:18:35.623+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (72ms)
[2024-06-27T08:18:35.642+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar ...
[2024-06-27T08:18:35.733+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.2.2!postgresql.jar(bundle) (107ms)
[2024-06-27T08:18:35.745+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T08:18:35.779+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (45ms)
[2024-06-27T08:18:35.815+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...
[2024-06-27T08:18:36.521+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (740ms)
[2024-06-27T08:18:36.537+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-27T08:18:36.559+0000] {docker.py:436} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (35ms)
[2024-06-27T08:18:36.570+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-27T08:18:36.598+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (39ms)
[2024-06-27T08:18:36.611+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[2024-06-27T08:18:36.624+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (25ms)
[2024-06-27T08:18:36.634+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...
[2024-06-27T08:18:40.995+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (4369ms)
[2024-06-27T08:18:41.007+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-27T08:18:41.151+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (156ms)
[2024-06-27T08:18:41.177+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...
[2024-06-27T08:18:41.413+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (254ms)
[2024-06-27T08:18:41.423+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...
[2024-06-27T08:18:41.439+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (24ms)
[2024-06-27T08:18:41.449+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...
[2024-06-27T08:18:44.281+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (2841ms)
[2024-06-27T08:18:44.297+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-27T08:18:44.315+0000] {docker.py:436} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (32ms)
[2024-06-27T08:18:44.316+0000] {docker.py:436} INFO - :: resolution report :: resolve 6449ms :: artifacts dl 8766ms
	:: modules in use:
[2024-06-27T08:18:44.317+0000] {docker.py:436} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.2.2 from central in [default]
	org.slf4j#slf4j-api;1.7.32 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-27T08:18:44.328+0000] {docker.py:436} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-dd4a4e94-92ec-4b1a-a3d0-584865db4c3e
	confs: [default]
[2024-06-27T08:18:44.448+0000] {docker.py:436} INFO - 13 artifacts copied, 0 already retrieved (57403kB/121ms)
[2024-06-27T08:18:44.859+0000] {docker.py:436} INFO - 24/06/27 08:18:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-27T08:18:46.478+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SparkContext: Running Spark version 3.5.1
[2024-06-27T08:18:46.479+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SparkContext: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T08:18:46.481+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SparkContext: Java version 17.0.11
[2024-06-27T08:18:46.533+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO ResourceUtils: ==============================================================
[2024-06-27T08:18:46.533+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-27T08:18:46.534+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO ResourceUtils: ==============================================================
[2024-06-27T08:18:46.535+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-27T08:18:46.574+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-27T08:18:46.590+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO ResourceProfile: Limiting resource is cpu
[2024-06-27T08:18:46.592+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-27T08:18:46.667+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SecurityManager: Changing view acls to: spark
[2024-06-27T08:18:46.668+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SecurityManager: Changing modify acls to: spark
[2024-06-27T08:18:46.669+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SecurityManager: Changing view acls groups to:
[2024-06-27T08:18:46.670+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SecurityManager: Changing modify acls groups to:
[2024-06-27T08:18:46.672+0000] {docker.py:436} INFO - 24/06/27 08:18:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-27T08:18:47.036+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO Utils: Successfully started service 'sparkDriver' on port 33463.
[2024-06-27T08:18:47.073+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkEnv: Registering MapOutputTracker
[2024-06-27T08:18:47.119+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-27T08:18:47.152+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-27T08:18:47.153+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-27T08:18:47.159+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-27T08:18:47.191+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cbe5f995-8e52-4657-af33-d449f988bdbc
[2024-06-27T08:18:47.213+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-27T08:18:47.241+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-27T08:18:47.545+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-27T08:18:47.690+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-27T08:18:47.804+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://localhost:33463/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:47.824+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at spark://localhost:33463/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476326466
[2024-06-27T08:18:47.828+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://localhost:33463/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:47.829+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://localhost:33463/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476326466
[2024-06-27T08:18:47.830+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:33463/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476326466
[2024-06-27T08:18:47.832+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:33463/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476326466
[2024-06-27T08:18:47.832+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://localhost:33463/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476326466
24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://localhost:33463/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:47.833+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:33463/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476326466
[2024-06-27T08:18:47.834+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://localhost:33463/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476326466
[2024-06-27T08:18:47.835+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://localhost:33463/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476326466
24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://localhost:33463/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:47.835+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:33463/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476326466
[2024-06-27T08:18:47.842+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:47.844+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:18:47.859+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476326466
[2024-06-27T08:18:47.860+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T08:18:47.868+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:18:47.880+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476326466
24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T08:18:47.899+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476326466
24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T08:18:47.923+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476326466
24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T08:18:47.942+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476326466
[2024-06-27T08:18:47.943+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T08:18:47.966+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:47.967+0000] {docker.py:436} INFO - 24/06/27 08:18:47 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T08:18:48.022+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.023+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T08:18:48.033+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476326466
[2024-06-27T08:18:48.034+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T08:18:48.051+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476326466
[2024-06-27T08:18:48.052+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T08:18:48.065+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476326466
24/06/27 08:18:48 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T08:18:48.093+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476326466
[2024-06-27T08:18:48.094+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T08:18:48.200+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Starting executor ID driver on host localhost
[2024-06-27T08:18:48.201+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T08:18:48.202+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Java version 17.0.11
[2024-06-27T08:18:48.214+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-27T08:18:48.215+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1ed70cc7 for default.
[2024-06-27T08:18:48.232+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476326466
[2024-06-27T08:18:48.258+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T08:18:48.263+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:48.288+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T08:18:48.295+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476326466
[2024-06-27T08:18:48.298+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T08:18:48.303+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:48.336+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T08:18:48.341+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476326466
[2024-06-27T08:18:48.342+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T08:18:48.346+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.346+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:18:48.350+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476326466
[2024-06-27T08:18:48.352+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T08:18:48.376+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.381+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:18:48.387+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.388+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T08:18:48.395+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.396+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T08:18:48.401+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476326466
[2024-06-27T08:18:48.401+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T08:18:48.406+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.406+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T08:18:48.411+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476326466
[2024-06-27T08:18:48.420+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T08:18:48.429+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:48.481+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:33463 after 40 ms (0 ms spent in bootstraps)
[2024-06-27T08:18:48.493+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp6707319633902431599.tmp
[2024-06-27T08:18:48.712+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp6707319633902431599.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T08:18:48.719+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader default
[2024-06-27T08:18:48.720+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.721+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp9880743372667833763.tmp
[2024-06-27T08:18:48.722+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp9880743372667833763.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:18:48.727+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T08:18:48.727+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.728+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp5322412040857020845.tmp
[2024-06-27T08:18:48.731+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp5322412040857020845.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T08:18:48.735+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-27T08:18:48.736+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719476326466
[2024-06-27T08:18:48.737+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp17159806272914015426.tmp
[2024-06-27T08:18:48.739+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp17159806272914015426.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T08:18:48.743+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/commons-logging_commons-logging-1.1.3.jar to class loader default
[2024-06-27T08:18:48.744+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719476326466
[2024-06-27T08:18:48.745+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp5161080781669437934.tmp
[2024-06-27T08:18:48.764+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp5161080781669437934.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T08:18:48.773+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.kafka_kafka-clients-2.8.1.jar to class loader default
[2024-06-27T08:18:48.773+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719476326466
[2024-06-27T08:18:48.774+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp16735491416381129761.tmp
[2024-06-27T08:18:48.776+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp16735491416381129761.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T08:18:48.780+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2024-06-27T08:18:48.781+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719476326466
[2024-06-27T08:18:48.781+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp6671838461075685823.tmp
[2024-06-27T08:18:48.793+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp6671838461075685823.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T08:18:48.800+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader default
[2024-06-27T08:18:48.801+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Executor: Fetching spark://localhost:33463/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719476326466
[2024-06-27T08:18:48.802+0000] {docker.py:436} INFO - 24/06/27 08:18:48 INFO Utils: Fetching spark://localhost:33463/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp4118134180793878925.tmp
[2024-06-27T08:18:49.077+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp4118134180793878925.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T08:18:49.086+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader default
[2024-06-27T08:18:49.087+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Fetching spark://localhost:33463/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719476326466
[2024-06-27T08:18:49.088+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: Fetching spark://localhost:33463/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp5188383655866520988.tmp
[2024-06-27T08:18:49.089+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp5188383655866520988.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T08:18:49.093+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.slf4j_slf4j-api-1.7.32.jar to class loader default
[2024-06-27T08:18:49.094+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Fetching spark://localhost:33463/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719476326466
[2024-06-27T08:18:49.094+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: Fetching spark://localhost:33463/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp6780778431334640480.tmp
[2024-06-27T08:18:49.096+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp6780778431334640480.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T08:18:49.099+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.spark-project.spark_unused-1.0.0.jar to class loader default
[2024-06-27T08:18:49.100+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Fetching spark://localhost:33463/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719476326466
[2024-06-27T08:18:49.100+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: Fetching spark://localhost:33463/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp7245812325184920034.tmp
[2024-06-27T08:18:49.104+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp7245812325184920034.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T08:18:49.108+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T08:18:49.109+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Fetching spark://localhost:33463/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719476326466
24/06/27 08:18:49 INFO Utils: Fetching spark://localhost:33463/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp4513358587434884174.tmp
[2024-06-27T08:18:49.111+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp4513358587434884174.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T08:18:49.114+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-06-27T08:18:49.115+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Fetching spark://localhost:33463/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719476326466
[2024-06-27T08:18:49.117+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: Fetching spark://localhost:33463/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp9955378988026621784.tmp
[2024-06-27T08:18:49.121+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/fetchFileTemp9955378988026621784.tmp has been previously copied to /tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T08:18:49.125+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Executor: Adding file:/tmp/spark-bc3f835e-37bc-4199-a62c-ad58f0cb184d/userFiles-4ec1593a-d347-4c00-ae90-c5fb71a77507/org.postgresql_postgresql-42.2.2.jar to class loader default
[2024-06-27T08:18:49.138+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40553.
[2024-06-27T08:18:49.139+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO NettyBlockTransferService: Server created on localhost:40553
[2024-06-27T08:18:49.142+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-27T08:18:49.149+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 40553, None)
[2024-06-27T08:18:49.154+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40553 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 40553, None)
[2024-06-27T08:18:49.158+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 40553, None)
[2024-06-27T08:18:49.160+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 40553, None)
[2024-06-27T08:18:49.643+0000] {docker.py:436} INFO - 2024-06-27 08:18:49,642:create_spark_session:INFO:Spark session created successfully
[2024-06-27T08:18:49.652+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-27T08:18:49.656+0000] {docker.py:436} INFO - 24/06/27 08:18:49 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-27T08:18:51.693+0000] {docker.py:436} INFO - 2024-06-27 08:18:51,693:create_initial_dataframe:INFO:Initial dataframe created successfully:
[2024-06-27T08:18:52.142+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-27T08:18:52.174+0000] {docker.py:436} INFO - 24/06/27 08:18:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-27T08:18:52.196+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f resolved to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f.
[2024-06-27T08:18:52.197+0000] {docker.py:436} INFO - 24/06/27 08:18:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-27T08:18:52.289+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/metadata using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/.metadata.9c341cbc-1b63-4499-a3a2-92a8451144cb.tmp
[2024-06-27T08:18:52.404+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/.metadata.9c341cbc-1b63-4499-a3a2-92a8451144cb.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/metadata
[2024-06-27T08:18:52.444+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO MicroBatchExecution: Starting [id = 02b4530d-3569-4ad8-9560-4576be621abb, runId = dd284288-5f65-4233-8fcd-e0f050301bf6]. Use file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f to store the query checkpoint.
[2024-06-27T08:18:52.465+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4fa40ee9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@737b8636]
[2024-06-27T08:18:52.527+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T08:18:52.530+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T08:18:52.531+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-27T08:18:52.537+0000] {docker.py:436} INFO - 24/06/27 08:18:52 INFO MicroBatchExecution: Stream started from {}
[2024-06-27T08:18:53.043+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-27T08:18:53.148+0000] {docker.py:436} INFO - 24/06/27 08:18:53 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/06/27 08:18:53 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/06/27 08:18:53 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/06/27 08:18:53 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/06/27 08:18:53 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
[2024-06-27T08:18:53.153+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO AppInfoParser: Kafka version: 2.8.1
24/06/27 08:18:53 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 08:18:53 INFO AppInfoParser: Kafka startTimeMs: 1719476333147
[2024-06-27T08:18:53.769+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/sources/0/0 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/sources/0/.0.845c9682-e16d-4e0c-a2de-0c7c702d64aa.tmp
[2024-06-27T08:18:53.806+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/sources/0/.0.845c9682-e16d-4e0c-a2de-0c7c702d64aa.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/sources/0/0
[2024-06-27T08:18:53.808+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO KafkaMicroBatchStream: Initial offsets: {"store_source_data":{"0":8}}
[2024-06-27T08:18:53.843+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/0 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.0.26aa9d9d-d10d-4414-a650-dc2e3c018f21.tmp
[2024-06-27T08:18:53.933+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.0.26aa9d9d-d10d-4414-a650-dc2e3c018f21.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/0
[2024-06-27T08:18:53.934+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719476333828,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:18:54.486+0000] {docker.py:436} INFO - 24/06/27 08:18:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:54.616+0000] {docker.py:436} INFO - 24/06/27 08:18:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:54.755+0000] {docker.py:436} INFO - 24/06/27 08:18:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:54.760+0000] {docker.py:436} INFO - 24/06/27 08:18:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:54.858+0000] {docker.py:436} INFO - 24/06/27 08:18:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:54.860+0000] {docker.py:436} INFO - 24/06/27 08:18:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:55.308+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO CodeGenerator: Code generated in 255.714089 ms
[2024-06-27T08:18:55.418+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:18:55.440+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:18:55.456+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:18:55.457+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:18:55 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:18:55.459+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:18:55.469+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:18:55.701+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:18:55.731+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:18:55.735+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:18:55.741+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:18:55.762+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:18:55.763+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-27T08:18:55.852+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:18:55.877+0000] {docker.py:436} INFO - 24/06/27 08:18:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-06-27T08:18:56.120+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO CodeGenerator: Code generated in 73.07302 ms
[2024-06-27T08:18:56.169+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO CodeGenerator: Code generated in 45.974787 ms
[2024-06-27T08:18:56.285+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO CodeGenerator: Code generated in 27.051068 ms
[2024-06-27T08:18:56.332+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO CodeGenerator: Code generated in 30.679975 ms
[2024-06-27T08:18:56.355+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:18:56.432+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:18:56.433+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:18:56.434+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO AppInfoParser: Kafka startTimeMs: 1719476336431
[2024-06-27T08:18:56.440+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:18:56.456+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 8 for partition store_source_data-0
[2024-06-27T08:18:56.470+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:18:56.552+0000] {docker.py:436} INFO - 24/06/27 08:18:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:18:57.054+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:18:57.054+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:18:57.055+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=54, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:18:57.324+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:18:57.326+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2024-06-27T08:18:57.353+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 6579 bytes result sent to driver
[2024-06-27T08:18:57.366+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1550 ms on localhost (executor driver) (1/1)
[2024-06-27T08:18:57.369+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-27T08:18:57.377+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.890 s
[2024-06-27T08:18:57.381+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:18:57.384+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-06-27T08:18:57.387+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 1.955930 s
[2024-06-27T08:18:57.389+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:18:57.396+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 0
-------------------------------------------
[2024-06-27T08:18:57.538+0000] {docker.py:436} INFO - 24/06/27 08:18:57 INFO CodeGenerator: Code generated in 15.899907 ms
[2024-06-27T08:18:58.996+0000] {docker.py:436} INFO - 24/06/27 08:18:58 INFO CodeGenerator: Code generated in 20.740313 ms
[2024-06-27T08:18:59.013+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
only showing top 20 rows
[2024-06-27T08:18:59.014+0000] {docker.py:436} INFO - 24/06/27 08:18:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:18:59.030+0000] {docker.py:436} INFO - 24/06/27 08:18:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/0 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.0.7d4d267d-8fff-4704-9ad0-4f53171f0c0c.tmp
[2024-06-27T08:18:59.074+0000] {docker.py:436} INFO - 24/06/27 08:18:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.0.7d4d267d-8fff-4704-9ad0-4f53171f0c0c.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/0
[2024-06-27T08:18:59.141+0000] {docker.py:436} INFO - 24/06/27 08:18:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:18:52.513Z",
  "batchId" : 0,
  "numInputRows" : 46,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 7.01219512195122,
  "durationMs" : {
    "addBatch" : 4341,
    "commitOffsets" : 55,
    "getBatch" : 29,
    "latestOffset" : 1285,
    "queryPlanning" : 698,
    "triggerExecution" : 6558,
    "walCommit" : 104
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : null,
    "endOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "numInputRows" : 46,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 7.01219512195122,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 46
  }
}
[2024-06-27T08:19:02.093+0000] {docker.py:436} INFO - 24/06/27 08:19:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:19:09.108+0000] {docker.py:436} INFO - 24/06/27 08:19:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:19.113+0000] {docker.py:436} INFO - 24/06/27 08:19:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:29.117+0000] {docker.py:436} INFO - 24/06/27 08:19:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:39.124+0000] {docker.py:436} INFO - 24/06/27 08:19:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:49.130+0000] {docker.py:436} INFO - 24/06/27 08:19:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:58.031+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/1 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.1.bdb3773b-c245-495e-96b9-e661cb261b27.tmp
[2024-06-27T08:19:58.232+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.1.bdb3773b-c245-495e-96b9-e661cb261b27.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/1
24/06/27 08:19:58 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719476397986,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:19:58.596+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.617+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.833+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.846+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.929+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.941+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:59.004+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:19:59.024+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:19:59.053+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:19:59.057+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:19:59.058+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:19:59.058+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:19:59.059+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:19:59.082+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:19:59.085+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:19:59.094+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:19:59.095+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:19:59.096+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:19:59.106+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-27T08:19:59.118+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:19:59.119+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-27T08:19:59.274+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 54 for partition store_source_data-0
[2024-06-27T08:19:59.286+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:19:59.793+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:19:59.794+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:19:59.794+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=55, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:19:59.803+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:19:59.803+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2024-06-27T08:19:59.807+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2108 bytes result sent to driver
[2024-06-27T08:19:59.810+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 703 ms on localhost (executor driver) (1/1)
[2024-06-27T08:19:59.813+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.746 s
[2024-06-27T08:19:59.815+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:19:59.817+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-27T08:19:59.818+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-06-27T08:19:59.819+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.785495 s
[2024-06-27T08:19:59.820+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:19:59.820+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 1
-------------------------------------------
[2024-06-27T08:19:59.958+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:19:59.959+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:19:59.987+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/1 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.1.1fad9223-d05c-4b43-85ab-6ee8839bf782.tmp
[2024-06-27T08:20:00.033+0000] {docker.py:436} INFO - 24/06/27 08:20:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.1.1fad9223-d05c-4b43-85ab-6ee8839bf782.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/1
[2024-06-27T08:20:00.042+0000] {docker.py:436} INFO - 24/06/27 08:20:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:19:57.980Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.4870920603994155,
  "durationMs" : {
    "addBatch" : 1309,
    "commitOffsets" : 73,
    "getBatch" : 6,
    "latestOffset" : 6,
    "queryPlanning" : 364,
    "triggerExecution" : 2053,
    "walCommit" : 248
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.4870920603994155,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:20:10.047+0000] {docker.py:436} INFO - 24/06/27 08:20:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:12.829+0000] {docker.py:436} INFO - 24/06/27 08:20:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:20:20.063+0000] {docker.py:436} INFO - 24/06/27 08:20:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:30.060+0000] {docker.py:436} INFO - 24/06/27 08:20:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:40.066+0000] {docker.py:436} INFO - 24/06/27 08:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:50.071+0000] {docker.py:436} INFO - 24/06/27 08:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:00.075+0000] {docker.py:436} INFO - 24/06/27 08:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:10.075+0000] {docker.py:436} INFO - 24/06/27 08:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:20.076+0000] {docker.py:436} INFO - 24/06/27 08:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:30.084+0000] {docker.py:436} INFO - 24/06/27 08:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:40.091+0000] {docker.py:436} INFO - 24/06/27 08:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:50.096+0000] {docker.py:436} INFO - 24/06/27 08:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:00.103+0000] {docker.py:436} INFO - 24/06/27 08:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:10.111+0000] {docker.py:436} INFO - 24/06/27 08:22:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:20.113+0000] {docker.py:436} INFO - 24/06/27 08:22:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:30.112+0000] {docker.py:436} INFO - 24/06/27 08:22:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:40.116+0000] {docker.py:436} INFO - 24/06/27 08:22:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:50.123+0000] {docker.py:436} INFO - 24/06/27 08:22:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:00.126+0000] {docker.py:436} INFO - 24/06/27 08:23:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:10.132+0000] {docker.py:436} INFO - 24/06/27 08:23:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:20.142+0000] {docker.py:436} INFO - 24/06/27 08:23:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:30.142+0000] {docker.py:436} INFO - 24/06/27 08:23:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:40.151+0000] {docker.py:436} INFO - 24/06/27 08:23:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:50.156+0000] {docker.py:436} INFO - 24/06/27 08:23:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:53.049+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/2 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.2.0353f36d-f18a-4f85-a2ff-5b74c646c360.tmp
[2024-06-27T08:23:53.285+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.2.0353f36d-f18a-4f85-a2ff-5b74c646c360.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/2
24/06/27 08:23:53 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719476632999,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:23:53.397+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.439+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.541+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.545+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.650+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.656+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.702+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:23:53.703+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:23:53.705+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:23:53.707+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:23:53 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:23:53 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:23:53.708+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:23:53.711+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.833+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
24/06/27 08:23:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
24/06/27 08:23:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/27 08:23:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:23:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/27 08:23:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:23:53.847+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-06-27T08:23:53.922+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 55 for partition store_source_data-0
[2024-06-27T08:23:53.934+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:23:54.444+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:23:54.447+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=56, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:23:54.452+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:23:54 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2024-06-27T08:23:54.456+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2108 bytes result sent to driver
[2024-06-27T08:23:54.459+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 631 ms on localhost (executor driver) (1/1)
[2024-06-27T08:23:54.459+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-27T08:23:54.464+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.753 s
[2024-06-27T08:23:54.469+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:23:54.470+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-06-27T08:23:54.471+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.759089 s
[2024-06-27T08:23:54.472+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:23:54.473+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.474+0000] {docker.py:436} INFO - Batch: 2
[2024-06-27T08:23:54.474+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:23:54.601+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:23:54.602+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:23:54.613+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/2 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.2.80b99547-4889-4762-a5c1-e5fd5b0fdc75.tmp
[2024-06-27T08:23:54.667+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.2.80b99547-4889-4762-a5c1-e5fd5b0fdc75.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/2
[2024-06-27T08:23:54.673+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:23:52.976Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.5913660555884092,
  "durationMs" : {
    "addBatch" : 1142,
    "commitOffsets" : 67,
    "getBatch" : 0,
    "latestOffset" : 22,
    "queryPlanning" : 175,
    "triggerExecution" : 1691,
    "walCommit" : 280
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.5913660555884092,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:24:04.678+0000] {docker.py:436} INFO - 24/06/27 08:24:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:14.680+0000] {docker.py:436} INFO - 24/06/27 08:24:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:24.697+0000] {docker.py:436} INFO - 24/06/27 08:24:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:34.707+0000] {docker.py:436} INFO - 24/06/27 08:24:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:43.259+0000] {docker.py:436} INFO - 24/06/27 08:24:43 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:24:44.717+0000] {docker.py:436} INFO - 24/06/27 08:24:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:54.720+0000] {docker.py:436} INFO - 24/06/27 08:24:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:04.730+0000] {docker.py:436} INFO - 24/06/27 08:25:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:14.741+0000] {docker.py:436} INFO - 24/06/27 08:25:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:24.743+0000] {docker.py:436} INFO - 24/06/27 08:25:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:34.744+0000] {docker.py:436} INFO - 24/06/27 08:25:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:44.746+0000] {docker.py:436} INFO - 24/06/27 08:25:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:54.746+0000] {docker.py:436} INFO - 24/06/27 08:25:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:04.750+0000] {docker.py:436} INFO - 24/06/27 08:26:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:14.757+0000] {docker.py:436} INFO - 24/06/27 08:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:24.760+0000] {docker.py:436} INFO - 24/06/27 08:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:34.763+0000] {docker.py:436} INFO - 24/06/27 08:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:44.767+0000] {docker.py:436} INFO - 24/06/27 08:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:54.773+0000] {docker.py:436} INFO - 24/06/27 08:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:04.774+0000] {docker.py:436} INFO - 24/06/27 08:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:14.780+0000] {docker.py:436} INFO - 24/06/27 08:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:24.788+0000] {docker.py:436} INFO - 24/06/27 08:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:34.790+0000] {docker.py:436} INFO - 24/06/27 08:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:44.790+0000] {docker.py:436} INFO - 24/06/27 08:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:54.800+0000] {docker.py:436} INFO - 24/06/27 08:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:04.802+0000] {docker.py:436} INFO - 24/06/27 08:28:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:14.807+0000] {docker.py:436} INFO - 24/06/27 08:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:24.816+0000] {docker.py:436} INFO - 24/06/27 08:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:34.816+0000] {docker.py:436} INFO - 24/06/27 08:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:44.820+0000] {docker.py:436} INFO - 24/06/27 08:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:54.822+0000] {docker.py:436} INFO - 24/06/27 08:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:56.227+0000] {docker.py:436} INFO - 24/06/27 08:28:56 INFO Metrics: Metrics scheduler closed
24/06/27 08:28:56 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:28:56.228+0000] {docker.py:436} INFO - 24/06/27 08:28:56 INFO Metrics: Metrics reporters closed
[2024-06-27T08:28:56.231+0000] {docker.py:436} INFO - 24/06/27 08:28:56 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-1 unregistered
[2024-06-27T08:29:04.830+0000] {docker.py:436} INFO - 24/06/27 08:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:14.834+0000] {docker.py:436} INFO - 24/06/27 08:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:24.836+0000] {docker.py:436} INFO - 24/06/27 08:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:34.840+0000] {docker.py:436} INFO - 24/06/27 08:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:44.842+0000] {docker.py:436} INFO - 24/06/27 08:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:54.843+0000] {docker.py:436} INFO - 24/06/27 08:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:04.851+0000] {docker.py:436} INFO - 24/06/27 08:30:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:14.856+0000] {docker.py:436} INFO - 24/06/27 08:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:23.709+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/3 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.3.682f5708-0f8b-4213-aa10-6c8a5ddcfe93.tmp
[2024-06-27T08:30:23.852+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.3.682f5708-0f8b-4213-aa10-6c8a5ddcfe93.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/3
[2024-06-27T08:30:23.858+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719477023667,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:30:24.028+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.088+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.195+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.228+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.396+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:30:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:24.456+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:30:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:30:24.457+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:30:24.458+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:30:24.458+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:30:24.459+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:30:24.459+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:30:24.460+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:30:24.526+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:30:24.528+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:30:24.547+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:30:24.548+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:30:24.579+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-06-27T08:30:24.596+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:30:24.625+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2024-06-27T08:30:25.044+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:30:25.271+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:30:25.282+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:30:25.282+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO AppInfoParser: Kafka startTimeMs: 1719477025270
[2024-06-27T08:30:25.283+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Subscribed to partition(s): store_source_data-0
24/06/27 08:30:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 56 for partition store_source_data-0
[2024-06-27T08:30:25.432+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:30:25.493+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:30:26.002+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:30:26.003+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:30:26.013+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=57, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:30:26.021+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:30:26.022+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2024-06-27T08:30:26.022+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2108 bytes result sent to driver
[2024-06-27T08:30:26.039+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1465 ms on localhost (executor driver) (1/1)
[2024-06-27T08:30:26.049+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-06-27T08:30:26.055+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.597 s
24/06/27 08:30:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:30:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-06-27T08:30:26.060+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.608232 s
24/06/27 08:30:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:30:26.061+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 3
-------------------------------------------
[2024-06-27T08:30:26.407+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:30:26.408+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:30:26.450+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/3 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.3.303b6c5a-e674-4031-8eb8-5f77b228b4a7.tmp
[2024-06-27T08:30:26.619+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.3.303b6c5a-e674-4031-8eb8-5f77b228b4a7.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/3
[2024-06-27T08:30:26.623+0000] {docker.py:436} INFO - 24/06/27 08:30:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:30:23.653Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.33715441672285906,
  "durationMs" : {
    "addBatch" : 2313,
    "commitOffsets" : 210,
    "getBatch" : 0,
    "latestOffset" : 13,
    "queryPlanning" : 237,
    "triggerExecution" : 2966,
    "walCommit" : 192
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.33715441672285906,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:30:36.621+0000] {docker.py:436} INFO - 24/06/27 08:30:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:46.626+0000] {docker.py:436} INFO - 24/06/27 08:30:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:56.633+0000] {docker.py:436} INFO - 24/06/27 08:30:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:06.641+0000] {docker.py:436} INFO - 24/06/27 08:31:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:13.705+0000] {docker.py:436} INFO - 24/06/27 08:31:13 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:31:16.645+0000] {docker.py:436} INFO - 24/06/27 08:31:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:23.811+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/4 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.4.be1c5faf-ef36-4818-8bc6-25ff8827d011.tmp
[2024-06-27T08:31:23.943+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.4.be1c5faf-ef36-4818-8bc6-25ff8827d011.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/4
24/06/27 08:31:23 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719477083792,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:31:24.011+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.019+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.157+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.157+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.177+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.196+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:31:24.197+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:31:24.198+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:31:24 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:31:24 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:31:24.199+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:31:24.200+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:31:24.207+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.229+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.234+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:31:24.237+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:31:24.259+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:31:24.327+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2024-06-27T08:31:24.342+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:31:24.372+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-06-27T08:31:24.550+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 57 for partition store_source_data-0
[2024-06-27T08:31:24.582+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:31:25.083+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:31:25.086+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:31:25.087+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=58, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:31:25.091+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:31:25 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2024-06-27T08:31:25.096+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2108 bytes result sent to driver
[2024-06-27T08:31:25.099+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 770 ms on localhost (executor driver) (1/1)
[2024-06-27T08:31:25.100+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-06-27T08:31:25.105+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 4
-------------------------------------------
[2024-06-27T08:31:25.106+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.901 s
24/06/27 08:31:25 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:31:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/27 08:31:25 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.905842 s
24/06/27 08:31:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:31:25.190+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:31:25.191+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:31:25.252+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/4 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.4.0b002eec-a0ac-41b6-be23-533b60873584.tmp
[2024-06-27T08:31:25.348+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.4.0b002eec-a0ac-41b6-be23-533b60873584.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/4
[2024-06-27T08:31:25.351+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:31:23.789Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6414368184733804,
  "durationMs" : {
    "addBatch" : 1169,
    "commitOffsets" : 150,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 86,
    "triggerExecution" : 1559,
    "walCommit" : 149
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6414368184733804,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:31:35.352+0000] {docker.py:436} INFO - 24/06/27 08:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:45.354+0000] {docker.py:436} INFO - 24/06/27 08:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:55.355+0000] {docker.py:436} INFO - 24/06/27 08:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:05.360+0000] {docker.py:436} INFO - 24/06/27 08:32:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:13.876+0000] {docker.py:436} INFO - 24/06/27 08:32:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:32:15.366+0000] {docker.py:436} INFO - 24/06/27 08:32:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:25.371+0000] {docker.py:436} INFO - 24/06/27 08:32:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:35.372+0000] {docker.py:436} INFO - 24/06/27 08:32:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:45.375+0000] {docker.py:436} INFO - 24/06/27 08:32:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:55.384+0000] {docker.py:436} INFO - 24/06/27 08:32:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:05.387+0000] {docker.py:436} INFO - 24/06/27 08:33:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:15.394+0000] {docker.py:436} INFO - 24/06/27 08:33:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:25.394+0000] {docker.py:436} INFO - 24/06/27 08:33:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:35.394+0000] {docker.py:436} INFO - 24/06/27 08:33:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:45.398+0000] {docker.py:436} INFO - 24/06/27 08:33:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:55.406+0000] {docker.py:436} INFO - 24/06/27 08:33:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:05.408+0000] {docker.py:436} INFO - 24/06/27 08:34:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:15.419+0000] {docker.py:436} INFO - 24/06/27 08:34:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:25.418+0000] {docker.py:436} INFO - 24/06/27 08:34:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:35.427+0000] {docker.py:436} INFO - 24/06/27 08:34:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:45.428+0000] {docker.py:436} INFO - 24/06/27 08:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:55.434+0000] {docker.py:436} INFO - 24/06/27 08:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:05.444+0000] {docker.py:436} INFO - 24/06/27 08:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:15.451+0000] {docker.py:436} INFO - 24/06/27 08:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:25.454+0000] {docker.py:436} INFO - 24/06/27 08:35:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:35.462+0000] {docker.py:436} INFO - 24/06/27 08:35:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:45.471+0000] {docker.py:436} INFO - 24/06/27 08:35:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:55.475+0000] {docker.py:436} INFO - 24/06/27 08:35:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:05.482+0000] {docker.py:436} INFO - 24/06/27 08:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:15.482+0000] {docker.py:436} INFO - 24/06/27 08:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:25.484+0000] {docker.py:436} INFO - 24/06/27 08:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:35.490+0000] {docker.py:436} INFO - 24/06/27 08:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:45.490+0000] {docker.py:436} INFO - 24/06/27 08:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:50.189+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/5 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.5.6d227f34-08fd-45bd-8abf-1afc6e78d479.tmp
[2024-06-27T08:36:50.347+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.5.6d227f34-08fd-45bd-8abf-1afc6e78d479.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/5
24/06/27 08:36:50 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1719477410150,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:36:50.542+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.551+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.595+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.616+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.664+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.672+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.804+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:36:50.816+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:36:50.826+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:36:50.829+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:36:50.833+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:36:50.841+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:36:50.845+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:36:50.859+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.972+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.979+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:36:50.986+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:36:50.990+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:36:51.009+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-06-27T08:36:51.012+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:36:51.023+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-06-27T08:36:51.229+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 58 for partition store_source_data-0
[2024-06-27T08:36:51.379+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:36:51.762+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:36:51.763+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=59, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:36:51.776+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:36:51.781+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2024-06-27T08:36:51.792+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2108 bytes result sent to driver
[2024-06-27T08:36:51.813+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 802 ms on localhost (executor driver) (1/1)
[2024-06-27T08:36:51.814+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-06-27T08:36:51.816+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.965 s
[2024-06-27T08:36:51.822+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:36:51.823+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-06-27T08:36:51.825+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 1.000019 s
[2024-06-27T08:36:51.828+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:36:51.829+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:36:51.830+0000] {docker.py:436} INFO - Batch: 5
[2024-06-27T08:36:51.831+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:36:51.972+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:36:51.977+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:36:52.008+0000] {docker.py:436} INFO - 24/06/27 08:36:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/5 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.5.cf38c5ad-59a0-4a59-9c37-6b6098fdc0fb.tmp
[2024-06-27T08:36:52.108+0000] {docker.py:436} INFO - 24/06/27 08:36:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.5.cf38c5ad-59a0-4a59-9c37-6b6098fdc0fb.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/5
[2024-06-27T08:36:52.113+0000] {docker.py:436} INFO - 24/06/27 08:36:52 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:36:50.142Z",
  "batchId" : 5,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.508130081300813,
  "durationMs" : {
    "addBatch" : 1418,
    "commitOffsets" : 138,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 206,
    "triggerExecution" : 1968,
    "walCommit" : 196
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.508130081300813,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:37:02.112+0000] {docker.py:436} INFO - 24/06/27 08:37:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:12.121+0000] {docker.py:436} INFO - 24/06/27 08:37:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:22.126+0000] {docker.py:436} INFO - 24/06/27 08:37:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:32.131+0000] {docker.py:436} INFO - 24/06/27 08:37:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:36.866+0000] {docker.py:436} INFO - 24/06/27 08:37:36 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:37:42.138+0000] {docker.py:436} INFO - 24/06/27 08:37:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:52.146+0000] {docker.py:436} INFO - 24/06/27 08:37:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:02.146+0000] {docker.py:436} INFO - 24/06/27 08:38:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:12.155+0000] {docker.py:436} INFO - 24/06/27 08:38:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:22.161+0000] {docker.py:436} INFO - 24/06/27 08:38:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:32.171+0000] {docker.py:436} INFO - 24/06/27 08:38:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:42.177+0000] {docker.py:436} INFO - 24/06/27 08:38:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:52.183+0000] {docker.py:436} INFO - 24/06/27 08:38:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:02.195+0000] {docker.py:436} INFO - 24/06/27 08:39:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:12.197+0000] {docker.py:436} INFO - 24/06/27 08:39:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:22.199+0000] {docker.py:436} INFO - 24/06/27 08:39:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:32.205+0000] {docker.py:436} INFO - 24/06/27 08:39:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:42.208+0000] {docker.py:436} INFO - 24/06/27 08:39:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:52.211+0000] {docker.py:436} INFO - 24/06/27 08:39:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:55.692+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/6 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.6.6658bb2c-323e-4e20-9e6c-edcecc59b53b.tmp
[2024-06-27T08:39:55.866+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.6.6658bb2c-323e-4e20-9e6c-edcecc59b53b.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/6
24/06/27 08:39:55 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1719477595647,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:39:55.921+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.945+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.111+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.146+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.225+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.240+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:56.332+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:39:56.333+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:39:56.345+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:39:56 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:39:56 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:39:56 INFO DAGScheduler: Missing parents: List()
24/06/27 08:39:56 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:39:56.353+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
24/06/27 08:39:56 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:39:56.353+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:39:56.354+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:39:56.362+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:39:56 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/27 08:39:56 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:39:56.368+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2024-06-27T08:39:56.451+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 59 for partition store_source_data-0
[2024-06-27T08:39:56.457+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:39:56.977+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:39:56.979+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:39:56.988+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:39:56 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2024-06-27T08:39:56.998+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2108 bytes result sent to driver
24/06/27 08:39:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 643 ms on localhost (executor driver) (1/1)
[2024-06-27T08:39:57.012+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-06-27T08:39:57.013+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.665 s
[2024-06-27T08:39:57.014+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:39:57.014+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 6
-------------------------------------------
[2024-06-27T08:39:57.014+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/27 08:39:57 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.678416 s
24/06/27 08:39:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:39:57.276+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:39:57.289+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:39:57.346+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/6 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.6.22e4ae12-bdc1-42c2-a992-9562dc88e266.tmp
[2024-06-27T08:39:57.518+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.6.22e4ae12-bdc1-42c2-a992-9562dc88e266.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/6
[2024-06-27T08:39:57.527+0000] {docker.py:436} INFO - 24/06/27 08:39:57 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:39:55.643Z",
  "batchId" : 6,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.5327650506126798,
  "durationMs" : {
    "addBatch" : 1331,
    "commitOffsets" : 228,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 97,
    "triggerExecution" : 1877,
    "walCommit" : 216
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.5327650506126798,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:40:07.533+0000] {docker.py:436} INFO - 24/06/27 08:40:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:17.538+0000] {docker.py:436} INFO - 24/06/27 08:40:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:27.541+0000] {docker.py:436} INFO - 24/06/27 08:40:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:37.550+0000] {docker.py:436} INFO - 24/06/27 08:40:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:45.482+0000] {docker.py:436} INFO - 24/06/27 08:40:45 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:40:47.596+0000] {docker.py:436} INFO - 24/06/27 08:40:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:57.596+0000] {docker.py:436} INFO - 24/06/27 08:40:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:07.606+0000] {docker.py:436} INFO - 24/06/27 08:41:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:17.606+0000] {docker.py:436} INFO - 24/06/27 08:41:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:27.611+0000] {docker.py:436} INFO - 24/06/27 08:41:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:37.623+0000] {docker.py:436} INFO - 24/06/27 08:41:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:47.623+0000] {docker.py:436} INFO - 24/06/27 08:41:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:57.636+0000] {docker.py:436} INFO - 24/06/27 08:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:07.637+0000] {docker.py:436} INFO - 24/06/27 08:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:17.643+0000] {docker.py:436} INFO - 24/06/27 08:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:19.543+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/7 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.7.7bc381ec-4043-4930-bebc-00b85c5df42b.tmp
[2024-06-27T08:42:19.727+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.7.7bc381ec-4043-4930-bebc-00b85c5df42b.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/7
24/06/27 08:42:19 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1719477739496,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:19.936+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.937+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.980+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.993+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:20.064+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:20.067+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:20.117+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:20.118+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:20.120+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:42:20 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:42:20.120+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:20.121+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:20.122+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:20.135+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:42:20.150+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:42:20.164+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:42:20.166+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:20.167+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:42:20 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-06-27T08:42:20.168+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:42:20.169+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-06-27T08:42:20.335+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 60 for partition store_source_data-0
[2024-06-27T08:42:20.353+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:20.850+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:42:20.851+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:20.856+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=61, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:20 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2024-06-27T08:42:20.862+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2108 bytes result sent to driver
[2024-06-27T08:42:20.867+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 701 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:20.875+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/06/27 08:42:20 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.747 s
[2024-06-27T08:42:20.875+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:20.876+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-06-27T08:42:20.877+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.757015 s
[2024-06-27T08:42:20.879+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:20.880+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 7
-------------------------------------------
[2024-06-27T08:42:20.929+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:42:20.930+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:20.971+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/7 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.7.42d65f8e-1a11-4192-b52b-f79e2a8ce73a.tmp
[2024-06-27T08:42:21.044+0000] {docker.py:436} INFO - 24/06/27 08:42:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.7.42d65f8e-1a11-4192-b52b-f79e2a8ce73a.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/7
[2024-06-27T08:42:21.047+0000] {docker.py:436} INFO - 24/06/27 08:42:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:19.490Z",
  "batchId" : 7,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6435006435006435,
  "durationMs" : {
    "addBatch" : 990,
    "commitOffsets" : 115,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 218,
    "triggerExecution" : 1554,
    "walCommit" : 225
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6435006435006435,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:30.726+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/8 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.8.b58af650-5c9a-438d-979c-7b3c94f8778b.tmp
[2024-06-27T08:42:30.925+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.8.b58af650-5c9a-438d-979c-7b3c94f8778b.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/8
[2024-06-27T08:42:30.926+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1719477750693,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:31.158+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.172+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.224+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.321+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.468+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.483+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.624+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:42:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:31.636+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:42:31 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:42:31.638+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:42:31.645+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:42:31.653+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:31.682+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.2 KiB, free 434.3 MiB)
[2024-06-27T08:42:31.700+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.3 MiB)
[2024-06-27T08:42:31.712+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:42:31.712+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:31.726+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:42:31.729+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-06-27T08:42:31.744+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:42:31.761+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-06-27T08:42:31.978+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 61 for partition store_source_data-0
[2024-06-27T08:42:31.990+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:32.489+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:42:32.490+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:32.492+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=62, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:42:32.496+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:42:32.496+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2024-06-27T08:42:32.509+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2108 bytes result sent to driver
[2024-06-27T08:42:32.511+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 772 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:32.512+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-06-27T08:42:32.514+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.856 s
[2024-06-27T08:42:32.516+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:32.520+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-06-27T08:42:32.522+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.892841 s
[2024-06-27T08:42:32.524+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:32.525+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 8
-------------------------------------------
[2024-06-27T08:42:32.635+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:42:32.636+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:32.686+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/8 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.8.ef608fd8-8d09-4d4e-8971-4f87c7efbe38.tmp
[2024-06-27T08:42:32.802+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.8.ef608fd8-8d09-4d4e-8971-4f87c7efbe38.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/8
24/06/27 08:42:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:30.675Z",
  "batchId" : 8,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.47058823529411764,
  "durationMs" : {
    "addBatch" : 1465,
    "commitOffsets" : 156,
    "getBatch" : 4,
    "latestOffset" : 18,
    "queryPlanning" : 248,
    "triggerExecution" : 2125,
    "walCommit" : 232
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.47058823529411764,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:42.807+0000] {docker.py:436} INFO - 24/06/27 08:42:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:52.810+0000] {docker.py:436} INFO - 24/06/27 08:42:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:02.810+0000] {docker.py:436} INFO - 24/06/27 08:43:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:12.819+0000] {docker.py:436} INFO - 24/06/27 08:43:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:15.369+0000] {docker.py:436} INFO - 24/06/27 08:43:15 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:43:15.380+0000] {docker.py:436} INFO - 24/06/27 08:43:15 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:43:22.829+0000] {docker.py:436} INFO - 24/06/27 08:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:32.831+0000] {docker.py:436} INFO - 24/06/27 08:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:42.837+0000] {docker.py:436} INFO - 24/06/27 08:43:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:52.847+0000] {docker.py:436} INFO - 24/06/27 08:43:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:02.849+0000] {docker.py:436} INFO - 24/06/27 08:44:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:12.860+0000] {docker.py:436} INFO - 24/06/27 08:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:22.861+0000] {docker.py:436} INFO - 24/06/27 08:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:32.863+0000] {docker.py:436} INFO - 24/06/27 08:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:42.869+0000] {docker.py:436} INFO - 24/06/27 08:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:52.875+0000] {docker.py:436} INFO - 24/06/27 08:44:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:02.874+0000] {docker.py:436} INFO - 24/06/27 08:45:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:12.881+0000] {docker.py:436} INFO - 24/06/27 08:45:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:22.891+0000] {docker.py:436} INFO - 24/06/27 08:45:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:32.897+0000] {docker.py:436} INFO - 24/06/27 08:45:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:42.902+0000] {docker.py:436} INFO - 24/06/27 08:45:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:48.221+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/9 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.9.bb256ec1-1af3-43fb-8410-933a1b1baeaf.tmp
[2024-06-27T08:45:48.333+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.9.bb256ec1-1af3-43fb-8410-933a1b1baeaf.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/9
24/06/27 08:45:48 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1719477948166,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:45:48.508+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.538+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.793+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.829+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.961+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.984+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:49.086+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:45:49.086+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:45:49.087+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:45:49.087+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:45:49.097+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:45:49.098+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:45:49.101+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:45:49.113+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
24/06/27 08:45:49 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
24/06/27 08:45:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
24/06/27 08:45:49 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
24/06/27 08:45:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:45:49 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-06-27T08:45:49.136+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:45:49.173+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2024-06-27T08:45:49.271+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 62 for partition store_source_data-0
[2024-06-27T08:45:49.278+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:45:49.783+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.786+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:45:49.797+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=63, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.800+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:45:49.801+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2024-06-27T08:45:49.810+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2108 bytes result sent to driver
[2024-06-27T08:45:49.813+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 679 ms on localhost (executor driver) (1/1)
[2024-06-27T08:45:49.815+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-06-27T08:45:49.817+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.716 s
24/06/27 08:45:49 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:45:49.820+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-06-27T08:45:49.828+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.754634 s
24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:45:49.828+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:45:49.829+0000] {docker.py:436} INFO - Batch: 9
[2024-06-27T08:45:49.829+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:45:49.959+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:45:49.960+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:45:50.021+0000] {docker.py:436} INFO - 24/06/27 08:45:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/9 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.9.fe1ab767-7a52-4b9d-9a31-c89453cf7268.tmp
[2024-06-27T08:45:50.118+0000] {docker.py:436} INFO - 24/06/27 08:45:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.9.fe1ab767-7a52-4b9d-9a31-c89453cf7268.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/9
[2024-06-27T08:45:50.121+0000] {docker.py:436} INFO - 24/06/27 08:45:50 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:45:48.150Z",
  "batchId" : 9,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 71.42857142857143,
  "processedRowsPerSecond" : 0.508130081300813,
  "durationMs" : {
    "addBatch" : 1396,
    "commitOffsets" : 156,
    "getBatch" : 0,
    "latestOffset" : 16,
    "queryPlanning" : 221,
    "triggerExecution" : 1968,
    "walCommit" : 152
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 71.42857142857143,
    "processedRowsPerSecond" : 0.508130081300813,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:46:00.121+0000] {docker.py:436} INFO - 24/06/27 08:46:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:10.125+0000] {docker.py:436} INFO - 24/06/27 08:46:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:16.460+0000] {docker.py:436} INFO - 24/06/27 08:46:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:46:20.137+0000] {docker.py:436} INFO - 24/06/27 08:46:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:30.135+0000] {docker.py:436} INFO - 24/06/27 08:46:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:40.137+0000] {docker.py:436} INFO - 24/06/27 08:46:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:50.138+0000] {docker.py:436} INFO - 24/06/27 08:46:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:00.140+0000] {docker.py:436} INFO - 24/06/27 08:47:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:10.150+0000] {docker.py:436} INFO - 24/06/27 08:47:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:20.156+0000] {docker.py:436} INFO - 24/06/27 08:47:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:30.160+0000] {docker.py:436} INFO - 24/06/27 08:47:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:40.168+0000] {docker.py:436} INFO - 24/06/27 08:47:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:50.176+0000] {docker.py:436} INFO - 24/06/27 08:47:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:00.182+0000] {docker.py:436} INFO - 24/06/27 08:48:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:10.185+0000] {docker.py:436} INFO - 24/06/27 08:48:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:20.193+0000] {docker.py:436} INFO - 24/06/27 08:48:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:30.197+0000] {docker.py:436} INFO - 24/06/27 08:48:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:40.207+0000] {docker.py:436} INFO - 24/06/27 08:48:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:50.207+0000] {docker.py:436} INFO - 24/06/27 08:48:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:00.217+0000] {docker.py:436} INFO - 24/06/27 08:49:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:10.225+0000] {docker.py:436} INFO - 24/06/27 08:49:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:20.234+0000] {docker.py:436} INFO - 24/06/27 08:49:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:30.243+0000] {docker.py:436} INFO - 24/06/27 08:49:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:40.252+0000] {docker.py:436} INFO - 24/06/27 08:49:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:50.259+0000] {docker.py:436} INFO - 24/06/27 08:49:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:00.261+0000] {docker.py:436} INFO - 24/06/27 08:50:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:10.261+0000] {docker.py:436} INFO - 24/06/27 08:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:20.266+0000] {docker.py:436} INFO - 24/06/27 08:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:30.279+0000] {docker.py:436} INFO - 24/06/27 08:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:40.286+0000] {docker.py:436} INFO - 24/06/27 08:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:50.294+0000] {docker.py:436} INFO - 24/06/27 08:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:56.244+0000] {docker.py:436} INFO - 24/06/27 08:50:56 INFO Metrics: Metrics scheduler closed
[2024-06-27T08:50:56.244+0000] {docker.py:436} INFO - 24/06/27 08:50:56 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:50:56.245+0000] {docker.py:436} INFO - 24/06/27 08:50:56 INFO Metrics: Metrics reporters closed
[2024-06-27T08:50:56.246+0000] {docker.py:436} INFO - 24/06/27 08:50:56 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-2 unregistered
[2024-06-27T08:51:00.298+0000] {docker.py:436} INFO - 24/06/27 08:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:08.363+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/10 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.10.44f11cc9-0f98-401b-940b-1d654b50ac6b.tmp
[2024-06-27T08:51:08.545+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/.10.44f11cc9-0f98-401b-940b-1d654b50ac6b.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/offsets/10
[2024-06-27T08:51:08.545+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1719478268290,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:51:08.690+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.702+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.720+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.760+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.769+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.906+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:51:08.914+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:51:08.917+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:51:08.918+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:51:08.923+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:51:08.925+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:51:08.926+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:51:08.931+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 24.2 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.977+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.994+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:40553 (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:51:09.046+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:51:09.047+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:51:09.047+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-06-27T08:51:09.048+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:51:09.079+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2024-06-27T08:51:09.184+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:51:09.259+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:51:09.260+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:51:09.262+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO AppInfoParser: Kafka startTimeMs: 1719478269228
[2024-06-27T08:51:09.263+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:51:09.266+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to offset 63 for partition store_source_data-0
[2024-06-27T08:51:09.329+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:51:09.360+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:51:09.870+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:51:09.871+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:51:09.871+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor-3, groupId=spark-kafka-source-9bb44440-57a8-466a-b810-82f2b0321659-2021635890-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=64, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:51:09.886+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:51:09.889+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2024-06-27T08:51:09.911+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2108 bytes result sent to driver
[2024-06-27T08:51:09.915+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 867 ms on localhost (executor driver) (1/1)
[2024-06-27T08:51:09.917+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-06-27T08:51:09.925+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.994 s
[2024-06-27T08:51:09.926+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:51:09.929+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-06-27T08:51:09.930+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 1.013938 s
[2024-06-27T08:51:09.935+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 10
-------------------------------------------
[2024-06-27T08:51:09.936+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:51:10.005+0000] {docker.py:436} INFO - +--------------+----------------+------+--------+------------+--------------+
|item_purchased|date_of_purchase|  cost|quantity|purchased_by|payment_method|
+--------------+----------------+------+--------+------------+--------------+
|        Laptop|      2023-01-01|1200.0|       1|      Father|          Card|
+--------------+----------------+------+--------+------------+--------------+
[2024-06-27T08:51:10.006+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:51:10.068+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/10 using temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.10.ef76d34f-6063-41cd-83ab-40f220c3f0d1.tmp
[2024-06-27T08:51:10.216+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/.10.ef76d34f-6063-41cd-83ab-40f220c3f0d1.tmp to file:/tmp/temporary-869d5565-fcda-4fe2-9f17-66afabce0c8f/commits/10
[2024-06-27T08:51:10.218+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "02b4530d-3569-4ad8-9560-4576be621abb",
  "runId" : "dd284288-5f65-4233-8fcd-e0f050301bf6",
  "name" : null,
  "timestamp" : "2024-06-27T08:51:08.287Z",
  "batchId" : 10,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 55.55555555555556,
  "processedRowsPerSecond" : 0.5184033177812338,
  "durationMs" : {
    "addBatch" : 1305,
    "commitOffsets" : 208,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 162,
    "triggerExecution" : 1929,
    "walCommit" : 250
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 55.55555555555556,
    "processedRowsPerSecond" : 0.5184033177812338,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@60f4dc93",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:51:20.220+0000] {docker.py:436} INFO - 24/06/27 08:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:30.228+0000] {docker.py:436} INFO - 24/06/27 08:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:40.232+0000] {docker.py:436} INFO - 24/06/27 08:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:50.233+0000] {docker.py:436} INFO - 24/06/27 08:51:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:54.092+0000] {docker.py:436} INFO - 24/06/27 08:51:54 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:40553 in memory (size: 10.8 KiB, free: 434.4 MiB)
[2024-06-27T08:52:00.245+0000] {docker.py:436} INFO - 24/06/27 08:52:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:10.259+0000] {docker.py:436} INFO - 24/06/27 08:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:20.269+0000] {docker.py:436} INFO - 24/06/27 08:52:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:30.269+0000] {docker.py:436} INFO - 24/06/27 08:52:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:40.276+0000] {docker.py:436} INFO - 24/06/27 08:52:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:50.275+0000] {docker.py:436} INFO - 24/06/27 08:52:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:00.278+0000] {docker.py:436} INFO - 24/06/27 08:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:10.285+0000] {docker.py:436} INFO - 24/06/27 08:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:20.292+0000] {docker.py:436} INFO - 24/06/27 08:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:30.293+0000] {docker.py:436} INFO - 24/06/27 08:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:36.462+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:36.599+0000] {process_utils.py:132} INFO - Sending 15 to group 1317. PIDs of all processes in the group: [1317]
[2024-06-27T08:53:36.617+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 1317
[2024-06-27T08:53:36.629+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-06-27T08:53:36.629+0000] {docker.py:528} INFO - Stopping docker container
[2024-06-27T08:53:36.662+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T08:53:36.666+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 265, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.45/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 371, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 398, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 439, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 456, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 271, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 267, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.45/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/airflowtmpn51vy8ad")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26dfa420>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26dfa420>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 438, in _run_image_with_mounts
    result = self.cli.wait(self.container["Id"])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1346, in wait
    res = self._post(url, timeout=timeout, params=params)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26dfa420>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c62000>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c62000>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 380, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 462, in _run_image_with_mounts
    self.cli.remove_container(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1033, in remove_container
    res = self._delete(
          ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 244, in _delete
    return self.delete(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 671, in delete
    return self.request("DELETE", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c62000>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c624e0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c624e0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 509, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 368, in _run_image
    with TemporaryDirectory(prefix="airflowtmp", dir=self.host_tmp_dir) as host_tmp_dir_generated:
  File "/usr/local/lib/python3.12/tempfile.py", line 946, in __exit__
    self.cleanup()
  File "/usr/local/lib/python3.12/tempfile.py", line 949, in cleanup
    if self._finalizer.detach() or _os.path.exists(self.name):
       ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/weakref.py", line 592, in detach
    def detach(self):
    
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2612, in signal_handler
    self.task.on_kill()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 532, in on_kill
    self.cli.stop(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1210, in stop
    res = self._post(url, params=params, timeout=conn_timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c624e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
[2024-06-27T08:53:37.085+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=Stage_1, task_id=run_bronze_job, run_id=manual__2024-06-27T08:18:18.522986+00:00, execution_date=20240627T081818, start_date=20240627T081825, end_date=20240627T085337
[2024-06-27T08:53:37.387+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 266 for task run_bronze_job (HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/86fe2c81eccb0c703aa6fdf545fecf2cb6ce4bc463296f0f0fefb28767671f4a/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c624e0>: Failed to establish a new connection: [Errno 111] Connection refused')); 1317)
[2024-06-27T08:53:37.498+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=1317, status='terminated', exitcode=1, started='08:18:24') (1317) terminated with exit code 1
[2024-06-27T08:53:37.500+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 143
[2024-06-27T08:53:37.618+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T08:53:37.673+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
