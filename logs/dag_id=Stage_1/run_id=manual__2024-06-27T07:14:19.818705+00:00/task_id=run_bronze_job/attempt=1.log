[2024-06-27T07:14:25.967+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-27T07:14:26.008+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:14:19.818705+00:00 [queued]>
[2024-06-27T07:14:26.019+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:14:19.818705+00:00 [queued]>
[2024-06-27T07:14:26.020+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-27T07:14:26.037+0000] {taskinstance.py:2330} INFO - Executing <Task(DockerOperator): run_bronze_job> on 2024-06-27 07:14:19.818705+00:00
[2024-06-27T07:14:26.046+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61 DeprecationWarning: This process (pid=390) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-06-27T07:14:26.048+0000] {standard_task_runner.py:63} INFO - Started process 391 to run task
[2024-06-27T07:14:26.046+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'Stage_1', 'run_bronze_job', 'manual__2024-06-27T07:14:19.818705+00:00', '--job-id', '136', '--raw', '--subdir', 'DAGS_FOLDER/kafka_dag.py', '--cfg-path', '/tmp/tmpp1q8i927']
[2024-06-27T07:14:26.049+0000] {standard_task_runner.py:91} INFO - Job 136: Subtask run_bronze_job
[2024-06-27T07:14:26.065+0000] {logging_mixin.py:188} WARNING - /home/***/.local/lib/python3.12/site-packages/***/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-06-27T07:14:26.101+0000] {task_command.py:426} INFO - Running <TaskInstance: Stage_1.run_bronze_job manual__2024-06-27T07:14:19.818705+00:00 [running]> on host 08bfa8b73cac
[2024-06-27T07:14:26.206+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Stage_1' AIRFLOW_CTX_TASK_ID='run_bronze_job' AIRFLOW_CTX_EXECUTION_DATE='2024-06-27T07:14:19.818705+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-27T07:14:19.818705+00:00'
[2024-06-27T07:14:26.207+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-27T07:14:26.244+0000] {docker.py:366} INFO - Starting docker container from image bitnami/spark:latest
[2024-06-27T07:14:26.247+0000] {docker.py:374} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2024-06-27T07:14:26.719+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:14:26.71 [0m[38;5;2mINFO [0m ==>
[2024-06-27T07:14:26.721+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:14:26.72 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-27T07:14:26.723+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:14:26.72 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-27T07:14:26.727+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:14:26.72 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-27T07:14:26.730+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:14:26.72 [0m[38;5;2mINFO [0m ==> Upgrade to Tanzu Application Catalog for production environments to access custom-configured and pre-packaged software components. Gain enhanced features, including Software Bill of Materials (SBOM), CVE scan result reports, and VEX documents. To learn more, visit [1mhttps://bitnami.com/enterprise[0m
[2024-06-27T07:14:26.732+0000] {docker.py:436} INFO - [38;5;6mspark [38;5;5m07:14:26.73 [0m[38;5;2mINFO [0m ==>
[2024-06-27T07:14:26.745+0000] {docker.py:436} INFO - 
[2024-06-27T07:14:29.887+0000] {docker.py:436} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-27T07:14:30.017+0000] {docker.py:436} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2024-06-27T07:14:30.018+0000] {docker.py:436} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-27T07:14:30.023+0000] {docker.py:436} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-06-27T07:14:30.024+0000] {docker.py:436} INFO - org.postgresql#postgresql added as a dependency
[2024-06-27T07:14:30.025+0000] {docker.py:436} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-97161f20-f7cb-425c-8a86-8a59a1e3e1d4;1.0
	confs: [default]
[2024-06-27T07:14:32.751+0000] {docker.py:436} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T07:14:33.298+0000] {docker.py:436} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
[2024-06-27T07:14:33.489+0000] {docker.py:436} INFO - found org.apache.kafka#kafka-clients;2.8.1 in central
[2024-06-27T07:14:33.563+0000] {docker.py:436} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-27T07:14:33.634+0000] {docker.py:436} INFO - found org.xerial.snappy#snappy-java;1.1.8.4 in central
[2024-06-27T07:14:33.793+0000] {docker.py:436} INFO - found org.slf4j#slf4j-api;1.7.32 in central
[2024-06-27T07:14:34.132+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
[2024-06-27T07:14:34.291+0000] {docker.py:436} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2024-06-27T07:14:34.424+0000] {docker.py:436} INFO - found org.apache.hadoop#hadoop-client-api;3.3.2 in central
[2024-06-27T07:14:36.098+0000] {docker.py:436} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-27T07:14:36.152+0000] {docker.py:436} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-27T07:14:36.437+0000] {docker.py:436} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-27T07:14:38.245+0000] {docker.py:436} INFO - found org.postgresql#postgresql;42.2.2 in central
[2024-06-27T07:14:38.270+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T07:14:38.405+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (143ms)
[2024-06-27T07:14:38.418+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar ...
[2024-06-27T07:14:38.628+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.2.2!postgresql.jar(bundle) (221ms)
[2024-06-27T07:14:38.640+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...
[2024-06-27T07:14:38.666+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (35ms)
[2024-06-27T07:14:38.678+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...
[2024-06-27T07:14:39.512+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (843ms)
[2024-06-27T07:14:39.521+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-27T07:14:39.538+0000] {docker.py:436} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (25ms)
[2024-06-27T07:14:39.548+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-27T07:14:39.581+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (42ms)
[2024-06-27T07:14:39.591+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
[2024-06-27T07:14:39.602+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (19ms)
[2024-06-27T07:14:39.612+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...
[2024-06-27T07:14:43.779+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (4175ms)
[2024-06-27T07:14:43.789+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-27T07:14:43.866+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (87ms)
[2024-06-27T07:14:43.880+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...
[2024-06-27T07:14:44.112+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (244ms)
[2024-06-27T07:14:44.122+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...
[2024-06-27T07:14:44.136+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (22ms)
[2024-06-27T07:14:44.147+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...
[2024-06-27T07:14:46.085+0000] {docker.py:436} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (1948ms)
[2024-06-27T07:14:46.096+0000] {docker.py:436} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-27T07:14:46.132+0000] {docker.py:436} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (44ms)
[2024-06-27T07:14:46.134+0000] {docker.py:436} INFO - :: resolution report :: resolve 8234ms :: artifacts dl 7873ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.2.2 from central in [default]
	org.slf4j#slf4j-api;1.7.32 from central in [default]
[2024-06-27T07:14:46.134+0000] {docker.py:436} INFO - org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-27T07:14:46.152+0000] {docker.py:436} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-97161f20-f7cb-425c-8a86-8a59a1e3e1d4
[2024-06-27T07:14:46.153+0000] {docker.py:436} INFO - confs: [default]
[2024-06-27T07:14:46.264+0000] {docker.py:436} INFO - 13 artifacts copied, 0 already retrieved (57403kB/112ms)
[2024-06-27T07:14:46.646+0000] {docker.py:436} INFO - 24/06/27 07:14:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-27T07:14:49.290+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SparkContext: Running Spark version 3.5.1
[2024-06-27T07:14:49.292+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SparkContext: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T07:14:49.294+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SparkContext: Java version 17.0.11
[2024-06-27T07:14:49.331+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO ResourceUtils: ==============================================================
[2024-06-27T07:14:49.332+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-27T07:14:49.333+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO ResourceUtils: ==============================================================
[2024-06-27T07:14:49.334+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-27T07:14:49.371+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-27T07:14:49.382+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO ResourceProfile: Limiting resource is cpu
[2024-06-27T07:14:49.383+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-27T07:14:49.590+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SecurityManager: Changing view acls to: spark
[2024-06-27T07:14:49.591+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SecurityManager: Changing modify acls to: spark
[2024-06-27T07:14:49.592+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SecurityManager: Changing view acls groups to:
[2024-06-27T07:14:49.593+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SecurityManager: Changing modify acls groups to:
[2024-06-27T07:14:49.595+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-27T07:14:49.966+0000] {docker.py:436} INFO - 24/06/27 07:14:49 INFO Utils: Successfully started service 'sparkDriver' on port 41019.
[2024-06-27T07:14:50.030+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkEnv: Registering MapOutputTracker
[2024-06-27T07:14:50.117+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-27T07:14:50.146+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-27T07:14:50.148+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-27T07:14:50.156+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-27T07:14:50.195+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1af9ba74-ad43-4a7c-8e68-48299af5b07d
[2024-06-27T07:14:50.220+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-27T07:14:50.245+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-27T07:14:50.554+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-27T07:14:50.701+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-27T07:14:50.768+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://localhost:41019/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.769+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at spark://localhost:41019/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472489278
[2024-06-27T07:14:50.770+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://localhost:41019/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.770+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://localhost:41019/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472489278
[2024-06-27T07:14:50.771+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:41019/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.771+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:41019/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472489278
[2024-06-27T07:14:50.772+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://localhost:41019/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.772+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://localhost:41019/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:50.773+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:41019/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.773+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://localhost:41019/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472489278
[2024-06-27T07:14:50.773+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://localhost:41019/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472489278
[2024-06-27T07:14:50.774+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://localhost:41019/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:50.774+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:41019/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472489278
[2024-06-27T07:14:50.780+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.782+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:14:50.795+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472489278
[2024-06-27T07:14:50.796+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:14:50.802+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.803+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:14:50.809+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472489278
[2024-06-27T07:14:50.810+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:14:50.822+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472489278
24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:14:50.831+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472489278
[2024-06-27T07:14:50.831+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:14:50.839+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.840+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:14:50.860+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:50.861+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:14:50.929+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472489278
[2024-06-27T07:14:50.933+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:14:50.944+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472489278
[2024-06-27T07:14:50.945+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:14:50.972+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472489278
[2024-06-27T07:14:50.973+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:14:50.979+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:50.980+0000] {docker.py:436} INFO - 24/06/27 07:14:50 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:14:51.002+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472489278
[2024-06-27T07:14:51.004+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:14:51.143+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Starting executor ID driver on host localhost
[2024-06-27T07:14:51.145+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: OS info Linux, 6.5.0-41-generic, amd64
[2024-06-27T07:14:51.146+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Java version 17.0.11
[2024-06-27T07:14:51.160+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-27T07:14:51.161+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5878f5fe for default.
[2024-06-27T07:14:51.183+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472489278
[2024-06-27T07:14:51.220+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.2.2.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:14:51.224+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:51.246+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:14:51.251+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472489278
[2024-06-27T07:14:51.254+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:14:51.259+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:51.290+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:14:51.295+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472489278
[2024-06-27T07:14:51.296+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:14:51.301+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:51.302+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:14:51.307+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472489278
[2024-06-27T07:14:51.308+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:14:51.312+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:51.313+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:14:51.318+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472489278
[2024-06-27T07:14:51.319+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:14:51.323+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472489278
[2024-06-27T07:14:51.325+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:14:51.330+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472489278
[2024-06-27T07:14:51.330+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:14:51.335+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472489278
[2024-06-27T07:14:51.336+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:14:51.340+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472489278
[2024-06-27T07:14:51.346+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:14:51.355+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching spark://localhost:41019/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:51.428+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:41019 after 53 ms (0 ms spent in bootstraps)
[2024-06-27T07:14:51.439+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: Fetching spark://localhost:41019/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp12065716346971175421.tmp
[2024-06-27T07:14:51.545+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp12065716346971175421.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:14:51.555+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T07:14:51.556+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching spark://localhost:41019/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1719472489278
[2024-06-27T07:14:51.556+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: Fetching spark://localhost:41019/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp6579321867838908759.tmp
[2024-06-27T07:14:51.559+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp6579321867838908759.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.slf4j_slf4j-api-1.7.32.jar
[2024-06-27T07:14:51.579+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.slf4j_slf4j-api-1.7.32.jar to class loader default
[2024-06-27T07:14:51.579+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Executor: Fetching spark://localhost:41019/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:51.580+0000] {docker.py:436} INFO - 24/06/27 07:14:51 INFO Utils: Fetching spark://localhost:41019/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp9856960193779271964.tmp
[2024-06-27T07:14:52.040+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp9856960193779271964.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
[2024-06-27T07:14:52.051+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to class loader default
[2024-06-27T07:14:52.051+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1719472489278
[2024-06-27T07:14:52.052+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp9839088472846693059.tmp
[2024-06-27T07:14:52.078+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp9839088472846693059.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.xerial.snappy_snappy-java-1.1.8.4.jar
[2024-06-27T07:14:52.084+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader default
[2024-06-27T07:14:52.085+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.postgresql_postgresql-42.2.2.jar with timestamp 1719472489278
[2024-06-27T07:14:52.088+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.postgresql_postgresql-42.2.2.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp14669595136007130845.tmp
[2024-06-27T07:14:52.099+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp14669595136007130845.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.postgresql_postgresql-42.2.2.jar
[2024-06-27T07:14:52.108+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.postgresql_postgresql-42.2.2.jar to class loader default
[2024-06-27T07:14:52.109+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719472489278
[2024-06-27T07:14:52.109+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp14555254435816622680.tmp
[2024-06-27T07:14:52.124+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp14555254435816622680.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-27T07:14:52.134+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1719472489278
24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp13323702052909940186.tmp
[2024-06-27T07:14:52.138+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp13323702052909940186.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.spark-project.spark_unused-1.0.0.jar
[2024-06-27T07:14:52.142+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.spark-project.spark_unused-1.0.0.jar to class loader default
[2024-06-27T07:14:52.143+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1719472489278
[2024-06-27T07:14:52.145+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp1438020752405567650.tmp
[2024-06-27T07:14:52.303+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp1438020752405567650.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-api-3.3.2.jar
[2024-06-27T07:14:52.313+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.hadoop_hadoop-client-api-3.3.2.jar to class loader default
[2024-06-27T07:14:52.313+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719472489278
[2024-06-27T07:14:52.315+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp6438627011660007416.tmp
[2024-06-27T07:14:52.329+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp6438627011660007416.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.lz4_lz4-java-1.8.0.jar
[2024-06-27T07:14:52.336+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-27T07:14:52.337+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1719472489278
[2024-06-27T07:14:52.338+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp8752653268727611022.tmp
[2024-06-27T07:14:52.358+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp8752653268727611022.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.kafka_kafka-clients-2.8.1.jar
[2024-06-27T07:14:52.372+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.kafka_kafka-clients-2.8.1.jar to class loader default
[2024-06-27T07:14:52.372+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1719472489278
[2024-06-27T07:14:52.373+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp13639865569351999719.tmp
[2024-06-27T07:14:52.381+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp13639865569351999719.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
[2024-06-27T07:14:52.387+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to class loader default
[2024-06-27T07:14:52.387+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719472489278
[2024-06-27T07:14:52.388+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp14662512308850379982.tmp
[2024-06-27T07:14:52.406+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp14662512308850379982.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/commons-logging_commons-logging-1.1.3.jar
[2024-06-27T07:14:52.411+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/commons-logging_commons-logging-1.1.3.jar to class loader default
[2024-06-27T07:14:52.412+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Fetching spark://localhost:41019/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719472489278
[2024-06-27T07:14:52.415+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Fetching spark://localhost:41019/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp11933361974991181984.tmp
[2024-06-27T07:14:52.431+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/fetchFileTemp11933361974991181984.tmp has been previously copied to /tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-27T07:14:52.436+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Executor: Adding file:/tmp/spark-94350296-9ed7-40a7-95d7-a6e2a4fffa32/userFiles-1fc496f7-78e0-4057-a4b0-38ef5621ed93/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
[2024-06-27T07:14:52.505+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37687.
[2024-06-27T07:14:52.505+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO NettyBlockTransferService: Server created on localhost:37687
[2024-06-27T07:14:52.507+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-27T07:14:52.524+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 37687, None)
[2024-06-27T07:14:52.528+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO BlockManagerMasterEndpoint: Registering block manager localhost:37687 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 37687, None)
[2024-06-27T07:14:52.534+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 37687, None)
[2024-06-27T07:14:52.537+0000] {docker.py:436} INFO - 24/06/27 07:14:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 37687, None)
[2024-06-27T07:14:53.212+0000] {docker.py:436} INFO - 2024-06-27 07:14:53,209:create_spark_session:INFO:Spark session created successfully
[2024-06-27T07:14:53.221+0000] {docker.py:436} INFO - 24/06/27 07:14:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-27T07:14:53.225+0000] {docker.py:436} INFO - 24/06/27 07:14:53 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-27T07:14:55.550+0000] {docker.py:436} INFO - 2024-06-27 07:14:55,547:create_initial_dataframe:INFO:Initial dataframe created successfully:
[2024-06-27T07:14:55.798+0000] {docker.py:436} INFO - 24/06/27 07:14:55 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-27T07:14:55.834+0000] {docker.py:436} INFO - 24/06/27 07:14:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-27T07:14:55.877+0000] {docker.py:436} INFO - 24/06/27 07:14:55 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a resolved to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a.
24/06/27 07:14:55 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-27T07:14:55.969+0000] {docker.py:436} INFO - 24/06/27 07:14:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/metadata using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/.metadata.8dcb3aa4-557c-458a-b241-4825954ef121.tmp
[2024-06-27T07:14:56.110+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/.metadata.8dcb3aa4-557c-458a-b241-4825954ef121.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/metadata
[2024-06-27T07:14:56.157+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO MicroBatchExecution: Starting [id = dfb325bf-f699-4842-a1b7-b61e75930027, runId = 70448083-fb9f-4c88-b6d7-0434aedb98db]. Use file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a to store the query checkpoint.
[2024-06-27T07:14:56.171+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4799c328] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7ef37b2b]
[2024-06-27T07:14:56.227+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T07:14:56.229+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-27T07:14:56.230+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-27T07:14:56.235+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO MicroBatchExecution: Stream started from {}
[2024-06-27T07:14:56.780+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-27T07:14:56.906+0000] {docker.py:436} INFO - 24/06/27 07:14:56 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
[2024-06-27T07:14:56.906+0000] {docker.py:436} INFO - 24/06/27 07:14:56 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/06/27 07:14:56 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/06/27 07:14:56 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
[2024-06-27T07:14:56.907+0000] {docker.py:436} INFO - 24/06/27 07:14:56 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
[2024-06-27T07:14:56.910+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:14:56.910+0000] {docker.py:436} INFO - 24/06/27 07:14:56 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 07:14:56 INFO AppInfoParser: Kafka startTimeMs: 1719472496906
[2024-06-27T07:14:57.499+0000] {docker.py:436} INFO - 24/06/27 07:14:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/sources/0/0 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/sources/0/.0.254f58d2-44fe-47d0-a11c-37b4dbc61dfc.tmp
[2024-06-27T07:14:57.537+0000] {docker.py:436} INFO - 24/06/27 07:14:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/sources/0/.0.254f58d2-44fe-47d0-a11c-37b4dbc61dfc.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/sources/0/0
[2024-06-27T07:14:57.538+0000] {docker.py:436} INFO - 24/06/27 07:14:57 INFO KafkaMicroBatchStream: Initial offsets: {"store_source_data":{"0":8}}
[2024-06-27T07:14:57.568+0000] {docker.py:436} INFO - 24/06/27 07:14:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/0 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.0.23b089c1-eea1-4558-a4a7-da6a798b9557.tmp
[2024-06-27T07:14:57.615+0000] {docker.py:436} INFO - 24/06/27 07:14:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.0.23b089c1-eea1-4558-a4a7-da6a798b9557.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/0
[2024-06-27T07:14:57.616+0000] {docker.py:436} INFO - 24/06/27 07:14:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719472497552,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:14:58.088+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:58.163+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:58.227+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:58.231+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:58.296+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:58.301+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:14:58.726+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO CodeGenerator: Code generated in 255.522308 ms
[2024-06-27T07:14:58.840+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:14:58.851+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:14:58.876+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:14:58.877+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:14:58.879+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:14:58.881+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:14:58.886+0000] {docker.py:436} INFO - 24/06/27 07:14:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:14:59.053+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:14:59.094+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:14:59.097+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:14:59.105+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:14:59.128+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:14:59.130+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-27T07:14:59.210+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:14:59.228+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-06-27T07:14:59.405+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO CodeGenerator: Code generated in 34.457099 ms
[2024-06-27T07:14:59.693+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO CodeGenerator: Code generated in 148.837177 ms
[2024-06-27T07:14:59.792+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO CodeGenerator: Code generated in 50.941263 ms
[2024-06-27T07:14:59.836+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T07:14:59.909+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:14:59.912+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/06/27 07:14:59 INFO AppInfoParser: Kafka startTimeMs: 1719472499908
[2024-06-27T07:14:59.914+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T07:14:59.926+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 8 for partition store_source_data-0
[2024-06-27T07:14:59.937+0000] {docker.py:436} INFO - 24/06/27 07:14:59 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T07:15:00.008+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:15:00.510+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:15:00.511+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:15:00.512+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=28, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:15:00.658+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:15:00.661+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2024-06-27T07:15:00.693+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3402 bytes result sent to driver
[2024-06-27T07:15:00.714+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1533 ms on localhost (executor driver) (1/1)
[2024-06-27T07:15:00.716+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-27T07:15:00.729+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 1.822 s
[2024-06-27T07:15:00.740+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:15:00.741+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-06-27T07:15:00.749+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 1.896940 s
[2024-06-27T07:15:00.755+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:15:00.759+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 0
-------------------------------------------
[2024-06-27T07:15:00.857+0000] {docker.py:436} INFO - 24/06/27 07:15:00 INFO CodeGenerator: Code generated in 15.649829 ms
[2024-06-27T07:15:02.380+0000] {docker.py:436} INFO - 24/06/27 07:15:02 INFO CodeGenerator: Code generated in 16.303787 ms
[2024-06-27T07:15:02.397+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:15:02.397+0000] {docker.py:436} INFO - 24/06/27 07:15:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:15:02.413+0000] {docker.py:436} INFO - 24/06/27 07:15:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/0 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.0.4adef7dc-9dac-420d-829a-e969d9d4df38.tmp
[2024-06-27T07:15:02.455+0000] {docker.py:436} INFO - 24/06/27 07:15:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.0.4adef7dc-9dac-420d-829a-e969d9d4df38.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/0
[2024-06-27T07:15:02.495+0000] {docker.py:436} INFO - 24/06/27 07:15:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:14:56.216Z",
  "batchId" : 0,
  "numInputRows" : 20,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 3.205128205128205,
  "durationMs" : {
    "addBatch" : 4202,
    "commitOffsets" : 55,
    "getBatch" : 33,
    "latestOffset" : 1310,
    "queryPlanning" : 540,
    "triggerExecution" : 6239,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : null,
    "endOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "numInputRows" : 20,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 3.205128205128205,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 20
  }
}
[2024-06-27T07:15:07.191+0000] {docker.py:436} INFO - 24/06/27 07:15:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:15:12.477+0000] {docker.py:436} INFO - 24/06/27 07:15:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:22.486+0000] {docker.py:436} INFO - 24/06/27 07:15:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:32.496+0000] {docker.py:436} INFO - 24/06/27 07:15:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:42.496+0000] {docker.py:436} INFO - 24/06/27 07:15:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:15:52.498+0000] {docker.py:436} INFO - 24/06/27 07:15:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:02.508+0000] {docker.py:436} INFO - 24/06/27 07:16:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:12.515+0000] {docker.py:436} INFO - 24/06/27 07:16:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:22.521+0000] {docker.py:436} INFO - 24/06/27 07:16:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:32.522+0000] {docker.py:436} INFO - 24/06/27 07:16:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:42.528+0000] {docker.py:436} INFO - 24/06/27 07:16:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:16:52.537+0000] {docker.py:436} INFO - 24/06/27 07:16:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:02.542+0000] {docker.py:436} INFO - 24/06/27 07:17:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:12.547+0000] {docker.py:436} INFO - 24/06/27 07:17:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:22.555+0000] {docker.py:436} INFO - 24/06/27 07:17:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:32.565+0000] {docker.py:436} INFO - 24/06/27 07:17:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:42.565+0000] {docker.py:436} INFO - 24/06/27 07:17:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:17:52.572+0000] {docker.py:436} INFO - 24/06/27 07:17:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:02.573+0000] {docker.py:436} INFO - 24/06/27 07:18:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:12.576+0000] {docker.py:436} INFO - 24/06/27 07:18:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:19.983+0000] {docker.py:436} INFO - 24/06/27 07:18:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/1 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.1.795c6df6-7b1c-42ec-8f61-de6254a1ffc8.tmp
[2024-06-27T07:18:20.051+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.1.795c6df6-7b1c-42ec-8f61-de6254a1ffc8.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/1
24/06/27 07:18:20 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719472699958,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:18:20.131+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.132+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.165+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.166+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.227+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.233+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:18:20.331+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:18:20.331+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:18:20.357+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:18:20 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:18:20 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:18:20.365+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:18:20.366+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:18:20.391+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:18:20.397+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:18:20.405+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:18:20.406+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:18:20.432+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:18:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-27T07:18:20.452+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:18:20.462+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-27T07:18:20.571+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 28 for partition store_source_data-0
[2024-06-27T07:18:20.574+0000] {docker.py:436} INFO - 24/06/27 07:18:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:18:21.088+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:18:21.100+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=29, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:18:21 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:18:21 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2024-06-27T07:18:21.103+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2076 bytes result sent to driver
[2024-06-27T07:18:21.112+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 658 ms on localhost (executor driver) (1/1)
24/06/27 07:18:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-27T07:18:21.116+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.745 s
24/06/27 07:18:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:18:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-06-27T07:18:21.120+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.787662 s
24/06/27 07:18:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:18:21.121+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 1
[2024-06-27T07:18:21.121+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:18:21.203+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:18:21.210+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:18:21.309+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/1 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.1.a9c4a038-9ba8-4535-bf0d-26a2b1c85e93.tmp
[2024-06-27T07:18:21.472+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.1.a9c4a038-9ba8-4535-bf0d-26a2b1c85e93.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/1
[2024-06-27T07:18:21.481+0000] {docker.py:436} INFO - 24/06/27 07:18:21 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:18:19.955Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.6591957811470007,
  "durationMs" : {
    "addBatch" : 1076,
    "commitOffsets" : 255,
    "getBatch" : 1,
    "latestOffset" : 3,
    "queryPlanning" : 85,
    "triggerExecution" : 1517,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 28
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.6591957811470007,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:18:29.231+0000] {docker.py:436} INFO - 24/06/27 07:18:29 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:18:31.474+0000] {docker.py:436} INFO - 24/06/27 07:18:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:41.476+0000] {docker.py:436} INFO - 24/06/27 07:18:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:18:51.483+0000] {docker.py:436} INFO - 24/06/27 07:18:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:01.487+0000] {docker.py:436} INFO - 24/06/27 07:19:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:11.486+0000] {docker.py:436} INFO - 24/06/27 07:19:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:21.157+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/2 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.2.547b7f57-e4d8-4fcd-91fa-4302e1bc054d.tmp
[2024-06-27T07:19:21.214+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.2.547b7f57-e4d8-4fcd-91fa-4302e1bc054d.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/2
[2024-06-27T07:19:21.215+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719472761121,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:19:21.273+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.276+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.314+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.324+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.362+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.366+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:19:21.400+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:19:21.401+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:19:21.402+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:19:21.403+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:19:21 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:19:21 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:19:21.404+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:19:21.417+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:19:21.424+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:19:21.429+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:19:21.432+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:19:21.433+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:19:21.433+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-06-27T07:19:21.435+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:19:21.438+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2024-06-27T07:19:21.504+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 29 for partition store_source_data-0
[2024-06-27T07:19:21.510+0000] {docker.py:436} INFO - 24/06/27 07:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:19:22.018+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:19:22.019+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:19:22.021+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=30, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:19:22.021+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:19:22 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2024-06-27T07:19:22.034+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2162 bytes result sent to driver
[2024-06-27T07:19:22.038+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 605 ms on localhost (executor driver) (1/1)
[2024-06-27T07:19:22.041+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.635 s
[2024-06-27T07:19:22.043+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:19:22.044+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-27T07:19:22.045+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-06-27T07:19:22.046+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.644659 s
[2024-06-27T07:19:22.047+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:19:22.047+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 2
-------------------------------------------
[2024-06-27T07:19:22.092+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:19:22.093+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:19:22.121+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/2 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.2.35e5db82-ac99-4bfc-9aeb-50f2a85a7296.tmp
[2024-06-27T07:19:22.356+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.2.35e5db82-ac99-4bfc-9aeb-50f2a85a7296.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/2
[2024-06-27T07:19:22.370+0000] {docker.py:436} INFO - 24/06/27 07:19:22 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:19:21.113Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.8045052292839903,
  "durationMs" : {
    "addBatch" : 809,
    "commitOffsets" : 262,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 63,
    "triggerExecution" : 1243,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 29
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.8045052292839903,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:19:32.375+0000] {docker.py:436} INFO - 24/06/27 07:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:33.920+0000] {docker.py:436} INFO - 24/06/27 07:19:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:19:42.382+0000] {docker.py:436} INFO - 24/06/27 07:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:19:52.389+0000] {docker.py:436} INFO - 24/06/27 07:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:02.398+0000] {docker.py:436} INFO - 24/06/27 07:20:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:12.405+0000] {docker.py:436} INFO - 24/06/27 07:20:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:22.407+0000] {docker.py:436} INFO - 24/06/27 07:20:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:32.414+0000] {docker.py:436} INFO - 24/06/27 07:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:42.424+0000] {docker.py:436} INFO - 24/06/27 07:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:20:52.432+0000] {docker.py:436} INFO - 24/06/27 07:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:02.442+0000] {docker.py:436} INFO - 24/06/27 07:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:08.894+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/3 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.3.bba701fb-6ed1-4a8a-b939-39800c1e3216.tmp
[2024-06-27T07:21:08.968+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.3.bba701fb-6ed1-4a8a-b939-39800c1e3216.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/3
[2024-06-27T07:21:08.969+0000] {docker.py:436} INFO - 24/06/27 07:21:08 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719472868875,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:21:09.028+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.049+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.103+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.107+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.126+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.134+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:21:09.167+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:21:09.175+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:21:09.188+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:21:09.189+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:21:09 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:21:09.190+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:21:09.190+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:21:09.196+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:21:09.211+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:21:09.212+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:21:09.228+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:21:09.259+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:21:09.263+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2024-06-27T07:21:09.271+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:21:09.308+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2024-06-27T07:21:09.390+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 30 for partition store_source_data-0
[2024-06-27T07:21:09.395+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:21:09.897+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:21:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:21:09.898+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=31, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:21:09.898+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:21:09.902+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2024-06-27T07:21:09.933+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2162 bytes result sent to driver
[2024-06-27T07:21:09.964+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 690 ms on localhost (executor driver) (1/1)
[2024-06-27T07:21:09.975+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-06-27T07:21:09.975+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 0.782 s
[2024-06-27T07:21:09.976+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:21:09.976+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-06-27T07:21:09.993+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 0.810022 s
[2024-06-27T07:21:09.997+0000] {docker.py:436} INFO - 24/06/27 07:21:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:21:09.997+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 3
-------------------------------------------
[2024-06-27T07:21:10.107+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:21:10.108+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:21:10.177+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/3 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.3.c710f552-ef8d-4cb5-9231-ea664ed9c676.tmp
[2024-06-27T07:21:10.235+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.3.c710f552-ef8d-4cb5-9231-ea664ed9c676.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/3
[2024-06-27T07:21:10.238+0000] {docker.py:436} INFO - 24/06/27 07:21:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:21:08.874Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7342143906020557,
  "durationMs" : {
    "addBatch" : 1051,
    "commitOffsets" : 127,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 88,
    "triggerExecution" : 1362,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 30
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7342143906020557,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:21:20.241+0000] {docker.py:436} INFO - 24/06/27 07:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:21.367+0000] {docker.py:436} INFO - 24/06/27 07:21:21 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:21:30.247+0000] {docker.py:436} INFO - 24/06/27 07:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:40.249+0000] {docker.py:436} INFO - 24/06/27 07:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:21:50.255+0000] {docker.py:436} INFO - 24/06/27 07:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:00.257+0000] {docker.py:436} INFO - 24/06/27 07:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:09.723+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/4 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.4.30250214-eddf-4de9-8ffb-cae1f7146aff.tmp
[2024-06-27T07:22:09.959+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.4.30250214-eddf-4de9-8ffb-cae1f7146aff.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/4
24/06/27 07:22:09 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719472929660,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:22:10.003+0000] {docker.py:436} INFO - 24/06/27 07:22:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.014+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.122+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.136+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.186+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.192+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:22:10.289+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:22:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:22:10.290+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:22:10.290+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:22:10 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:22:10.301+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO DAGScheduler: Missing parents: List()
24/06/27 07:22:10 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:22:10.341+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:22:10.419+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:22:10.420+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:22:10.434+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/27 07:22:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:22:10.442+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/27 07:22:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:22:10.444+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2024-06-27T07:22:10.766+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 31 for partition store_source_data-0
[2024-06-27T07:22:10.773+0000] {docker.py:436} INFO - 24/06/27 07:22:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:22:11.275+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:22:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:22:11.276+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=32, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:22:11.278+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:22:11.280+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2024-06-27T07:22:11.319+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2162 bytes result sent to driver
[2024-06-27T07:22:11.323+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 882 ms on localhost (executor driver) (1/1)
[2024-06-27T07:22:11.324+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-06-27T07:22:11.333+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 1.027 s
[2024-06-27T07:22:11.335+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:22:11.349+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-06-27T07:22:11.350+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 1.069553 s
24/06/27 07:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:22:11.351+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 4
-------------------------------------------
[2024-06-27T07:22:11.421+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:22:11.422+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:22:11.439+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/4 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.4.cc0c4ce4-e0c5-423f-a6cc-a5c2219f0cba.tmp
[2024-06-27T07:22:11.518+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.4.cc0c4ce4-e0c5-423f-a6cc-a5c2219f0cba.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/4
[2024-06-27T07:22:11.522+0000] {docker.py:436} INFO - 24/06/27 07:22:11 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:22:09.644Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 50.0,
  "processedRowsPerSecond" : 0.5339028296849974,
  "durationMs" : {
    "addBatch" : 1398,
    "commitOffsets" : 94,
    "getBatch" : 0,
    "latestOffset" : 16,
    "queryPlanning" : 55,
    "triggerExecution" : 1873,
    "walCommit" : 299
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 31
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 50.0,
    "processedRowsPerSecond" : 0.5339028296849974,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:22:21.520+0000] {docker.py:436} INFO - 24/06/27 07:22:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:22.571+0000] {docker.py:436} INFO - 24/06/27 07:22:22 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:22:31.519+0000] {docker.py:436} INFO - 24/06/27 07:22:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:41.525+0000] {docker.py:436} INFO - 24/06/27 07:22:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:22:51.528+0000] {docker.py:436} INFO - 24/06/27 07:22:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:01.536+0000] {docker.py:436} INFO - 24/06/27 07:23:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:06.333+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/5 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.5.adadc1db-33fb-4c68-ac83-22d7a641848d.tmp
[2024-06-27T07:23:06.432+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.5.adadc1db-33fb-4c68-ac83-22d7a641848d.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/5
[2024-06-27T07:23:06.433+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1719472986292,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:23:06.478+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.499+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.534+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.539+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.573+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.587+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:06.629+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:23:06.631+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:23:06.637+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:23:06.638+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:23:06.638+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:23:06.639+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:23:06.639+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:23:06.640+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:23:06.658+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:23:06.659+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:23:06.659+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:23:06.661+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:23:06.662+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2024-06-27T07:23:06.669+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes) 
24/06/27 07:23:06 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2024-06-27T07:23:06.687+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 32 for partition store_source_data-0
[2024-06-27T07:23:06.691+0000] {docker.py:436} INFO - 24/06/27 07:23:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:23:07.196+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:23:07.196+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=33, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:23:07.197+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:23:07.198+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2024-06-27T07:23:07.210+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2162 bytes result sent to driver
[2024-06-27T07:23:07.215+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 549 ms on localhost (executor driver) (1/1)
[2024-06-27T07:23:07.216+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.578 s
[2024-06-27T07:23:07.216+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:23:07.222+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-06-27T07:23:07.232+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/27 07:23:07 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.592038 s
24/06/27 07:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:23:07.233+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 5
-------------------------------------------
[2024-06-27T07:23:07.303+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:23:07.304+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:23:07.345+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/5 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.5.aa84df54-5c8a-47ef-a9b1-6a9785d2bf06.tmp
[2024-06-27T07:23:07.415+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.5.aa84df54-5c8a-47ef-a9b1-6a9785d2bf06.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/5
[2024-06-27T07:23:07.417+0000] {docker.py:436} INFO - 24/06/27 07:23:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:23:06.282Z",
  "batchId" : 5,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 52.631578947368425,
  "processedRowsPerSecond" : 0.88339222614841,
  "durationMs" : {
    "addBatch" : 807,
    "commitOffsets" : 109,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 62,
    "triggerExecution" : 1132,
    "walCommit" : 142
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 32
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 52.631578947368425,
    "processedRowsPerSecond" : 0.88339222614841,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:23:17.424+0000] {docker.py:436} INFO - 24/06/27 07:23:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:18.978+0000] {docker.py:436} INFO - 24/06/27 07:23:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:23:27.434+0000] {docker.py:436} INFO - 24/06/27 07:23:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:37.454+0000] {docker.py:436} INFO - 24/06/27 07:23:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:47.456+0000] {docker.py:436} INFO - 24/06/27 07:23:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:57.478+0000] {docker.py:436} INFO - 24/06/27 07:23:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:23:59.636+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/6 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.6.e186b1a7-b1c0-4e16-a597-752305ae9df8.tmp
[2024-06-27T07:23:59.733+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.6.e186b1a7-b1c0-4e16-a597-752305ae9df8.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/6
24/06/27 07:23:59 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1719473039577,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:23:59.784+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.798+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.857+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.874+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.894+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:23:59.896+0000] {docker.py:436} INFO - 24/06/27 07:23:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:24:00.013+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:24:00.015+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:24:00.017+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:24:00.021+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:24:00.022+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:24:00.024+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:24:00.025+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:24:00.031+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:24:00.076+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:24:00.077+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:24:00.078+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:24:00.079+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:24:00.080+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2024-06-27T07:24:00.082+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:24:00.083+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2024-06-27T07:24:00.269+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 33 for partition store_source_data-0
[2024-06-27T07:24:00.313+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:24:00.781+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:24:00.783+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=34, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:24:00.793+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:24:00 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2024-06-27T07:24:00.857+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2162 bytes result sent to driver
[2024-06-27T07:24:00.880+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 789 ms on localhost (executor driver) (1/1)
24/06/27 07:24:00 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/27 07:24:00 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.848 s
[2024-06-27T07:24:00.881+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:24:00.892+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-06-27T07:24:00.893+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.876275 s
[2024-06-27T07:24:00.893+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:24:00.894+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:24:00.894+0000] {docker.py:436} INFO - Batch: 6
[2024-06-27T07:24:00.895+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:24:00.947+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:24:00.947+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:24:00.991+0000] {docker.py:436} INFO - 24/06/27 07:24:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/6 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.6.2b281ff4-29d6-487d-91e5-e0182972c6b1.tmp
[2024-06-27T07:24:01.154+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.6.2b281ff4-29d6-487d-91e5-e0182972c6b1.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/6
[2024-06-27T07:24:01.159+0000] {docker.py:436} INFO - 24/06/27 07:24:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:23:59.556Z",
  "batchId" : 6,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 41.666666666666664,
  "processedRowsPerSecond" : 0.6257822277847309,
  "durationMs" : {
    "addBatch" : 1135,
    "commitOffsets" : 206,
    "getBatch" : 0,
    "latestOffset" : 21,
    "queryPlanning" : 78,
    "triggerExecution" : 1598,
    "walCommit" : 155
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 33
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 41.666666666666664,
    "processedRowsPerSecond" : 0.6257822277847309,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:24:11.167+0000] {docker.py:436} INFO - 24/06/27 07:24:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:12.381+0000] {docker.py:436} INFO - 24/06/27 07:24:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:24:21.171+0000] {docker.py:436} INFO - 24/06/27 07:24:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:31.172+0000] {docker.py:436} INFO - 24/06/27 07:24:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:41.176+0000] {docker.py:436} INFO - 24/06/27 07:24:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:24:51.185+0000] {docker.py:436} INFO - 24/06/27 07:24:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:01.193+0000] {docker.py:436} INFO - 24/06/27 07:25:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:11.202+0000] {docker.py:436} INFO - 24/06/27 07:25:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:21.204+0000] {docker.py:436} INFO - 24/06/27 07:25:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:31.205+0000] {docker.py:436} INFO - 24/06/27 07:25:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:32.086+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/7 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.7.6264a793-f5f3-4ae3-9a0a-e9fc98f2ffdd.tmp
[2024-06-27T07:25:32.190+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.7.6264a793-f5f3-4ae3-9a0a-e9fc98f2ffdd.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/7
[2024-06-27T07:25:32.191+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1719473132068,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:25:32.219+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.243+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.273+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.277+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.297+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.315+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:25:32.352+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:25:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:25:32.353+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:25:32.354+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:25:32.356+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:25:32.357+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:25:32.359+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:25:32.363+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:25:32.395+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:25:32.397+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:25:32.404+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:25:32.439+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:25:32.440+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2024-06-27T07:25:32.442+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:25:32.447+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2024-06-27T07:25:32.496+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 34 for partition store_source_data-0
[2024-06-27T07:25:32.497+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:25:32.994+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:25:32.997+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=35, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:25:32.998+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:25:32.998+0000] {docker.py:436} INFO - 24/06/27 07:25:32 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2024-06-27T07:25:33.013+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2162 bytes result sent to driver
[2024-06-27T07:25:33.014+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 574 ms on localhost (executor driver) (1/1)
[2024-06-27T07:25:33.015+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-06-27T07:25:33.019+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.654 s
24/06/27 07:25:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:25:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2024-06-27T07:25:33.020+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.670008 s
[2024-06-27T07:25:33.021+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:25:33.021+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:25:33.022+0000] {docker.py:436} INFO - Batch: 7
[2024-06-27T07:25:33.022+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:25:33.086+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:25:33.087+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:25:33.114+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/7 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.7.ae45a45e-4f6a-48b9-83bd-bc37b7a9941b.tmp
[2024-06-27T07:25:33.220+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.7.ae45a45e-4f6a-48b9-83bd-bc37b7a9941b.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/7
[2024-06-27T07:25:33.223+0000] {docker.py:436} INFO - 24/06/27 07:25:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:25:32.065Z",
  "batchId" : 7,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8658008658008658,
  "durationMs" : {
    "addBatch" : 827,
    "commitOffsets" : 138,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 61,
    "triggerExecution" : 1155,
    "walCommit" : 124
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 34
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8658008658008658,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:25:43.227+0000] {docker.py:436} INFO - 24/06/27 07:25:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:25:44.451+0000] {docker.py:436} INFO - 24/06/27 07:25:44 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:25:53.231+0000] {docker.py:436} INFO - 24/06/27 07:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:03.241+0000] {docker.py:436} INFO - 24/06/27 07:26:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:13.240+0000] {docker.py:436} INFO - 24/06/27 07:26:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:23.244+0000] {docker.py:436} INFO - 24/06/27 07:26:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:33.245+0000] {docker.py:436} INFO - 24/06/27 07:26:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:43.245+0000] {docker.py:436} INFO - 24/06/27 07:26:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:26:53.249+0000] {docker.py:436} INFO - 24/06/27 07:26:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:03.251+0000] {docker.py:436} INFO - 24/06/27 07:27:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:06.379+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/8 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.8.40df58f8-aff8-4c2e-a43a-02615760a186.tmp
[2024-06-27T07:27:06.427+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.8.40df58f8-aff8-4c2e-a43a-02615760a186.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/8
[2024-06-27T07:27:06.428+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1719473226360,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:27:06.457+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.461+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.494+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.497+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.528+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.530+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:27:06.570+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:27:06.575+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:27:06.579+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:27:06.579+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:27:06.580+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:27:06.581+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Missing parents: List()
24/06/27 07:27:06 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:27:06.585+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:27:06.603+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:27:06.604+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:27:06.606+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:27:06.607+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:27:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2024-06-27T07:27:06.609+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:27:06.628+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2024-06-27T07:27:06.679+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 35 for partition store_source_data-0
[2024-06-27T07:27:06.704+0000] {docker.py:436} INFO - 24/06/27 07:27:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:27:07.197+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:27:07.200+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:27:07.207+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:27:07.208+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2024-06-27T07:27:07.228+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2162 bytes result sent to driver
[2024-06-27T07:27:07.247+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 630 ms on localhost (executor driver) (1/1)
[2024-06-27T07:27:07.256+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-06-27T07:27:07.257+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.667 s
[2024-06-27T07:27:07.257+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:27:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2024-06-27T07:27:07.259+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.683572 s
[2024-06-27T07:27:07.259+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:27:07.260+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 8
-------------------------------------------
[2024-06-27T07:27:07.392+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:27:07.393+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:27:07.449+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/8 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.8.e75cb8a2-d04b-4399-8b42-eacce34df006.tmp
[2024-06-27T07:27:07.656+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.8.e75cb8a2-d04b-4399-8b42-eacce34df006.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/8
[2024-06-27T07:27:07.662+0000] {docker.py:436} INFO - 24/06/27 07:27:07 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:27:06.358Z",
  "batchId" : 8,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7704160246533127,
  "durationMs" : {
    "addBatch" : 930,
    "commitOffsets" : 257,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 40,
    "triggerExecution" : 1298,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 35
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7704160246533127,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:27:17.670+0000] {docker.py:436} INFO - 24/06/27 07:27:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:19.011+0000] {docker.py:436} INFO - 24/06/27 07:27:19 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:27:27.673+0000] {docker.py:436} INFO - 24/06/27 07:27:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:37.684+0000] {docker.py:436} INFO - 24/06/27 07:27:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:47.686+0000] {docker.py:436} INFO - 24/06/27 07:27:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:27:57.689+0000] {docker.py:436} INFO - 24/06/27 07:27:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:07.691+0000] {docker.py:436} INFO - 24/06/27 07:28:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:17.699+0000] {docker.py:436} INFO - 24/06/27 07:28:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:27.699+0000] {docker.py:436} INFO - 24/06/27 07:28:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:37.704+0000] {docker.py:436} INFO - 24/06/27 07:28:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:47.713+0000] {docker.py:436} INFO - 24/06/27 07:28:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:28:57.724+0000] {docker.py:436} INFO - 24/06/27 07:28:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:07.735+0000] {docker.py:436} INFO - 24/06/27 07:29:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:17.738+0000] {docker.py:436} INFO - 24/06/27 07:29:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:27.748+0000] {docker.py:436} INFO - 24/06/27 07:29:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:37.753+0000] {docker.py:436} INFO - 24/06/27 07:29:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:47.758+0000] {docker.py:436} INFO - 24/06/27 07:29:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:29:57.759+0000] {docker.py:436} INFO - 24/06/27 07:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:07.764+0000] {docker.py:436} INFO - 24/06/27 07:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:17.765+0000] {docker.py:436} INFO - 24/06/27 07:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:27.769+0000] {docker.py:436} INFO - 24/06/27 07:30:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:37.781+0000] {docker.py:436} INFO - 24/06/27 07:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:47.789+0000] {docker.py:436} INFO - 24/06/27 07:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:30:57.791+0000] {docker.py:436} INFO - 24/06/27 07:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:07.796+0000] {docker.py:436} INFO - 24/06/27 07:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:17.797+0000] {docker.py:436} INFO - 24/06/27 07:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:27.804+0000] {docker.py:436} INFO - 24/06/27 07:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:37.810+0000] {docker.py:436} INFO - 24/06/27 07:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:47.813+0000] {docker.py:436} INFO - 24/06/27 07:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:31:57.823+0000] {docker.py:436} INFO - 24/06/27 07:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:07.833+0000] {docker.py:436} INFO - 24/06/27 07:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:17.837+0000] {docker.py:436} INFO - 24/06/27 07:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:27.842+0000] {docker.py:436} INFO - 24/06/27 07:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:35.479+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/9 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.9.8c55b29b-89ac-4041-8981-8eaeb5f1f5ba.tmp
[2024-06-27T07:32:35.536+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.9.8c55b29b-89ac-4041-8981-8eaeb5f1f5ba.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/9
[2024-06-27T07:32:35.536+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1719473555462,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:32:35.555+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.558+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.571+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.577+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.591+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.598+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:32:35.616+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:32:35.618+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:32:35.622+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:32:35.625+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:32:35.626+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:32:35.627+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:32:35.627+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:32:35.630+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:32:35.649+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:32:35.653+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:32:35.657+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:32:35.662+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:32:35.663+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-06-27T07:32:35.664+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:32:35.669+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2024-06-27T07:32:35.708+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 36 for partition store_source_data-0
[2024-06-27T07:32:35.714+0000] {docker.py:436} INFO - 24/06/27 07:32:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:32:36.216+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:32:36.216+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:32:36.217+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=37, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:32:36.217+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:32:36.218+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2024-06-27T07:32:36.223+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2162 bytes result sent to driver
[2024-06-27T07:32:36.224+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 560 ms on localhost (executor driver) (1/1)
[2024-06-27T07:32:36.225+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-06-27T07:32:36.226+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.598 s
24/06/27 07:32:36 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:32:36.226+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-06-27T07:32:36.227+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.607420 s
[2024-06-27T07:32:36.227+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:32:36.228+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:32:36.228+0000] {docker.py:436} INFO - Batch: 9
[2024-06-27T07:32:36.229+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:32:36.255+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:32:36.256+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:32:36.282+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/9 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.9.3611d584-7421-4877-8bef-a45d7b34d13c.tmp
[2024-06-27T07:32:36.330+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.9.3611d584-7421-4877-8bef-a45d7b34d13c.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/9
[2024-06-27T07:32:36.332+0000] {docker.py:436} INFO - 24/06/27 07:32:36 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:32:35.460Z",
  "batchId" : 9,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.1494252873563218,
  "durationMs" : {
    "addBatch" : 696,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 24,
    "triggerExecution" : 870,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 36
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.1494252873563218,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:32:46.334+0000] {docker.py:436} INFO - 24/06/27 07:32:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:32:47.938+0000] {docker.py:436} INFO - 24/06/27 07:32:47 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:32:56.341+0000] {docker.py:436} INFO - 24/06/27 07:32:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:06.351+0000] {docker.py:436} INFO - 24/06/27 07:33:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:16.356+0000] {docker.py:436} INFO - 24/06/27 07:33:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:26.363+0000] {docker.py:436} INFO - 24/06/27 07:33:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:36.366+0000] {docker.py:436} INFO - 24/06/27 07:33:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:46.366+0000] {docker.py:436} INFO - 24/06/27 07:33:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:33:56.369+0000] {docker.py:436} INFO - 24/06/27 07:33:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:06.377+0000] {docker.py:436} INFO - 24/06/27 07:34:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:16.386+0000] {docker.py:436} INFO - 24/06/27 07:34:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:26.393+0000] {docker.py:436} INFO - 24/06/27 07:34:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:36.402+0000] {docker.py:436} INFO - 24/06/27 07:34:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:46.403+0000] {docker.py:436} INFO - 24/06/27 07:34:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:34:56.411+0000] {docker.py:436} INFO - 24/06/27 07:34:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:06.411+0000] {docker.py:436} INFO - 24/06/27 07:35:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:16.417+0000] {docker.py:436} INFO - 24/06/27 07:35:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:26.428+0000] {docker.py:436} INFO - 24/06/27 07:35:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:36.438+0000] {docker.py:436} INFO - 24/06/27 07:35:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:46.440+0000] {docker.py:436} INFO - 24/06/27 07:35:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:35:56.445+0000] {docker.py:436} INFO - 24/06/27 07:35:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:06.448+0000] {docker.py:436} INFO - 24/06/27 07:36:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:16.453+0000] {docker.py:436} INFO - 24/06/27 07:36:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:26.460+0000] {docker.py:436} INFO - 24/06/27 07:36:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:36.468+0000] {docker.py:436} INFO - 24/06/27 07:36:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:46.474+0000] {docker.py:436} INFO - 24/06/27 07:36:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:36:56.480+0000] {docker.py:436} INFO - 24/06/27 07:36:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:06.486+0000] {docker.py:436} INFO - 24/06/27 07:37:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:16.489+0000] {docker.py:436} INFO - 24/06/27 07:37:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:26.490+0000] {docker.py:436} INFO - 24/06/27 07:37:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:36.498+0000] {docker.py:436} INFO - 24/06/27 07:37:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:46.498+0000] {docker.py:436} INFO - 24/06/27 07:37:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:56.499+0000] {docker.py:436} INFO - 24/06/27 07:37:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:37:59.458+0000] {docker.py:436} INFO - 24/06/27 07:37:59 INFO Metrics: Metrics scheduler closed
[2024-06-27T07:37:59.459+0000] {docker.py:436} INFO - 24/06/27 07:37:59 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T07:37:59.459+0000] {docker.py:436} INFO - 24/06/27 07:37:59 INFO Metrics: Metrics reporters closed
[2024-06-27T07:37:59.464+0000] {docker.py:436} INFO - 24/06/27 07:37:59 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-1 unregistered
[2024-06-27T07:38:06.506+0000] {docker.py:436} INFO - 24/06/27 07:38:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:16.508+0000] {docker.py:436} INFO - 24/06/27 07:38:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:26.516+0000] {docker.py:436} INFO - 24/06/27 07:38:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:36.525+0000] {docker.py:436} INFO - 24/06/27 07:38:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:46.527+0000] {docker.py:436} INFO - 24/06/27 07:38:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:38:56.529+0000] {docker.py:436} INFO - 24/06/27 07:38:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:06.532+0000] {docker.py:436} INFO - 24/06/27 07:39:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:16.537+0000] {docker.py:436} INFO - 24/06/27 07:39:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:26.543+0000] {docker.py:436} INFO - 24/06/27 07:39:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:36.554+0000] {docker.py:436} INFO - 24/06/27 07:39:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:46.553+0000] {docker.py:436} INFO - 24/06/27 07:39:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:39:56.558+0000] {docker.py:436} INFO - 24/06/27 07:39:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:06.563+0000] {docker.py:436} INFO - 24/06/27 07:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:16.570+0000] {docker.py:436} INFO - 24/06/27 07:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:26.579+0000] {docker.py:436} INFO - 24/06/27 07:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:36.584+0000] {docker.py:436} INFO - 24/06/27 07:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:46.592+0000] {docker.py:436} INFO - 24/06/27 07:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:40:56.599+0000] {docker.py:436} INFO - 24/06/27 07:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:06.605+0000] {docker.py:436} INFO - 24/06/27 07:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:16.609+0000] {docker.py:436} INFO - 24/06/27 07:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:26.615+0000] {docker.py:436} INFO - 24/06/27 07:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:36.626+0000] {docker.py:436} INFO - 24/06/27 07:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:46.627+0000] {docker.py:436} INFO - 24/06/27 07:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:41:56.632+0000] {docker.py:436} INFO - 24/06/27 07:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:06.638+0000] {docker.py:436} INFO - 24/06/27 07:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:16.643+0000] {docker.py:436} INFO - 24/06/27 07:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:26.650+0000] {docker.py:436} INFO - 24/06/27 07:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:36.651+0000] {docker.py:436} INFO - 24/06/27 07:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:46.661+0000] {docker.py:436} INFO - 24/06/27 07:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:42:56.666+0000] {docker.py:436} INFO - 24/06/27 07:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:06.675+0000] {docker.py:436} INFO - 24/06/27 07:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:16.680+0000] {docker.py:436} INFO - 24/06/27 07:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:26.687+0000] {docker.py:436} INFO - 24/06/27 07:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:36.688+0000] {docker.py:436} INFO - 24/06/27 07:43:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:46.696+0000] {docker.py:436} INFO - 24/06/27 07:43:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:43:56.700+0000] {docker.py:436} INFO - 24/06/27 07:43:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:06.710+0000] {docker.py:436} INFO - 24/06/27 07:44:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:16.710+0000] {docker.py:436} INFO - 24/06/27 07:44:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:26.715+0000] {docker.py:436} INFO - 24/06/27 07:44:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:36.720+0000] {docker.py:436} INFO - 24/06/27 07:44:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:46.729+0000] {docker.py:436} INFO - 24/06/27 07:44:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:44:56.729+0000] {docker.py:436} INFO - 24/06/27 07:44:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:06.733+0000] {docker.py:436} INFO - 24/06/27 07:45:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:16.739+0000] {docker.py:436} INFO - 24/06/27 07:45:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:26.745+0000] {docker.py:436} INFO - 24/06/27 07:45:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:36.752+0000] {docker.py:436} INFO - 24/06/27 07:45:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:46.760+0000] {docker.py:436} INFO - 24/06/27 07:45:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:45:56.766+0000] {docker.py:436} INFO - 24/06/27 07:45:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:06.769+0000] {docker.py:436} INFO - 24/06/27 07:46:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:16.777+0000] {docker.py:436} INFO - 24/06/27 07:46:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:26.784+0000] {docker.py:436} INFO - 24/06/27 07:46:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:36.786+0000] {docker.py:436} INFO - 24/06/27 07:46:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:46.786+0000] {docker.py:436} INFO - 24/06/27 07:46:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:46:56.789+0000] {docker.py:436} INFO - 24/06/27 07:46:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:06.792+0000] {docker.py:436} INFO - 24/06/27 07:47:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:16.798+0000] {docker.py:436} INFO - 24/06/27 07:47:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:26.798+0000] {docker.py:436} INFO - 24/06/27 07:47:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:36.807+0000] {docker.py:436} INFO - 24/06/27 07:47:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:46.814+0000] {docker.py:436} INFO - 24/06/27 07:47:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:56.836+0000] {docker.py:436} INFO - 24/06/27 07:47:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:47:59.902+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/10 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.10.95cf2fd5-ca9e-4400-9b95-6d6cb03451d3.tmp
[2024-06-27T07:47:59.949+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.10.95cf2fd5-ca9e-4400-9b95-6d6cb03451d3.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/10
[2024-06-27T07:47:59.951+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1719474479885,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:47:59.968+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.973+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.987+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:47:59.990+0000] {docker.py:436} INFO - 24/06/27 07:47:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.010+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:48:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:48:00.030+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:48:00.032+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:48:00.037+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:48:00.037+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:48:00.038+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:48:00.038+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:48:00.039+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:48:00.039+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:48:00.059+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:48:00.063+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:48:00.066+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:48:00.077+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:48:00.082+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-06-27T07:48:00.088+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:48:00.091+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2024-06-27T07:48:00.115+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T07:48:00.127+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T07:48:00.129+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T07:48:00.130+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO AppInfoParser: Kafka startTimeMs: 1719474480127
[2024-06-27T07:48:00.132+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T07:48:00.134+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 37 for partition store_source_data-0
[2024-06-27T07:48:00.145+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T07:48:00.158+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:48:00.667+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:48:00.669+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:48:00.670+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:48:00.670+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:48:00.671+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2024-06-27T07:48:00.674+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2076 bytes result sent to driver
24/06/27 07:48:00 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 589 ms on localhost (executor driver) (1/1)
[2024-06-27T07:48:00.675+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-06-27T07:48:00.675+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.638 s
[2024-06-27T07:48:00.678+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:48:00.678+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-06-27T07:48:00.679+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.646653 s
[2024-06-27T07:48:00.679+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:48:00.680+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:48:00.680+0000] {docker.py:436} INFO - Batch: 10
[2024-06-27T07:48:00.681+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:48:00.773+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:48:00.775+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:48:00.853+0000] {docker.py:436} INFO - 24/06/27 07:48:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/10 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.10.8256a9f5-7f63-4b71-bd1b-602e2fd2855c.tmp
[2024-06-27T07:48:01.038+0000] {docker.py:436} INFO - 24/06/27 07:48:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.10.8256a9f5-7f63-4b71-bd1b-602e2fd2855c.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/10
24/06/27 07:48:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:47:59.884Z",
  "batchId" : 10,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8741258741258742,
  "durationMs" : {
    "addBatch" : 793,
    "commitOffsets" : 261,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 23,
    "triggerExecution" : 1144,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 37
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8741258741258742,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:48:11.040+0000] {docker.py:436} INFO - 24/06/27 07:48:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:16.034+0000] {docker.py:436} INFO - 24/06/27 07:48:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:48:21.046+0000] {docker.py:436} INFO - 24/06/27 07:48:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:31.052+0000] {docker.py:436} INFO - 24/06/27 07:48:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:41.055+0000] {docker.py:436} INFO - 24/06/27 07:48:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:48:51.065+0000] {docker.py:436} INFO - 24/06/27 07:48:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:01.070+0000] {docker.py:436} INFO - 24/06/27 07:49:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:11.073+0000] {docker.py:436} INFO - 24/06/27 07:49:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:21.077+0000] {docker.py:436} INFO - 24/06/27 07:49:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:31.086+0000] {docker.py:436} INFO - 24/06/27 07:49:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:41.094+0000] {docker.py:436} INFO - 24/06/27 07:49:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:51.095+0000] {docker.py:436} INFO - 24/06/27 07:49:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:49:59.757+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/11 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.11.58b09b04-39b4-499a-b264-f80c6aee6a0e.tmp
[2024-06-27T07:49:59.818+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.11.58b09b04-39b4-499a-b264-f80c6aee6a0e.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/11
[2024-06-27T07:49:59.824+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1719474599733,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:49:59.837+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.849+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.866+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.868+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.907+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:49:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:49:59.926+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:49:59.928+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:49:59.937+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:49:59.938+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:49:59 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:49:59 INFO DAGScheduler: Missing parents: List()
24/06/27 07:49:59 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:49:59.940+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:49:59.942+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:49:59.944+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:49:59.946+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:49:59.949+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:49:59 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/06/27 07:49:59 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:49:59.953+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2024-06-27T07:49:59.971+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 38 for partition store_source_data-0
[2024-06-27T07:49:59.973+0000] {docker.py:436} INFO - 24/06/27 07:49:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:50:00.475+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:50:00.475+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:50:00.476+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:50:00.477+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:50:00.477+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2024-06-27T07:50:00.479+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2076 bytes result sent to driver
[2024-06-27T07:50:00.480+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 532 ms on localhost (executor driver) (1/1)
[2024-06-27T07:50:00.485+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/06/27 07:50:00 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.545 s
24/06/27 07:50:00 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:50:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-06-27T07:50:00.485+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 11
-------------------------------------------
[2024-06-27T07:50:00.486+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.554292 s
24/06/27 07:50:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:50:00.511+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:50:00.512+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:50:00.524+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/11 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.11.bee469ba-b1aa-4948-b36d-7b11d2ff7d6d.tmp
[2024-06-27T07:50:00.627+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.11.bee469ba-b1aa-4948-b36d-7b11d2ff7d6d.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/11
[2024-06-27T07:50:00.629+0000] {docker.py:436} INFO - 24/06/27 07:50:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:49:59.732Z",
  "batchId" : 11,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.1173184357541899,
  "durationMs" : {
    "addBatch" : 659,
    "commitOffsets" : 115,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 32,
    "triggerExecution" : 895,
    "walCommit" : 88
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 38
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.1173184357541899,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:50:02.305+0000] {docker.py:436} INFO - 24/06/27 07:50:02 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:50:10.630+0000] {docker.py:436} INFO - 24/06/27 07:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:20.632+0000] {docker.py:436} INFO - 24/06/27 07:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:30.635+0000] {docker.py:436} INFO - 24/06/27 07:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:40.642+0000] {docker.py:436} INFO - 24/06/27 07:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:50:50.648+0000] {docker.py:436} INFO - 24/06/27 07:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:00.650+0000] {docker.py:436} INFO - 24/06/27 07:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:10.661+0000] {docker.py:436} INFO - 24/06/27 07:51:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:20.671+0000] {docker.py:436} INFO - 24/06/27 07:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:30.676+0000] {docker.py:436} INFO - 24/06/27 07:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:40.681+0000] {docker.py:436} INFO - 24/06/27 07:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:51:42.123+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/12 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.12.f3452b31-ac6a-4b1a-8fec-a16c85ae49a2.tmp
[2024-06-27T07:51:42.282+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.12.f3452b31-ac6a-4b1a-8fec-a16c85ae49a2.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/12
[2024-06-27T07:51:42.292+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1719474702093,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:51:42.342+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.355+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.381+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.396+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.430+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.437+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:51:42.575+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:51:42.620+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:51:42.621+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:51:42.621+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:51:42.622+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:51:42.623+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:51:42.624+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:51:42.644+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:51:42.649+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:51:42.673+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:51:42.673+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:51:42.674+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[38] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:51:42.674+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-06-27T07:51:42.675+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:51:42.699+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2024-06-27T07:51:42.771+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 39 for partition store_source_data-0
[2024-06-27T07:51:42.777+0000] {docker.py:436} INFO - 24/06/27 07:51:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:51:43.287+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 07:51:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=40, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:51:43 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:51:43 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
24/06/27 07:51:43 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2076 bytes result sent to driver
[2024-06-27T07:51:43.288+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 616 ms on localhost (executor driver) (1/1)
[2024-06-27T07:51:43.292+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-06-27T07:51:43.294+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.665 s
24/06/27 07:51:43 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:51:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
24/06/27 07:51:43 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.681666 s
24/06/27 07:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:51:43.294+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 12
-------------------------------------------
[2024-06-27T07:51:43.373+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:51:43.374+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:51:43.400+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/12 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.12.94fcbc6c-80c2-4bed-983e-898716a20bcb.tmp
[2024-06-27T07:51:43.569+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.12.94fcbc6c-80c2-4bed-983e-898716a20bcb.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/12
[2024-06-27T07:51:43.581+0000] {docker.py:436} INFO - 24/06/27 07:51:43 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:51:42.091Z",
  "batchId" : 12,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.6784260515603799,
  "durationMs" : {
    "addBatch" : 1019,
    "commitOffsets" : 197,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 61,
    "triggerExecution" : 1474,
    "walCommit" : 195
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 39
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.6784260515603799,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:51:49.388+0000] {docker.py:436} INFO - 24/06/27 07:51:49 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:51:53.580+0000] {docker.py:436} INFO - 24/06/27 07:51:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:03.590+0000] {docker.py:436} INFO - 24/06/27 07:52:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:13.598+0000] {docker.py:436} INFO - 24/06/27 07:52:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:23.604+0000] {docker.py:436} INFO - 24/06/27 07:52:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:33.615+0000] {docker.py:436} INFO - 24/06/27 07:52:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:43.621+0000] {docker.py:436} INFO - 24/06/27 07:52:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:53.624+0000] {docker.py:436} INFO - 24/06/27 07:52:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:52:59.330+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/13 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.13.78b56742-f162-4da1-9288-a975876aff23.tmp
[2024-06-27T07:52:59.401+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.13.78b56742-f162-4da1-9288-a975876aff23.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/13
24/06/27 07:52:59 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1719474779311,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:52:59.486+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.493+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.610+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.636+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.649+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:52:59.733+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:52:59.741+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:52:59.748+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:52:59 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:52:59 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:52:59.749+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Missing parents: List()
24/06/27 07:52:59 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:52:59.752+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:52:59.765+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:52:59.770+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:52:59.773+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:52:59.779+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:52:59 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2024-06-27T07:52:59.785+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:52:59.799+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2024-06-27T07:52:59.813+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 40 for partition store_source_data-0
[2024-06-27T07:52:59.818+0000] {docker.py:436} INFO - 24/06/27 07:52:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:53:00.322+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:53:00.323+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=41, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:53:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:53:00.323+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2024-06-27T07:53:00.328+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2076 bytes result sent to driver
[2024-06-27T07:53:00.329+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 547 ms on localhost (executor driver) (1/1)
[2024-06-27T07:53:00.330+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-06-27T07:53:00.330+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
[2024-06-27T07:53:00.331+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:53:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-06-27T07:53:00.331+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.585939 s
[2024-06-27T07:53:00.332+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:53:00.332+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:53:00.333+0000] {docker.py:436} INFO - Batch: 13
-------------------------------------------
[2024-06-27T07:53:00.370+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:53:00.375+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:53:00.414+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/13 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.13.745bdf06-271b-4c11-8d1c-f48310e0bb8e.tmp
[2024-06-27T07:53:00.516+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.13.745bdf06-271b-4c11-8d1c-f48310e0bb8e.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/13
[2024-06-27T07:53:00.519+0000] {docker.py:436} INFO - 24/06/27 07:53:00 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:52:59.307Z",
  "batchId" : 13,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.8264462809917356,
  "durationMs" : {
    "addBatch" : 853,
    "commitOffsets" : 145,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 108,
    "triggerExecution" : 1210,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 40
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.8264462809917356,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:53:10.000+0000] {docker.py:436} INFO - 24/06/27 07:53:10 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:53:10.520+0000] {docker.py:436} INFO - 24/06/27 07:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:20.525+0000] {docker.py:436} INFO - 24/06/27 07:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:30.536+0000] {docker.py:436} INFO - 24/06/27 07:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:40.541+0000] {docker.py:436} INFO - 24/06/27 07:53:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:53:50.542+0000] {docker.py:436} INFO - 24/06/27 07:53:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:00.549+0000] {docker.py:436} INFO - 24/06/27 07:54:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:10.551+0000] {docker.py:436} INFO - 24/06/27 07:54:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:20.561+0000] {docker.py:436} INFO - 24/06/27 07:54:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:30.567+0000] {docker.py:436} INFO - 24/06/27 07:54:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:40.578+0000] {docker.py:436} INFO - 24/06/27 07:54:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:54:47.770+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/14 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.14.483b81f2-ab8f-4b8e-875e-a3a92d89b9ef.tmp
[2024-06-27T07:54:47.830+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.14.483b81f2-ab8f-4b8e-875e-a3a92d89b9ef.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/14
24/06/27 07:54:47 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1719474887751,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:54:47.865+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.866+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.877+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.881+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.889+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.890+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:54:47.909+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 07:54:47 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 07:54:47 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:54:47 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:54:47 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:54:47 INFO DAGScheduler: Missing parents: List()
24/06/27 07:54:47 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 07:54:47 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
24/06/27 07:54:47 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
24/06/27 07:54:47 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:54:47.920+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:54:47.924+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[44] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 07:54:47 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2024-06-27T07:54:47.929+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:54:47.938+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2024-06-27T07:54:47.955+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 41 for partition store_source_data-0
[2024-06-27T07:54:47.960+0000] {docker.py:436} INFO - 24/06/27 07:54:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:54:48.460+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:54:48.461+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:54:48.463+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:54:48 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
24/06/27 07:54:48 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2076 bytes result sent to driver
[2024-06-27T07:54:48.465+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 537 ms on localhost (executor driver) (1/1)
[2024-06-27T07:54:48.466+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-06-27T07:54:48.472+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 14
-------------------------------------------
[2024-06-27T07:54:48.472+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 0.561 s
24/06/27 07:54:48 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:54:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/06/27 07:54:48 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.564662 s
24/06/27 07:54:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:54:48.494+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:54:48.495+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:54:48.515+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/14 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.14.732cdebf-1608-496a-9d45-9e12afff1a27.tmp
[2024-06-27T07:54:48.619+0000] {docker.py:436} INFO - 24/06/27 07:54:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.14.732cdebf-1608-496a-9d45-9e12afff1a27.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/14
24/06/27 07:54:48 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:54:47.749Z",
  "batchId" : 14,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 1.1520737327188941,
  "durationMs" : {
    "addBatch" : 625,
    "commitOffsets" : 125,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 868,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 41
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 1.1520737327188941,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:54:55.435+0000] {docker.py:436} INFO - 24/06/27 07:54:55 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:54:58.628+0000] {docker.py:436} INFO - 24/06/27 07:54:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:08.638+0000] {docker.py:436} INFO - 24/06/27 07:55:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:18.646+0000] {docker.py:436} INFO - 24/06/27 07:55:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:28.654+0000] {docker.py:436} INFO - 24/06/27 07:55:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:38.661+0000] {docker.py:436} INFO - 24/06/27 07:55:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:48.663+0000] {docker.py:436} INFO - 24/06/27 07:55:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:55:58.668+0000] {docker.py:436} INFO - 24/06/27 07:55:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:08.677+0000] {docker.py:436} INFO - 24/06/27 07:56:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:18.679+0000] {docker.py:436} INFO - 24/06/27 07:56:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:28.688+0000] {docker.py:436} INFO - 24/06/27 07:56:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:38.692+0000] {docker.py:436} INFO - 24/06/27 07:56:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:48.702+0000] {docker.py:436} INFO - 24/06/27 07:56:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:56:58.705+0000] {docker.py:436} INFO - 24/06/27 07:56:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:08.715+0000] {docker.py:436} INFO - 24/06/27 07:57:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:18.718+0000] {docker.py:436} INFO - 24/06/27 07:57:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:28.725+0000] {docker.py:436} INFO - 24/06/27 07:57:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:38.736+0000] {docker.py:436} INFO - 24/06/27 07:57:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:48.743+0000] {docker.py:436} INFO - 24/06/27 07:57:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:57:58.746+0000] {docker.py:436} INFO - 24/06/27 07:57:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:08.749+0000] {docker.py:436} INFO - 24/06/27 07:58:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:18.757+0000] {docker.py:436} INFO - 24/06/27 07:58:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:28.766+0000] {docker.py:436} INFO - 24/06/27 07:58:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:38.767+0000] {docker.py:436} INFO - 24/06/27 07:58:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:58:47.985+0000] {docker.py:436} INFO - 24/06/27 07:58:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/15 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.15.eaba664d-3fa6-4186-b6b7-d61907d1637b.tmp
[2024-06-27T07:58:48.037+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.15.eaba664d-3fa6-4186-b6b7-d61907d1637b.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/15
24/06/27 07:58:48 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1719475127971,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:58:48.042+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.046+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.101+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.105+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.145+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.162+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:58:48.184+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:58:48.185+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:58:48.190+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T07:58:48.191+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T07:58:48.192+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T07:58:48.193+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:58:48.194+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[47] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:58:48.202+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:58:48.203+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:58:48.209+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:58:48.210+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:58:48.212+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[47] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:58:48.212+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2024-06-27T07:58:48.215+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:58:48.219+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2024-06-27T07:58:48.251+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 42 for partition store_source_data-0
[2024-06-27T07:58:48.256+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:58:48.766+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:58:48.766+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=43, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:58:48.766+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T07:58:48.767+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2024-06-27T07:58:48.792+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2076 bytes result sent to driver
[2024-06-27T07:58:48.797+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 581 ms on localhost (executor driver) (1/1)
[2024-06-27T07:58:48.798+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-06-27T07:58:48.799+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 0.598 s
24/06/27 07:58:48 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 07:58:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-06-27T07:58:48.799+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.612363 s
[2024-06-27T07:58:48.799+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:58:48.800+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:58:48.800+0000] {docker.py:436} INFO - Batch: 15
[2024-06-27T07:58:48.807+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T07:58:48.912+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:58:48.917+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:58:48.949+0000] {docker.py:436} INFO - 24/06/27 07:58:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/15 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.15.3475bfbd-1497-40fe-9d25-058deff570e6.tmp
[2024-06-27T07:58:49.045+0000] {docker.py:436} INFO - 24/06/27 07:58:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.15.3475bfbd-1497-40fe-9d25-058deff570e6.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/15
[2024-06-27T07:58:49.048+0000] {docker.py:436} INFO - 24/06/27 07:58:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:58:47.969Z",
  "batchId" : 15,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.9285051067780873,
  "durationMs" : {
    "addBatch" : 859,
    "commitOffsets" : 134,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 24,
    "triggerExecution" : 1077,
    "walCommit" : 58
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 42
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.9285051067780873,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:58:49.307+0000] {docker.py:436} INFO - 24/06/27 07:58:49 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:58:59.050+0000] {docker.py:436} INFO - 24/06/27 07:58:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:09.054+0000] {docker.py:436} INFO - 24/06/27 07:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:19.057+0000] {docker.py:436} INFO - 24/06/27 07:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:29.067+0000] {docker.py:436} INFO - 24/06/27 07:59:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:34.858+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/16 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.16.f418a4a1-29f7-4dfc-bbcd-dee8b31286af.tmp
[2024-06-27T07:59:34.903+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.16.f418a4a1-29f7-4dfc-bbcd-dee8b31286af.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/16
[2024-06-27T07:59:34.903+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1719475174843,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T07:59:34.922+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.928+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.943+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.972+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.978+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T07:59:34.998+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T07:59:34.999+0000] {docker.py:436} INFO - 24/06/27 07:59:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T07:59:35.015+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 07:59:35 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)
24/06/27 07:59:35 INFO DAGScheduler: Parents of final stage: List()
24/06/27 07:59:35 INFO DAGScheduler: Missing parents: List()
[2024-06-27T07:59:35.016+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[50] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T07:59:35.025+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T07:59:35.025+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T07:59:35.026+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:59:35.026+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
[2024-06-27T07:59:35.027+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[50] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T07:59:35.027+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2024-06-27T07:59:35.028+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T07:59:35.029+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2024-06-27T07:59:35.057+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 43 for partition store_source_data-0
[2024-06-27T07:59:35.062+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T07:59:35.565+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:59:35.574+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T07:59:35.594+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T07:59:35.607+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 07:59:35 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2024-06-27T07:59:35.628+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2076 bytes result sent to driver
[2024-06-27T07:59:35.629+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 601 ms on localhost (executor driver) (1/1)
[2024-06-27T07:59:35.630+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-06-27T07:59:35.630+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.611 s
[2024-06-27T07:59:35.632+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T07:59:35.641+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2024-06-27T07:59:35.645+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 16
-------------------------------------------
[2024-06-27T07:59:35.645+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.641376 s
24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T07:59:35.756+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T07:59:35.757+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T07:59:35.818+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/16 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.16.ba39b21d-955c-4f02-99c1-7cf3b11d2391.tmp
[2024-06-27T07:59:35.955+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.16.ba39b21d-955c-4f02-99c1-7cf3b11d2391.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/16
[2024-06-27T07:59:35.957+0000] {docker.py:436} INFO - 24/06/27 07:59:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T07:59:34.842Z",
  "batchId" : 16,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.8984725965858041,
  "durationMs" : {
    "addBatch" : 823,
    "commitOffsets" : 200,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 28,
    "triggerExecution" : 1113,
    "walCommit" : 60
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 43
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.8984725965858041,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T07:59:42.603+0000] {docker.py:436} INFO - 24/06/27 07:59:42 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T07:59:45.963+0000] {docker.py:436} INFO - 24/06/27 07:59:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T07:59:55.963+0000] {docker.py:436} INFO - 24/06/27 07:59:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:05.975+0000] {docker.py:436} INFO - 24/06/27 08:00:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:15.976+0000] {docker.py:436} INFO - 24/06/27 08:00:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:25.802+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/17 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.17.b405cba2-428a-473c-ac2b-910c751cd4a6.tmp
[2024-06-27T08:00:25.838+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.17.b405cba2-428a-473c-ac2b-910c751cd4a6.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/17
[2024-06-27T08:00:25.839+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1719475225788,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:00:25.869+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.880+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.926+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.939+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.951+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:25.960+0000] {docker.py:436} INFO - 24/06/27 08:00:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:00:26.004+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:00:26.004+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:00:26.006+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:00:26 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:00:26.007+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:00:26.009+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:00:26.010+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[53] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:00:26.013+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:00:26.014+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:00:26.015+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:00:26.016+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:00:26.018+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[53] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:00:26.019+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2024-06-27T08:00:26.022+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:00:26.023+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2024-06-27T08:00:26.037+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 44 for partition store_source_data-0
[2024-06-27T08:00:26.043+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:00:26.565+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:00:26.565+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:00:26.567+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:00:26.568+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2024-06-27T08:00:26.571+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2076 bytes result sent to driver
[2024-06-27T08:00:26.581+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 560 ms on localhost (executor driver) (1/1)
[2024-06-27T08:00:26.582+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-06-27T08:00:26.583+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s
[2024-06-27T08:00:26.584+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:00:26.585+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2024-06-27T08:00:26.586+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.579729 s
[2024-06-27T08:00:26.587+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:00:26.587+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 17
-------------------------------------------
[2024-06-27T08:00:26.719+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:00:26.720+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:00:26.822+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/17 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.17.429170e6-680e-4e3a-ae15-7d2a8c80b11c.tmp
[2024-06-27T08:00:26.866+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.17.429170e6-680e-4e3a-ae15-7d2a8c80b11c.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/17
[2024-06-27T08:00:26.868+0000] {docker.py:436} INFO - 24/06/27 08:00:26 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:00:25.786Z",
  "batchId" : 17,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9259259259259258,
  "durationMs" : {
    "addBatch" : 827,
    "commitOffsets" : 156,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 43,
    "triggerExecution" : 1080,
    "walCommit" : 50
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 44
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9259259259259258,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:00:36.053+0000] {docker.py:436} INFO - 24/06/27 08:00:36 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:00:36.876+0000] {docker.py:436} INFO - 24/06/27 08:00:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:46.880+0000] {docker.py:436} INFO - 24/06/27 08:00:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:00:56.882+0000] {docker.py:436} INFO - 24/06/27 08:00:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:06.884+0000] {docker.py:436} INFO - 24/06/27 08:01:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:16.892+0000] {docker.py:436} INFO - 24/06/27 08:01:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:26.903+0000] {docker.py:436} INFO - 24/06/27 08:01:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:36.908+0000] {docker.py:436} INFO - 24/06/27 08:01:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:46.912+0000] {docker.py:436} INFO - 24/06/27 08:01:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:01:56.917+0000] {docker.py:436} INFO - 24/06/27 08:01:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:06.916+0000] {docker.py:436} INFO - 24/06/27 08:02:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:16.925+0000] {docker.py:436} INFO - 24/06/27 08:02:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:26.931+0000] {docker.py:436} INFO - 24/06/27 08:02:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:36.934+0000] {docker.py:436} INFO - 24/06/27 08:02:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:46.937+0000] {docker.py:436} INFO - 24/06/27 08:02:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:02:56.942+0000] {docker.py:436} INFO - 24/06/27 08:02:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:04.438+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/18 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.18.11d927a9-18d9-48a6-b5e8-1ef9b840acbb.tmp
[2024-06-27T08:03:04.482+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.18.11d927a9-18d9-48a6-b5e8-1ef9b840acbb.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/18
[2024-06-27T08:03:04.483+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1719475384425,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:03:04.530+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.532+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.558+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.572+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.603+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:03:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:03:04.635+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:03:04.636+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:03:04.636+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:03:04 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:03:04.637+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:03:04.637+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:03:04.638+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:03:04.640+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:03:04.641+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:03:04.642+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:03:04.646+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:03:04.647+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:03:04.647+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-06-27T08:03:04.649+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:03:04.666+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2024-06-27T08:03:04.696+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 45 for partition store_source_data-0
[2024-06-27T08:03:04.699+0000] {docker.py:436} INFO - 24/06/27 08:03:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:03:05.202+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:03:05.204+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=46, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:03:05.205+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:03:05.205+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2024-06-27T08:03:05.207+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2076 bytes result sent to driver
[2024-06-27T08:03:05.212+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 562 ms on localhost (executor driver) (1/1)
24/06/27 08:03:05 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-06-27T08:03:05.213+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.576 s
[2024-06-27T08:03:05.218+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:03:05.220+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-06-27T08:03:05.222+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.591176 s
[2024-06-27T08:03:05.223+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:03:05.224+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:03:05.227+0000] {docker.py:436} INFO - Batch: 18
[2024-06-27T08:03:05.228+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:03:05.308+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:03:05.309+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:03:05.327+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/18 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.18.31588ccc-3bad-4afa-90e7-d07f040fc726.tmp
[2024-06-27T08:03:05.380+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.18.31588ccc-3bad-4afa-90e7-d07f040fc726.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/18
[2024-06-27T08:03:05.382+0000] {docker.py:436} INFO - 24/06/27 08:03:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:03:04.423Z",
  "batchId" : 18,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.0460251046025104,
  "durationMs" : {
    "addBatch" : 770,
    "commitOffsets" : 71,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 54,
    "triggerExecution" : 956,
    "walCommit" : 56
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 45
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.0460251046025104,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:03:11.911+0000] {docker.py:436} INFO - 24/06/27 08:03:11 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:03:15.396+0000] {docker.py:436} INFO - 24/06/27 08:03:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:25.393+0000] {docker.py:436} INFO - 24/06/27 08:03:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:35.398+0000] {docker.py:436} INFO - 24/06/27 08:03:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:45.401+0000] {docker.py:436} INFO - 24/06/27 08:03:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:03:55.404+0000] {docker.py:436} INFO - 24/06/27 08:03:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:05.412+0000] {docker.py:436} INFO - 24/06/27 08:04:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:15.419+0000] {docker.py:436} INFO - 24/06/27 08:04:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:25.422+0000] {docker.py:436} INFO - 24/06/27 08:04:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:33.000+0000] {docker.py:436} INFO - 24/06/27 08:04:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/19 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.19.3de15ec7-000a-4c2c-a3cc-ae1f802ecfc6.tmp
[2024-06-27T08:04:33.055+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.19.3de15ec7-000a-4c2c-a3cc-ae1f802ecfc6.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/19
[2024-06-27T08:04:33.059+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1719475472977,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:04:33.070+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.074+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.089+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.094+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.109+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.118+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:04:33.137+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:04:33.139+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:04:33.140+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:04:33.140+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:04:33.141+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:04:33.142+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:04:33.142+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:04:33.149+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:04:33.160+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:04:33.166+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
24/06/27 08:04:33 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:04:33.171+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:04:33 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-06-27T08:04:33.175+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:04:33.185+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2024-06-27T08:04:33.215+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 46 for partition store_source_data-0
[2024-06-27T08:04:33.226+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:04:33.746+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:04:33.749+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=47, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:04:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:04:33 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2024-06-27T08:04:33.750+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2076 bytes result sent to driver
[2024-06-27T08:04:33.754+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 579 ms on localhost (executor driver) (1/1)
24/06/27 08:04:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
24/06/27 08:04:33 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 0.610 s
24/06/27 08:04:33 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:04:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-06-27T08:04:33.768+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 19
-------------------------------------------
[2024-06-27T08:04:33.769+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.617316 s
24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:04:33.867+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:04:33.868+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:04:33.880+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/19 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.19.5212c525-b137-4a56-ba39-f71c2f19e35e.tmp
[2024-06-27T08:04:33.922+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.19.5212c525-b137-4a56-ba39-f71c2f19e35e.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/19
[2024-06-27T08:04:33.923+0000] {docker.py:436} INFO - 24/06/27 08:04:33 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:04:32.971Z",
  "batchId" : 19,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 55.55555555555556,
  "processedRowsPerSecond" : 1.0515247108307046,
  "durationMs" : {
    "addBatch" : 781,
    "commitOffsets" : 63,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 22,
    "triggerExecution" : 951,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 46
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 55.55555555555556,
    "processedRowsPerSecond" : 1.0515247108307046,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:04:43.926+0000] {docker.py:436} INFO - 24/06/27 08:04:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:04:44.552+0000] {docker.py:436} INFO - 24/06/27 08:04:44 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:04:53.928+0000] {docker.py:436} INFO - 24/06/27 08:04:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:03.935+0000] {docker.py:436} INFO - 24/06/27 08:05:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:13.936+0000] {docker.py:436} INFO - 24/06/27 08:05:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:23.937+0000] {docker.py:436} INFO - 24/06/27 08:05:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:33.946+0000] {docker.py:436} INFO - 24/06/27 08:05:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:43.946+0000] {docker.py:436} INFO - 24/06/27 08:05:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:05:53.948+0000] {docker.py:436} INFO - 24/06/27 08:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:03.958+0000] {docker.py:436} INFO - 24/06/27 08:06:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:13.966+0000] {docker.py:436} INFO - 24/06/27 08:06:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:23.971+0000] {docker.py:436} INFO - 24/06/27 08:06:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:33.971+0000] {docker.py:436} INFO - 24/06/27 08:06:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:43.978+0000] {docker.py:436} INFO - 24/06/27 08:06:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:06:53.983+0000] {docker.py:436} INFO - 24/06/27 08:06:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:03.988+0000] {docker.py:436} INFO - 24/06/27 08:07:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:13.996+0000] {docker.py:436} INFO - 24/06/27 08:07:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:16.662+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/20 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.20.ad724dd2-02e8-4a69-b274-4830da2cb143.tmp
[2024-06-27T08:07:16.707+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.20.ad724dd2-02e8-4a69-b274-4830da2cb143.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/20
[2024-06-27T08:07:16.707+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1719475636649,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:07:16.721+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.724+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.747+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.753+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.779+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.791+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:07:16.838+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:07:16.841+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:07:16.845+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:07:16 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:07:16 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:07:16 INFO DAGScheduler: Missing parents: List()
24/06/27 08:07:16 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[62] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:07:16.846+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:07:16.848+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:07:16.850+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:07:16.857+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:07:16.858+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[62] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:07:16.860+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-06-27T08:07:16.862+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:07:16.877+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2024-06-27T08:07:16.906+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 47 for partition store_source_data-0
[2024-06-27T08:07:16.913+0000] {docker.py:436} INFO - 24/06/27 08:07:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:07:17.415+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:07:17.418+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:07:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:07:17 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2024-06-27T08:07:17.419+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2076 bytes result sent to driver
[2024-06-27T08:07:17.424+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 562 ms on localhost (executor driver) (1/1)
24/06/27 08:07:17 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-06-27T08:07:17.426+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.582 s
24/06/27 08:07:17 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:07:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
24/06/27 08:07:17 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.584923 s
24/06/27 08:07:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:07:17.427+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 20
-------------------------------------------
[2024-06-27T08:07:17.474+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:07:17.492+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:07:17.498+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:07:17.519+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/20 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.20.505301e5-6ad1-4d07-a151-c96f053bfb37.tmp
[2024-06-27T08:07:17.579+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.20.505301e5-6ad1-4d07-a151-c96f053bfb37.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/20
[2024-06-27T08:07:17.580+0000] {docker.py:436} INFO - 24/06/27 08:07:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:07:16.647Z",
  "batchId" : 20,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 1.0741138560687433,
  "durationMs" : {
    "addBatch" : 771,
    "commitOffsets" : 81,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 20,
    "triggerExecution" : 931,
    "walCommit" : 57
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 47
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 1.0741138560687433,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:07:27.588+0000] {docker.py:436} INFO - 24/06/27 08:07:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:37.593+0000] {docker.py:436} INFO - 24/06/27 08:07:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:47.599+0000] {docker.py:436} INFO - 24/06/27 08:07:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:07:57.600+0000] {docker.py:436} INFO - 24/06/27 08:07:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:07.603+0000] {docker.py:436} INFO - 24/06/27 08:08:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:17.606+0000] {docker.py:436} INFO - 24/06/27 08:08:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:27.614+0000] {docker.py:436} INFO - 24/06/27 08:08:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:37.623+0000] {docker.py:436} INFO - 24/06/27 08:08:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:47.627+0000] {docker.py:436} INFO - 24/06/27 08:08:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:57.634+0000] {docker.py:436} INFO - 24/06/27 08:08:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:08:58.600+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/21 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.21.b196c83e-1ee8-4c97-aee4-da1cd80f7ace.tmp
[2024-06-27T08:08:58.725+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.21.b196c83e-1ee8-4c97-aee4-da1cd80f7ace.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/21
24/06/27 08:08:58 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1719475738576,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:08:58.743+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.750+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.769+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.774+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.794+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.798+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:08:58.855+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:08:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:08:58 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:08:58 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:08:58.856+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:08:58.858+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:08:58.860+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:08:58.870+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:08:58.876+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:08:58.882+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:08:58.882+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:08:58.884+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[65] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:08:58.885+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-06-27T08:08:58.888+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:08:58.894+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2024-06-27T08:08:58.941+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 48 for partition store_source_data-0
[2024-06-27T08:08:58.944+0000] {docker.py:436} INFO - 24/06/27 08:08:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:08:59.447+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:08:59.453+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=49, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:08:59.454+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:08:59 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)
[2024-06-27T08:08:59.457+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2076 bytes result sent to driver
[2024-06-27T08:08:59.458+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 570 ms on localhost (executor driver) (1/1)
24/06/27 08:08:59 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-06-27T08:08:59.460+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 0.592 s
[2024-06-27T08:08:59.461+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:08:59.462+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-06-27T08:08:59.465+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.611361 s
[2024-06-27T08:08:59.465+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:08:59.466+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 21
[2024-06-27T08:08:59.466+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:08:59.518+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:08:59.521+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:08:59.545+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/21 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.21.1716e3bb-4715-4910-87b2-241a513c5108.tmp
[2024-06-27T08:08:59.677+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.21.1716e3bb-4715-4910-87b2-241a513c5108.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/21
[2024-06-27T08:08:59.693+0000] {docker.py:436} INFO - 24/06/27 08:08:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:08:58.575Z",
  "batchId" : 21,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.9074410163339383,
  "durationMs" : {
    "addBatch" : 763,
    "commitOffsets" : 156,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 48,
    "triggerExecution" : 1102,
    "walCommit" : 133
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 48
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.9074410163339383,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:09:00.821+0000] {docker.py:436} INFO - 24/06/27 08:09:00 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:09:09.701+0000] {docker.py:436} INFO - 24/06/27 08:09:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:19.709+0000] {docker.py:436} INFO - 24/06/27 08:09:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:29.710+0000] {docker.py:436} INFO - 24/06/27 08:09:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:39.716+0000] {docker.py:436} INFO - 24/06/27 08:09:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:49.717+0000] {docker.py:436} INFO - 24/06/27 08:09:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:09:59.723+0000] {docker.py:436} INFO - 24/06/27 08:09:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:09.724+0000] {docker.py:436} INFO - 24/06/27 08:10:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:19.723+0000] {docker.py:436} INFO - 24/06/27 08:10:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:29.734+0000] {docker.py:436} INFO - 24/06/27 08:10:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:39.738+0000] {docker.py:436} INFO - 24/06/27 08:10:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:49.744+0000] {docker.py:436} INFO - 24/06/27 08:10:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:10:58.375+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/22 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.22.ae2b9657-d0e7-4a1d-bab1-b092d3b6baa7.tmp
[2024-06-27T08:10:58.485+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.22.ae2b9657-d0e7-4a1d-bab1-b092d3b6baa7.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/22
24/06/27 08:10:58 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1719475858358,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:10:58.533+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.535+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.554+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.555+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.568+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.569+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:10:58.617+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:10:58.621+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:10:58.630+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:10:58 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:10:58 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:10:58 INFO DAGScheduler: Missing parents: List()
24/06/27 08:10:58 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:10:58 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
24/06/27 08:10:58 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:10:58.641+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
24/06/27 08:10:58 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:10:58.659+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[68] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:10:58 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-06-27T08:10:58.660+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:10:58.663+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2024-06-27T08:10:58.703+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 49 for partition store_source_data-0
[2024-06-27T08:10:58.718+0000] {docker.py:436} INFO - 24/06/27 08:10:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:10:59.220+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:10:59.221+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:10:59.228+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=50, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:10:59.228+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:10:59.229+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DataWritingSparkTask: Committed partition 0 (task 22, attempt 0, stage 22.0)
[2024-06-27T08:10:59.229+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2076 bytes result sent to driver
24/06/27 08:10:59 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 570 ms on localhost (executor driver) (1/1)
[2024-06-27T08:10:59.230+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-06-27T08:10:59.233+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 0.603 s
[2024-06-27T08:10:59.234+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:10:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2024-06-27T08:10:59.240+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.610332 s
24/06/27 08:10:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:10:59.241+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 22
-------------------------------------------
[2024-06-27T08:10:59.339+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:10:59.339+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:10:59.377+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/22 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.22.fede961b-34d0-4e52-9b9b-ae9be2808feb.tmp
[2024-06-27T08:10:59.465+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.22.fede961b-34d0-4e52-9b9b-ae9be2808feb.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/22
[2024-06-27T08:10:59.466+0000] {docker.py:436} INFO - 24/06/27 08:10:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:10:58.356Z",
  "batchId" : 22,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9025270758122743,
  "durationMs" : {
    "addBatch" : 793,
    "commitOffsets" : 127,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 62,
    "triggerExecution" : 1108,
    "walCommit" : 123
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 49
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9025270758122743,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:11:09.470+0000] {docker.py:436} INFO - 24/06/27 08:11:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:10.139+0000] {docker.py:436} INFO - 24/06/27 08:11:10 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:11:19.479+0000] {docker.py:436} INFO - 24/06/27 08:11:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:29.486+0000] {docker.py:436} INFO - 24/06/27 08:11:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:39.494+0000] {docker.py:436} INFO - 24/06/27 08:11:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:49.501+0000] {docker.py:436} INFO - 24/06/27 08:11:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:11:59.508+0000] {docker.py:436} INFO - 24/06/27 08:11:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:09.516+0000] {docker.py:436} INFO - 24/06/27 08:12:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:19.523+0000] {docker.py:436} INFO - 24/06/27 08:12:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:29.529+0000] {docker.py:436} INFO - 24/06/27 08:12:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:39.528+0000] {docker.py:436} INFO - 24/06/27 08:12:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:49.531+0000] {docker.py:436} INFO - 24/06/27 08:12:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:12:59.534+0000] {docker.py:436} INFO - 24/06/27 08:12:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:09.535+0000] {docker.py:436} INFO - 24/06/27 08:13:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:13.159+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/23 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.23.2e81452b-9d5f-481e-8763-75615737d6ed.tmp
[2024-06-27T08:13:13.304+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.23.2e81452b-9d5f-481e-8763-75615737d6ed.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/23
24/06/27 08:13:13 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1719475993142,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:13:13.356+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.379+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.420+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.421+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.481+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.493+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:13:13.508+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:13:13.523+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:13:13 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:13:13 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:13:13 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:13:13 INFO DAGScheduler: Missing parents: List()
24/06/27 08:13:13 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:13:13.562+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:13:13.564+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:13:13.566+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:13:13.567+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:13:13.568+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[71] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:13:13.568+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2024-06-27T08:13:13.579+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:13:13.580+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2024-06-27T08:13:13.665+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 50 for partition store_source_data-0
[2024-06-27T08:13:13.683+0000] {docker.py:436} INFO - 24/06/27 08:13:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:13:14.179+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:13:14.183+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:13:14.184+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:13:14 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)
24/06/27 08:13:14 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2076 bytes result sent to driver
[2024-06-27T08:13:14.186+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 616 ms on localhost (executor driver) (1/1)
[2024-06-27T08:13:14.187+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-06-27T08:13:14.192+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.669 s
[2024-06-27T08:13:14.192+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:13:14.193+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2024-06-27T08:13:14.193+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.676248 s
[2024-06-27T08:13:14.193+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:13:14.194+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:13:14.194+0000] {docker.py:436} INFO - Batch: 23
[2024-06-27T08:13:14.194+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:13:14.223+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:13:14.223+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:13:14.238+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/23 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.23.4e519412-4525-41b9-b1d0-84cbe696de30.tmp
[2024-06-27T08:13:14.278+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.23.4e519412-4525-41b9-b1d0-84cbe696de30.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/23
[2024-06-27T08:13:14.281+0000] {docker.py:436} INFO - 24/06/27 08:13:14 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:13:13.132Z",
  "batchId" : 23,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 0.8733624454148472,
  "durationMs" : {
    "addBatch" : 849,
    "commitOffsets" : 50,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 80,
    "triggerExecution" : 1145,
    "walCommit" : 154
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 50
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 0.8733624454148472,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:13:20.218+0000] {docker.py:436} INFO - 24/06/27 08:13:20 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:13:24.287+0000] {docker.py:436} INFO - 24/06/27 08:13:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:34.289+0000] {docker.py:436} INFO - 24/06/27 08:13:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:44.298+0000] {docker.py:436} INFO - 24/06/27 08:13:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:13:54.300+0000] {docker.py:436} INFO - 24/06/27 08:13:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:04.308+0000] {docker.py:436} INFO - 24/06/27 08:14:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:14.317+0000] {docker.py:436} INFO - 24/06/27 08:14:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:24.322+0000] {docker.py:436} INFO - 24/06/27 08:14:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:34.323+0000] {docker.py:436} INFO - 24/06/27 08:14:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:44.330+0000] {docker.py:436} INFO - 24/06/27 08:14:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:14:54.341+0000] {docker.py:436} INFO - 24/06/27 08:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:04.348+0000] {docker.py:436} INFO - 24/06/27 08:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:14.359+0000] {docker.py:436} INFO - 24/06/27 08:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:24.367+0000] {docker.py:436} INFO - 24/06/27 08:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:34.372+0000] {docker.py:436} INFO - 24/06/27 08:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:44.375+0000] {docker.py:436} INFO - 24/06/27 08:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:15:54.376+0000] {docker.py:436} INFO - 24/06/27 08:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:03.413+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/24 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.24.978341c3-676a-48b6-8d26-c7b177914251.tmp
[2024-06-27T08:16:03.476+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.24.978341c3-676a-48b6-8d26-c7b177914251.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/24
[2024-06-27T08:16:03.477+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1719476163401,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:16:03.489+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.494+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.521+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.521+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.624+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.625+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:16:03.729+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:16:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:16:03 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:16:03 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:16:03 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:16:03 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:16:03.733+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[74] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:16:03.750+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:16:03.756+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:16:03.761+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:16:03.762+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:16:03.763+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[74] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:16:03.764+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2024-06-27T08:16:03.766+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:16:03.770+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2024-06-27T08:16:03.785+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 51 for partition store_source_data-0
[2024-06-27T08:16:03.804+0000] {docker.py:436} INFO - 24/06/27 08:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:16:04.304+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:16:04.305+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=52, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:16:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:16:04 INFO DataWritingSparkTask: Committed partition 0 (task 24, attempt 0, stage 24.0)
[2024-06-27T08:16:04.308+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2076 bytes result sent to driver
[2024-06-27T08:16:04.309+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 542 ms on localhost (executor driver) (1/1)
24/06/27 08:16:04 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2024-06-27T08:16:04.309+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.574 s
[2024-06-27T08:16:04.310+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:16:04.311+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2024-06-27T08:16:04.312+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.584074 s
[2024-06-27T08:16:04.313+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:16:04.313+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:16:04.314+0000] {docker.py:436} INFO - Batch: 24
-------------------------------------------
[2024-06-27T08:16:04.359+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:16:04.360+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:16:04.380+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/24 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.24.b1bc6546-4884-40b2-937f-16d6a12c8f48.tmp
[2024-06-27T08:16:04.498+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.24.b1bc6546-4884-40b2-937f-16d6a12c8f48.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/24
[2024-06-27T08:16:04.500+0000] {docker.py:436} INFO - 24/06/27 08:16:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:16:03.397Z",
  "batchId" : 24,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9082652134423251,
  "durationMs" : {
    "addBatch" : 864,
    "commitOffsets" : 139,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 19,
    "triggerExecution" : 1101,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 51
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9082652134423251,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:16:14.498+0000] {docker.py:436} INFO - 24/06/27 08:16:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:15.177+0000] {docker.py:436} INFO - 24/06/27 08:16:15 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:16:24.504+0000] {docker.py:436} INFO - 24/06/27 08:16:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:34.505+0000] {docker.py:436} INFO - 24/06/27 08:16:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:44.510+0000] {docker.py:436} INFO - 24/06/27 08:16:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:16:54.518+0000] {docker.py:436} INFO - 24/06/27 08:16:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:04.521+0000] {docker.py:436} INFO - 24/06/27 08:17:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:14.531+0000] {docker.py:436} INFO - 24/06/27 08:17:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:22.956+0000] {docker.py:436} INFO - 24/06/27 08:17:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/25 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.25.54508aff-6901-4185-83a6-3fead1bf54c4.tmp
[2024-06-27T08:17:23.005+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.25.54508aff-6901-4185-83a6-3fead1bf54c4.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/25
[2024-06-27T08:17:23.006+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1719476242943,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:17:23.023+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.047+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.081+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.116+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:17:23.142+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:17:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:17:23 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:17:23 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:17:23 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:17:23 INFO DAGScheduler: Missing parents: List()
24/06/27 08:17:23 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[77] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:17:23.149+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.154+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:17:23.155+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:17:23.155+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:17:23.156+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[77] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:17:23.158+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-06-27T08:17:23.160+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:17:23.164+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
[2024-06-27T08:17:23.193+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 52 for partition store_source_data-0
[2024-06-27T08:17:23.207+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:17:23.706+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:17:23.706+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:17:23.708+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=53, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:17:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:17:23 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 25.0)
[2024-06-27T08:17:23.709+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 2076 bytes result sent to driver
[2024-06-27T08:17:23.711+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 550 ms on localhost (executor driver) (1/1)
[2024-06-27T08:17:23.711+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-06-27T08:17:23.712+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 0.570 s
[2024-06-27T08:17:23.712+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:17:23.713+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-06-27T08:17:23.714+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.573766 s
[2024-06-27T08:17:23.714+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:17:23.715+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 25
-------------------------------------------
[2024-06-27T08:17:23.748+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:17:23.749+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:17:23.801+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/25 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.25.ec9d858e-d78b-4dc7-ac06-28e06d6c95d3.tmp
[2024-06-27T08:17:23.863+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.25.ec9d858e-d78b-4dc7-ac06-28e06d6c95d3.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/25
[2024-06-27T08:17:23.864+0000] {docker.py:436} INFO - 24/06/27 08:17:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:17:22.942Z",
  "batchId" : 25,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 1.0857763300760044,
  "durationMs" : {
    "addBatch" : 731,
    "commitOffsets" : 108,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 16,
    "triggerExecution" : 921,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 52
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 1.0857763300760044,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:17:25.696+0000] {docker.py:436} INFO - 24/06/27 08:17:25 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:17:33.867+0000] {docker.py:436} INFO - 24/06/27 08:17:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:43.869+0000] {docker.py:436} INFO - 24/06/27 08:17:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:17:53.875+0000] {docker.py:436} INFO - 24/06/27 08:17:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:03.876+0000] {docker.py:436} INFO - 24/06/27 08:18:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:13.878+0000] {docker.py:436} INFO - 24/06/27 08:18:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:21.999+0000] {docker.py:436} INFO - 24/06/27 08:18:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/26 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.26.eb82c2ba-383f-404a-98da-ada732384970.tmp
[2024-06-27T08:18:22.063+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.26.eb82c2ba-383f-404a-98da-ada732384970.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/26
[2024-06-27T08:18:22.070+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1719476301981,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:18:22.110+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.113+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.157+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.159+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.176+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.190+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:18:22.229+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:18:22.237+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:18:22 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:18:22 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:18:22 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:18:22 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:18:22.238+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:18:22.240+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:18:22.241+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:18:22.242+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:18:22.243+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:18:22.244+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:18:22 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2024-06-27T08:18:22.249+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:18:22.253+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
[2024-06-27T08:18:22.264+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 53 for partition store_source_data-0
[2024-06-27T08:18:22.271+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:18:22.785+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:18:22.786+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=54, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:18:22.786+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:18:22.787+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 26.0)
[2024-06-27T08:18:22.787+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 2076 bytes result sent to driver
[2024-06-27T08:18:22.788+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 536 ms on localhost (executor driver) (1/1)
[2024-06-27T08:18:22.791+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.552 s
[2024-06-27T08:18:22.791+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:18:22.794+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-06-27T08:18:22.795+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2024-06-27T08:18:22.796+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.563057 s
[2024-06-27T08:18:22.797+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:18:22.801+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:18:22.802+0000] {docker.py:436} INFO - Batch: 26
-------------------------------------------
[2024-06-27T08:18:22.833+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:18:22.834+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:18:22.909+0000] {docker.py:436} INFO - 24/06/27 08:18:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/26 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.26.6268e97f-2f0e-43ab-8e2f-9ce9c046b6c6.tmp
[2024-06-27T08:18:23.060+0000] {docker.py:436} INFO - 24/06/27 08:18:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.26.6268e97f-2f0e-43ab-8e2f-9ce9c046b6c6.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/26
[2024-06-27T08:18:23.063+0000] {docker.py:436} INFO - 24/06/27 08:18:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:18:21.979Z",
  "batchId" : 26,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9242144177449167,
  "durationMs" : {
    "addBatch" : 717,
    "commitOffsets" : 225,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 51,
    "triggerExecution" : 1082,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 53
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9242144177449167,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:18:33.067+0000] {docker.py:436} INFO - 24/06/27 08:18:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:34.915+0000] {docker.py:436} INFO - 24/06/27 08:18:34 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:18:43.076+0000] {docker.py:436} INFO - 24/06/27 08:18:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:18:53.084+0000] {docker.py:436} INFO - 24/06/27 08:18:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:03.086+0000] {docker.py:436} INFO - 24/06/27 08:19:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:13.096+0000] {docker.py:436} INFO - 24/06/27 08:19:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:23.104+0000] {docker.py:436} INFO - 24/06/27 08:19:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:33.106+0000] {docker.py:436} INFO - 24/06/27 08:19:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:43.109+0000] {docker.py:436} INFO - 24/06/27 08:19:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:53.113+0000] {docker.py:436} INFO - 24/06/27 08:19:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:19:58.006+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/27 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.27.3408ed62-306d-4700-bba7-2b8651d9e437.tmp
[2024-06-27T08:19:58.111+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.27.3408ed62-306d-4700-bba7-2b8651d9e437.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/27
[2024-06-27T08:19:58.116+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1719476397975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:19:58.141+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.149+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.168+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.180+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.225+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.233+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:19:58.282+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:19:58.284+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:19:58.286+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:19:58.288+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:19:58.289+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:19:58.290+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:19:58.291+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[83] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:19:58.294+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.297+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:19:58.334+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:19:58.346+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:19:58.361+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[83] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:19:58.361+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2024-06-27T08:19:58.375+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:19:58.390+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
[2024-06-27T08:19:58.445+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 54 for partition store_source_data-0
[2024-06-27T08:19:58.465+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:19:58.968+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:19:58.969+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=55, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:19:58.981+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:19:58.982+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO DataWritingSparkTask: Committed partition 0 (task 27, attempt 0, stage 27.0)
[2024-06-27T08:19:58.994+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 2076 bytes result sent to driver
[2024-06-27T08:19:58.999+0000] {docker.py:436} INFO - 24/06/27 08:19:58 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 625 ms on localhost (executor driver) (1/1)
[2024-06-27T08:19:59.008+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-06-27T08:19:59.013+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.716 s
[2024-06-27T08:19:59.014+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:19:59.025+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-06-27T08:19:59.025+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.735269 s
[2024-06-27T08:19:59.036+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:19:59.036+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:19:59.037+0000] {docker.py:436} INFO - Batch: 27
[2024-06-27T08:19:59.037+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:19:59.104+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:19:59.107+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:19:59.180+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/27 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.27.3dc36957-48a8-439f-a31f-3229e3b84b33.tmp
[2024-06-27T08:19:59.291+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.27.3dc36957-48a8-439f-a31f-3229e3b84b33.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/27
[2024-06-27T08:19:59.292+0000] {docker.py:436} INFO - 24/06/27 08:19:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:19:57.974Z",
  "batchId" : 27,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7604562737642586,
  "durationMs" : {
    "addBatch" : 951,
    "commitOffsets" : 180,
    "getBatch" : 0,
    "latestOffset" : 1,
    "queryPlanning" : 40,
    "triggerExecution" : 1315,
    "walCommit" : 142
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 54
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7604562737642586,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:20:09.294+0000] {docker.py:436} INFO - 24/06/27 08:20:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:10.619+0000] {docker.py:436} INFO - 24/06/27 08:20:10 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:20:19.303+0000] {docker.py:436} INFO - 24/06/27 08:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:29.306+0000] {docker.py:436} INFO - 24/06/27 08:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:39.314+0000] {docker.py:436} INFO - 24/06/27 08:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:49.332+0000] {docker.py:436} INFO - 24/06/27 08:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:20:59.335+0000] {docker.py:436} INFO - 24/06/27 08:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:09.338+0000] {docker.py:436} INFO - 24/06/27 08:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:19.343+0000] {docker.py:436} INFO - 24/06/27 08:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:29.344+0000] {docker.py:436} INFO - 24/06/27 08:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:39.357+0000] {docker.py:436} INFO - 24/06/27 08:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:49.367+0000] {docker.py:436} INFO - 24/06/27 08:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:21:59.377+0000] {docker.py:436} INFO - 24/06/27 08:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:09.376+0000] {docker.py:436} INFO - 24/06/27 08:22:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:19.388+0000] {docker.py:436} INFO - 24/06/27 08:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:29.398+0000] {docker.py:436} INFO - 24/06/27 08:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:39.398+0000] {docker.py:436} INFO - 24/06/27 08:22:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:49.407+0000] {docker.py:436} INFO - 24/06/27 08:22:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:22:59.406+0000] {docker.py:436} INFO - 24/06/27 08:22:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:09.408+0000] {docker.py:436} INFO - 24/06/27 08:23:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:19.420+0000] {docker.py:436} INFO - 24/06/27 08:23:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:29.427+0000] {docker.py:436} INFO - 24/06/27 08:23:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:39.437+0000] {docker.py:436} INFO - 24/06/27 08:23:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:49.441+0000] {docker.py:436} INFO - 24/06/27 08:23:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:23:53.041+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/28 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.28.47b8b49e-1d72-48af-a160-7add59374662.tmp
[2024-06-27T08:23:53.262+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.28.47b8b49e-1d72-48af-a160-7add59374662.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/28
24/06/27 08:23:53 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1719476632974,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:23:53.285+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.325+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.354+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.370+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.395+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.423+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:23:53.502+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:23:53.509+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:23:53.528+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:23:53 INFO DAGScheduler: Final stage: ResultStage 28 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:23:53 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:23:53 INFO DAGScheduler: Missing parents: List()
24/06/27 08:23:53 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:23:53 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.529+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:23:53.532+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:23:53.533+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:23:53.540+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:23:53.541+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2024-06-27T08:23:53.544+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:23:53.549+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
[2024-06-27T08:23:53.575+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 55 for partition store_source_data-0
[2024-06-27T08:23:53.584+0000] {docker.py:436} INFO - 24/06/27 08:23:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:23:54.090+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:23:54.092+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=56, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:23:54.092+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:23:54 INFO DataWritingSparkTask: Committed partition 0 (task 28, attempt 0, stage 28.0)
[2024-06-27T08:23:54.097+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 2076 bytes result sent to driver
[2024-06-27T08:23:54.098+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 553 ms on localhost (executor driver) (1/1)
24/06/27 08:23:54 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-06-27T08:23:54.099+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: ResultStage 28 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
[2024-06-27T08:23:54.099+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:23:54.100+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2024-06-27T08:23:54.101+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 28
-------------------------------------------
[2024-06-27T08:23:54.113+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.585515 s
24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:23:54.135+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:23:54.136+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:23:54.151+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/28 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.28.ee7485de-77ff-4acb-aae1-f6dfccc40fcc.tmp
[2024-06-27T08:23:54.235+0000] {docker.py:436} INFO - 24/06/27 08:23:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.28.ee7485de-77ff-4acb-aae1-f6dfccc40fcc.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/28
24/06/27 08:23:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:23:52.969Z",
  "batchId" : 28,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 58.8235294117647,
  "processedRowsPerSecond" : 0.7911392405063291,
  "durationMs" : {
    "addBatch" : 817,
    "commitOffsets" : 94,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 57,
    "triggerExecution" : 1264,
    "walCommit" : 287
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 55
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 58.8235294117647,
    "processedRowsPerSecond" : 0.7911392405063291,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:24:04.245+0000] {docker.py:436} INFO - 24/06/27 08:24:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:04.692+0000] {docker.py:436} INFO - 24/06/27 08:24:04 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:24:14.251+0000] {docker.py:436} INFO - 24/06/27 08:24:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:24.269+0000] {docker.py:436} INFO - 24/06/27 08:24:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:34.270+0000] {docker.py:436} INFO - 24/06/27 08:24:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:44.277+0000] {docker.py:436} INFO - 24/06/27 08:24:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:24:54.283+0000] {docker.py:436} INFO - 24/06/27 08:24:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:04.291+0000] {docker.py:436} INFO - 24/06/27 08:25:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:14.298+0000] {docker.py:436} INFO - 24/06/27 08:25:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:24.302+0000] {docker.py:436} INFO - 24/06/27 08:25:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:34.309+0000] {docker.py:436} INFO - 24/06/27 08:25:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:44.313+0000] {docker.py:436} INFO - 24/06/27 08:25:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:25:54.320+0000] {docker.py:436} INFO - 24/06/27 08:25:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:04.332+0000] {docker.py:436} INFO - 24/06/27 08:26:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:14.333+0000] {docker.py:436} INFO - 24/06/27 08:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:24.337+0000] {docker.py:436} INFO - 24/06/27 08:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:34.345+0000] {docker.py:436} INFO - 24/06/27 08:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:44.354+0000] {docker.py:436} INFO - 24/06/27 08:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:26:54.357+0000] {docker.py:436} INFO - 24/06/27 08:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:04.367+0000] {docker.py:436} INFO - 24/06/27 08:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:14.370+0000] {docker.py:436} INFO - 24/06/27 08:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:24.377+0000] {docker.py:436} INFO - 24/06/27 08:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:34.388+0000] {docker.py:436} INFO - 24/06/27 08:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:44.391+0000] {docker.py:436} INFO - 24/06/27 08:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:27:54.397+0000] {docker.py:436} INFO - 24/06/27 08:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:04.398+0000] {docker.py:436} INFO - 24/06/27 08:28:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:14.405+0000] {docker.py:436} INFO - 24/06/27 08:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:24.409+0000] {docker.py:436} INFO - 24/06/27 08:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:34.412+0000] {docker.py:436} INFO - 24/06/27 08:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:44.414+0000] {docker.py:436} INFO - 24/06/27 08:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:54.416+0000] {docker.py:436} INFO - 24/06/27 08:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:28:59.484+0000] {docker.py:436} INFO - 24/06/27 08:28:59 INFO Metrics: Metrics scheduler closed
24/06/27 08:28:59 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:28:59.485+0000] {docker.py:436} INFO - 24/06/27 08:28:59 INFO Metrics: Metrics reporters closed
[2024-06-27T08:28:59.487+0000] {docker.py:436} INFO - 24/06/27 08:28:59 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-2 unregistered
[2024-06-27T08:29:04.417+0000] {docker.py:436} INFO - 24/06/27 08:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:14.422+0000] {docker.py:436} INFO - 24/06/27 08:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:24.424+0000] {docker.py:436} INFO - 24/06/27 08:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:34.425+0000] {docker.py:436} INFO - 24/06/27 08:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:44.433+0000] {docker.py:436} INFO - 24/06/27 08:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:29:54.435+0000] {docker.py:436} INFO - 24/06/27 08:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:04.444+0000] {docker.py:436} INFO - 24/06/27 08:30:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:14.445+0000] {docker.py:436} INFO - 24/06/27 08:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:23.680+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/29 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.29.48cdc3d6-e4d4-43d6-8b62-7a29a827147c.tmp
[2024-06-27T08:30:23.780+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.29.48cdc3d6-e4d4-43d6-8b62-7a29a827147c.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/29
[2024-06-27T08:30:23.781+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1719477023659,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:30:23.816+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.833+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.841+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.858+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.865+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:30:23.888+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:30:23.916+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:30:23.922+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:30:23.923+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:30:23.924+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:30:23.924+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:30:23.925+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[89] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:30:23.932+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:30:23.937+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:30:23.947+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:30:23.947+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:30:23.949+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[89] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:30:23 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
24/06/27 08:30:23 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:30:23.951+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
[2024-06-27T08:30:23.992+0000] {docker.py:436} INFO - 24/06/27 08:30:23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:30:24.031+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:30:24.039+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:30:24.042+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO AppInfoParser: Kafka startTimeMs: 1719477024031
[2024-06-27T08:30:24.043+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:30:24.043+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 56 for partition store_source_data-0
[2024-06-27T08:30:24.091+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:30:24.234+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:30:24.742+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:30:24.765+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=57, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:30:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:30:24 INFO DataWritingSparkTask: Committed partition 0 (task 29, attempt 0, stage 29.0)
24/06/27 08:30:24 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 2076 bytes result sent to driver
24/06/27 08:30:24 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 812 ms on localhost (executor driver) (1/1)
24/06/27 08:30:24 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.835 s
[2024-06-27T08:30:24.781+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:30:24 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2024-06-27T08:30:24.781+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2024-06-27T08:30:24.791+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 29
-------------------------------------------
[2024-06-27T08:30:24.791+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.863962 s
24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:30:24.939+0000] {docker.py:436} INFO - 24/06/27 08:30:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:30:24.940+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:30:25.057+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/29 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.29.cdc2c5f2-f752-4a53-8cf3-49dd07956ee4.tmp
[2024-06-27T08:30:25.201+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.29.cdc2c5f2-f752-4a53-8cf3-49dd07956ee4.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/29
[2024-06-27T08:30:25.201+0000] {docker.py:436} INFO - 24/06/27 08:30:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:30:23.654Z",
  "batchId" : 29,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 90.90909090909092,
  "processedRowsPerSecond" : 0.6472491909385114,
  "durationMs" : {
    "addBatch" : 1139,
    "commitOffsets" : 242,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 37,
    "triggerExecution" : 1545,
    "walCommit" : 122
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 56
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 90.90909090909092,
    "processedRowsPerSecond" : 0.6472491909385114,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:30:35.211+0000] {docker.py:436} INFO - 24/06/27 08:30:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:37.832+0000] {docker.py:436} INFO - 24/06/27 08:30:37 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:30:45.218+0000] {docker.py:436} INFO - 24/06/27 08:30:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:30:55.224+0000] {docker.py:436} INFO - 24/06/27 08:30:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:05.227+0000] {docker.py:436} INFO - 24/06/27 08:31:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:15.237+0000] {docker.py:436} INFO - 24/06/27 08:31:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:23.829+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/30 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.30.5db5000a-062b-4482-b4f9-fb41903ebd40.tmp
[2024-06-27T08:31:23.985+0000] {docker.py:436} INFO - 24/06/27 08:31:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.30.5db5000a-062b-4482-b4f9-fb41903ebd40.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/30
24/06/27 08:31:23 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1719477083809,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:31:24.061+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.068+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.149+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.157+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.194+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:31:24 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:31:24.293+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:31:24.293+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:31:24.294+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:31:24.294+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:31:24 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:31:24 INFO DAGScheduler: Missing parents: List()
24/06/27 08:31:24 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:31:24.295+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.306+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:31:24.316+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:31:24.329+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:31:24.333+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[92] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-27T08:31:24.335+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2024-06-27T08:31:24.338+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:31:24.345+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
[2024-06-27T08:31:24.379+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 57 for partition store_source_data-0
[2024-06-27T08:31:24.390+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:31:24.897+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:31:24.901+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=58, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:31:24.902+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:31:24.903+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DataWritingSparkTask: Committed partition 0 (task 30, attempt 0, stage 30.0)
[2024-06-27T08:31:24.907+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 2119 bytes result sent to driver
[2024-06-27T08:31:24.912+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 573 ms on localhost (executor driver) (1/1)
24/06/27 08:31:24 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
24/06/27 08:31:24 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.627 s
[2024-06-27T08:31:24.912+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:31:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2024-06-27T08:31:24.914+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.639706 s
24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:31:24.914+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 30
-------------------------------------------
[2024-06-27T08:31:24.936+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:31:24.938+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:31:24.952+0000] {docker.py:436} INFO - 24/06/27 08:31:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/30 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.30.bf428b72-729e-4cba-b634-a6c72b4903b4.tmp
[2024-06-27T08:31:25.021+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.30.bf428b72-729e-4cba-b634-a6c72b4903b4.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/30
[2024-06-27T08:31:25.022+0000] {docker.py:436} INFO - 24/06/27 08:31:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:31:23.801Z",
  "batchId" : 30,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.819672131147541,
  "durationMs" : {
    "addBatch" : 847,
    "commitOffsets" : 84,
    "getBatch" : 1,
    "latestOffset" : 8,
    "queryPlanning" : 101,
    "triggerExecution" : 1220,
    "walCommit" : 173
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 57
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.819672131147541,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:31:34.157+0000] {docker.py:436} INFO - 24/06/27 08:31:34 INFO BlockManagerInfo: Removed broadcast_30_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:31:35.023+0000] {docker.py:436} INFO - 24/06/27 08:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:45.029+0000] {docker.py:436} INFO - 24/06/27 08:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:31:55.031+0000] {docker.py:436} INFO - 24/06/27 08:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:05.039+0000] {docker.py:436} INFO - 24/06/27 08:32:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:15.051+0000] {docker.py:436} INFO - 24/06/27 08:32:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:25.054+0000] {docker.py:436} INFO - 24/06/27 08:32:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:35.059+0000] {docker.py:436} INFO - 24/06/27 08:32:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:45.062+0000] {docker.py:436} INFO - 24/06/27 08:32:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:32:55.071+0000] {docker.py:436} INFO - 24/06/27 08:32:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:05.079+0000] {docker.py:436} INFO - 24/06/27 08:33:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:15.088+0000] {docker.py:436} INFO - 24/06/27 08:33:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:25.091+0000] {docker.py:436} INFO - 24/06/27 08:33:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:35.095+0000] {docker.py:436} INFO - 24/06/27 08:33:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:45.104+0000] {docker.py:436} INFO - 24/06/27 08:33:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:33:55.108+0000] {docker.py:436} INFO - 24/06/27 08:33:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:05.110+0000] {docker.py:436} INFO - 24/06/27 08:34:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:15.118+0000] {docker.py:436} INFO - 24/06/27 08:34:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:25.118+0000] {docker.py:436} INFO - 24/06/27 08:34:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:35.121+0000] {docker.py:436} INFO - 24/06/27 08:34:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:45.125+0000] {docker.py:436} INFO - 24/06/27 08:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:34:55.133+0000] {docker.py:436} INFO - 24/06/27 08:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:05.140+0000] {docker.py:436} INFO - 24/06/27 08:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:15.145+0000] {docker.py:436} INFO - 24/06/27 08:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:25.149+0000] {docker.py:436} INFO - 24/06/27 08:35:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:35.152+0000] {docker.py:436} INFO - 24/06/27 08:35:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:45.154+0000] {docker.py:436} INFO - 24/06/27 08:35:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:35:55.159+0000] {docker.py:436} INFO - 24/06/27 08:35:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:05.162+0000] {docker.py:436} INFO - 24/06/27 08:36:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:15.166+0000] {docker.py:436} INFO - 24/06/27 08:36:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:25.176+0000] {docker.py:436} INFO - 24/06/27 08:36:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:35.179+0000] {docker.py:436} INFO - 24/06/27 08:36:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:45.184+0000] {docker.py:436} INFO - 24/06/27 08:36:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:36:50.155+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/31 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.31.3e2e394e-8319-472b-917f-aed0347c17bd.tmp
[2024-06-27T08:36:50.284+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.31.3e2e394e-8319-472b-917f-aed0347c17bd.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/31
[2024-06-27T08:36:50.285+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1719477410143,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:36:50.320+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.323+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.347+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.355+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.357+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:36:50.373+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:36:50.374+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:36:50.378+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:36:50 INFO DAGScheduler: Final stage: ResultStage 31 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:36:50 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:36:50.383+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Missing parents: List()
24/06/27 08:36:50 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[95] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:36:50.387+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:36:50.391+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
24/06/27 08:36:50 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
24/06/27 08:36:50 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:36:50.395+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[95] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:36:50 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2024-06-27T08:36:50.395+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:36:50.398+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
[2024-06-27T08:36:50.411+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 58 for partition store_source_data-0
[2024-06-27T08:36:50.429+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:36:50.928+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:36:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:36:50.932+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=59, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:36:50.937+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:36:50 INFO DataWritingSparkTask: Committed partition 0 (task 31, attempt 0, stage 31.0)
24/06/27 08:36:50 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 2076 bytes result sent to driver
24/06/27 08:36:50 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 540 ms on localhost (executor driver) (1/1)
24/06/27 08:36:50 INFO DAGScheduler: ResultStage 31 (start at NativeMethodAccessorImpl.java:0) finished in 0.552 s
24/06/27 08:36:50 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:36:50.948+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 31
-------------------------------------------
[2024-06-27T08:36:50.948+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/06/27 08:36:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
24/06/27 08:36:50 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.571647 s
24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:36:50.982+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:36:50.986+0000] {docker.py:436} INFO - 24/06/27 08:36:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:36:51.025+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/31 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.31.cdab9a89-3fd4-4f3f-9506-9b2dfdc431ed.tmp
[2024-06-27T08:36:51.152+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.31.cdab9a89-3fd4-4f3f-9506-9b2dfdc431ed.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/31
[2024-06-27T08:36:51.158+0000] {docker.py:436} INFO - 24/06/27 08:36:51 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:36:50.141Z",
  "batchId" : 31,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.9852216748768474,
  "durationMs" : {
    "addBatch" : 663,
    "commitOffsets" : 168,
    "getBatch" : 0,
    "latestOffset" : 2,
    "queryPlanning" : 41,
    "triggerExecution" : 1015,
    "walCommit" : 141
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 58
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.9852216748768474,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:36:55.115+0000] {docker.py:436} INFO - 24/06/27 08:36:55 INFO BlockManagerInfo: Removed broadcast_31_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:37:01.172+0000] {docker.py:436} INFO - 24/06/27 08:37:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:11.173+0000] {docker.py:436} INFO - 24/06/27 08:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:21.177+0000] {docker.py:436} INFO - 24/06/27 08:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:31.181+0000] {docker.py:436} INFO - 24/06/27 08:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:41.188+0000] {docker.py:436} INFO - 24/06/27 08:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:37:51.198+0000] {docker.py:436} INFO - 24/06/27 08:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:01.198+0000] {docker.py:436} INFO - 24/06/27 08:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:11.209+0000] {docker.py:436} INFO - 24/06/27 08:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:21.211+0000] {docker.py:436} INFO - 24/06/27 08:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:31.216+0000] {docker.py:436} INFO - 24/06/27 08:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:41.216+0000] {docker.py:436} INFO - 24/06/27 08:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:38:51.217+0000] {docker.py:436} INFO - 24/06/27 08:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:01.222+0000] {docker.py:436} INFO - 24/06/27 08:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:11.231+0000] {docker.py:436} INFO - 24/06/27 08:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:21.240+0000] {docker.py:436} INFO - 24/06/27 08:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:31.250+0000] {docker.py:436} INFO - 24/06/27 08:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:41.258+0000] {docker.py:436} INFO - 24/06/27 08:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:51.261+0000] {docker.py:436} INFO - 24/06/27 08:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:39:55.659+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/32 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.32.a1f87258-4b29-46ca-b4f8-e24e5e32def4.tmp
[2024-06-27T08:39:55.800+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.32.a1f87258-4b29-46ca-b4f8-e24e5e32def4.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/32
[2024-06-27T08:39:55.816+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1719477595645,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:39:55.849+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.861+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.891+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.894+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.909+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.910+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:39:55.929+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:39:55.964+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:39:55.983+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:39:55.983+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:39:55.984+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:39:55.984+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:39:55.994+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/27 08:39:55 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
24/06/27 08:39:55 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
24/06/27 08:39:55 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:39:55.999+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
24/06/27 08:39:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[98] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:39:55 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2024-06-27T08:39:56.000+0000] {docker.py:436} INFO - 24/06/27 08:39:55 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:39:56.014+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
[2024-06-27T08:39:56.070+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 59 for partition store_source_data-0
[2024-06-27T08:39:56.115+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:39:56.623+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 08:39:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=60, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:39:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:39:56 INFO DataWritingSparkTask: Committed partition 0 (task 32, attempt 0, stage 32.0)
24/06/27 08:39:56 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 2076 bytes result sent to driver
24/06/27 08:39:56 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 625 ms on localhost (executor driver) (1/1)
[2024-06-27T08:39:56.624+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2024-06-27T08:39:56.626+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 32
-------------------------------------------
[2024-06-27T08:39:56.627+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.639 s
24/06/27 08:39:56 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:39:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
24/06/27 08:39:56 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.659659 s
24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:39:56.695+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:39:56.695+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:39:56.765+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/32 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.32.bc557e0c-b772-455d-b5f2-b6b13cb35e18.tmp
[2024-06-27T08:39:56.953+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.32.bc557e0c-b772-455d-b5f2-b6b13cb35e18.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/32
[2024-06-27T08:39:56.977+0000] {docker.py:436} INFO - 24/06/27 08:39:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:39:55.642Z",
  "batchId" : 32,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7639419404125287,
  "durationMs" : {
    "addBatch" : 845,
    "commitOffsets" : 238,
    "getBatch" : 0,
    "latestOffset" : 3,
    "queryPlanning" : 52,
    "triggerExecution" : 1309,
    "walCommit" : 170
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 59
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7639419404125287,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:40:06.666+0000] {docker.py:436} INFO - 24/06/27 08:40:06 INFO BlockManagerInfo: Removed broadcast_32_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:40:06.954+0000] {docker.py:436} INFO - 24/06/27 08:40:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:16.961+0000] {docker.py:436} INFO - 24/06/27 08:40:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:26.962+0000] {docker.py:436} INFO - 24/06/27 08:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:36.967+0000] {docker.py:436} INFO - 24/06/27 08:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:46.984+0000] {docker.py:436} INFO - 24/06/27 08:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:40:56.988+0000] {docker.py:436} INFO - 24/06/27 08:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:06.995+0000] {docker.py:436} INFO - 24/06/27 08:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:17.005+0000] {docker.py:436} INFO - 24/06/27 08:41:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:27.006+0000] {docker.py:436} INFO - 24/06/27 08:41:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:37.012+0000] {docker.py:436} INFO - 24/06/27 08:41:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:47.018+0000] {docker.py:436} INFO - 24/06/27 08:41:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:41:57.020+0000] {docker.py:436} INFO - 24/06/27 08:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:07.025+0000] {docker.py:436} INFO - 24/06/27 08:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:17.029+0000] {docker.py:436} INFO - 24/06/27 08:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:19.531+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/33 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.33.d71b2591-38bd-4166-b94b-732174dd426f.tmp
[2024-06-27T08:42:19.727+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.33.d71b2591-38bd-4166-b94b-732174dd426f.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/33
24/06/27 08:42:19 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1719477739494,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:19.758+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.774+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.834+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.840+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.926+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.930+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:19.964+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/27 08:42:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/27 08:42:19 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:42:19 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:42:19 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:42:19 INFO DAGScheduler: Missing parents: List()
24/06/27 08:42:19 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:19.968+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:42:19.971+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:42:19.979+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:42:19.982+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:19.983+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[101] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:42:19 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2024-06-27T08:42:19.984+0000] {docker.py:436} INFO - 24/06/27 08:42:19 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:42:20.001+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
[2024-06-27T08:42:20.038+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 60 for partition store_source_data-0
[2024-06-27T08:42:20.045+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:20.548+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
24/06/27 08:42:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=61, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:20 INFO DataWritingSparkTask: Committed partition 0 (task 33, attempt 0, stage 33.0)
[2024-06-27T08:42:20.552+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 2076 bytes result sent to driver
[2024-06-27T08:42:20.553+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 570 ms on localhost (executor driver) (1/1)
[2024-06-27T08:42:20.554+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2024-06-27T08:42:20.555+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.593 s
[2024-06-27T08:42:20.555+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:42:20.556+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2024-06-27T08:42:20.556+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.595482 s
[2024-06-27T08:42:20.574+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:20.574+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 33
-------------------------------------------
[2024-06-27T08:42:20.645+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:20.645+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:42:20.700+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/33 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.33.690bb1a7-d0b2-4fa2-b7d6-64b0bb373b71.tmp
[2024-06-27T08:42:20.841+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.33.690bb1a7-d0b2-4fa2-b7d6-64b0bb373b71.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/33
[2024-06-27T08:42:20.842+0000] {docker.py:436} INFO - 24/06/27 08:42:20 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:19.490Z",
  "batchId" : 33,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.7412898443291327,
  "durationMs" : {
    "addBatch" : 875,
    "commitOffsets" : 195,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 47,
    "triggerExecution" : 1349,
    "walCommit" : 225
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 60
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.7412898443291327,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:25.392+0000] {docker.py:436} INFO - 24/06/27 08:42:25 INFO BlockManagerInfo: Removed broadcast_33_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:42:30.707+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/34 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.34.6696e55b-40fc-42d9-9fe5-10e1e99d45a4.tmp
[2024-06-27T08:42:30.819+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.34.6696e55b-40fc-42d9-9fe5-10e1e99d45a4.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/34
24/06/27 08:42:30 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1719477750683,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:42:30.904+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.912+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.943+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:30.949+0000] {docker.py:436} INFO - 24/06/27 08:42:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.008+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.020+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:42:31.130+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:42:31.141+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:42:31.145+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/27 08:42:31 INFO DAGScheduler: Final stage: ResultStage 34 (start at NativeMethodAccessorImpl.java:0)
24/06/27 08:42:31 INFO DAGScheduler: Parents of final stage: List()
24/06/27 08:42:31 INFO DAGScheduler: Missing parents: List()
24/06/27 08:42:31 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:42:31.157+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.167+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:42:31.170+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:42:31.176+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585
[2024-06-27T08:42:31.182+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:42:31 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2024-06-27T08:42:31.183+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:42:31.192+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
[2024-06-27T08:42:31.252+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 61 for partition store_source_data-0
[2024-06-27T08:42:31.293+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:42:31.777+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:42:31.797+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=62, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:42:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:42:31 INFO DataWritingSparkTask: Committed partition 0 (task 34, attempt 0, stage 34.0)
[2024-06-27T08:42:31.798+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 2076 bytes result sent to driver
[2024-06-27T08:42:31.799+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 603 ms on localhost (executor driver) (1/1)
24/06/27 08:42:31 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-06-27T08:42:31.799+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: ResultStage 34 (start at NativeMethodAccessorImpl.java:0) finished in 0.645 s
24/06/27 08:42:31 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/27 08:42:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2024-06-27T08:42:31.800+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 34
-------------------------------------------
[2024-06-27T08:42:31.800+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.651616 s
24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:42:31.864+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:42:31.865+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:42:31.908+0000] {docker.py:436} INFO - 24/06/27 08:42:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/34 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.34.9a1ed1fb-94a2-4905-a1c3-48c19fc6b39c.tmp
[2024-06-27T08:42:32.097+0000] {docker.py:436} INFO - 24/06/27 08:42:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.34.9a1ed1fb-94a2-4905-a1c3-48c19fc6b39c.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/34
24/06/27 08:42:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:42:30.678Z",
  "batchId" : 34,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 0.7112375533428166,
  "durationMs" : {
    "addBatch" : 927,
    "commitOffsets" : 231,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 104,
    "triggerExecution" : 1406,
    "walCommit" : 133
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 61
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 0.7112375533428166,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:42:42.089+0000] {docker.py:436} INFO - 24/06/27 08:42:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:42:42.173+0000] {docker.py:436} INFO - 24/06/27 08:42:42 INFO BlockManagerInfo: Removed broadcast_34_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:42:52.091+0000] {docker.py:436} INFO - 24/06/27 08:42:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:02.102+0000] {docker.py:436} INFO - 24/06/27 08:43:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:12.120+0000] {docker.py:436} INFO - 24/06/27 08:43:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:22.124+0000] {docker.py:436} INFO - 24/06/27 08:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:32.124+0000] {docker.py:436} INFO - 24/06/27 08:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:42.129+0000] {docker.py:436} INFO - 24/06/27 08:43:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:43:52.131+0000] {docker.py:436} INFO - 24/06/27 08:43:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:02.142+0000] {docker.py:436} INFO - 24/06/27 08:44:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:12.153+0000] {docker.py:436} INFO - 24/06/27 08:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:22.164+0000] {docker.py:436} INFO - 24/06/27 08:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:32.165+0000] {docker.py:436} INFO - 24/06/27 08:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:42.171+0000] {docker.py:436} INFO - 24/06/27 08:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:44:52.174+0000] {docker.py:436} INFO - 24/06/27 08:44:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:02.178+0000] {docker.py:436} INFO - 24/06/27 08:45:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:12.189+0000] {docker.py:436} INFO - 24/06/27 08:45:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:22.191+0000] {docker.py:436} INFO - 24/06/27 08:45:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:32.201+0000] {docker.py:436} INFO - 24/06/27 08:45:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:42.207+0000] {docker.py:436} INFO - 24/06/27 08:45:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:45:48.202+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/35 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.35.071ce0a2-75d6-45d2-891a-382e7e2d4f12.tmp
[2024-06-27T08:45:48.453+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.35.071ce0a2-75d6-45d2-891a-382e7e2d4f12.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/35
24/06/27 08:45:48 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1719477948156,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:45:48.507+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.508+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.532+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.533+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.587+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.598+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:45:48.762+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:45:48.762+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:45:48.784+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:45:48.793+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:45:48.793+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:45:48.794+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:45:48.799+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:45:48.823+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:45:48.823+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:45:48.824+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:45:48.851+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
24/06/27 08:45:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:45:48 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2024-06-27T08:45:48.852+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:45:48.890+0000] {docker.py:436} INFO - 24/06/27 08:45:48 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
[2024-06-27T08:45:49.006+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 62 for partition store_source_data-0
[2024-06-27T08:45:49.050+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:45:49.562+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.564+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:45:49.569+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=63, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:45:49.573+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
[2024-06-27T08:45:49.580+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DataWritingSparkTask: Committed partition 0 (task 35, attempt 0, stage 35.0)
[2024-06-27T08:45:49.584+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 2076 bytes result sent to driver
[2024-06-27T08:45:49.586+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 745 ms on localhost (executor driver) (1/1)
[2024-06-27T08:45:49.587+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.784 s
[2024-06-27T08:45:49.588+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:45:49.588+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-06-27T08:45:49.589+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2024-06-27T08:45:49.589+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.805135 s
[2024-06-27T08:45:49.594+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:45:49.595+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:45:49.595+0000] {docker.py:436} INFO - Batch: 35
[2024-06-27T08:45:49.596+0000] {docker.py:436} INFO - -------------------------------------------
[2024-06-27T08:45:49.665+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:45:49.665+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:45:49.699+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/35 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.35.9b503abe-d82f-4d07-9cc0-3c3e8863853e.tmp
[2024-06-27T08:45:49.884+0000] {docker.py:436} INFO - 24/06/27 08:45:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.35.9b503abe-d82f-4d07-9cc0-3c3e8863853e.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/35
24/06/27 08:45:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:45:48.148Z",
  "batchId" : 35,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 83.33333333333333,
  "processedRowsPerSecond" : 0.5767012687427913,
  "durationMs" : {
    "addBatch" : 1151,
    "commitOffsets" : 218,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 61,
    "triggerExecution" : 1734,
    "walCommit" : 296
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 62
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 83.33333333333333,
    "processedRowsPerSecond" : 0.5767012687427913,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:45:59.888+0000] {docker.py:436} INFO - 24/06/27 08:45:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:04.625+0000] {docker.py:436} INFO - 24/06/27 08:46:04 INFO BlockManagerInfo: Removed broadcast_35_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:46:09.890+0000] {docker.py:436} INFO - 24/06/27 08:46:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:19.900+0000] {docker.py:436} INFO - 24/06/27 08:46:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:29.900+0000] {docker.py:436} INFO - 24/06/27 08:46:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:39.906+0000] {docker.py:436} INFO - 24/06/27 08:46:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:49.918+0000] {docker.py:436} INFO - 24/06/27 08:46:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:46:59.921+0000] {docker.py:436} INFO - 24/06/27 08:46:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:09.924+0000] {docker.py:436} INFO - 24/06/27 08:47:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:19.933+0000] {docker.py:436} INFO - 24/06/27 08:47:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:29.942+0000] {docker.py:436} INFO - 24/06/27 08:47:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:39.950+0000] {docker.py:436} INFO - 24/06/27 08:47:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:49.954+0000] {docker.py:436} INFO - 24/06/27 08:47:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:47:59.957+0000] {docker.py:436} INFO - 24/06/27 08:47:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:09.957+0000] {docker.py:436} INFO - 24/06/27 08:48:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:19.959+0000] {docker.py:436} INFO - 24/06/27 08:48:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:29.967+0000] {docker.py:436} INFO - 24/06/27 08:48:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:39.977+0000] {docker.py:436} INFO - 24/06/27 08:48:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:49.982+0000] {docker.py:436} INFO - 24/06/27 08:48:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:48:59.986+0000] {docker.py:436} INFO - 24/06/27 08:48:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:09.989+0000] {docker.py:436} INFO - 24/06/27 08:49:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:19.993+0000] {docker.py:436} INFO - 24/06/27 08:49:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:30.003+0000] {docker.py:436} INFO - 24/06/27 08:49:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:40.004+0000] {docker.py:436} INFO - 24/06/27 08:49:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:49:50.013+0000] {docker.py:436} INFO - 24/06/27 08:49:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:00.019+0000] {docker.py:436} INFO - 24/06/27 08:50:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:10.026+0000] {docker.py:436} INFO - 24/06/27 08:50:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:20.033+0000] {docker.py:436} INFO - 24/06/27 08:50:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:30.040+0000] {docker.py:436} INFO - 24/06/27 08:50:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:40.048+0000] {docker.py:436} INFO - 24/06/27 08:50:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:50.050+0000] {docker.py:436} INFO - 24/06/27 08:50:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:50:59.493+0000] {docker.py:436} INFO - 24/06/27 08:50:59 INFO Metrics: Metrics scheduler closed
24/06/27 08:50:59 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-27T08:50:59.494+0000] {docker.py:436} INFO - 24/06/27 08:50:59 INFO Metrics: Metrics reporters closed
[2024-06-27T08:50:59.496+0000] {docker.py:436} INFO - 24/06/27 08:50:59 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-3 unregistered
[2024-06-27T08:51:00.055+0000] {docker.py:436} INFO - 24/06/27 08:51:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:08.361+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/36 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.36.278b6f03-9c4f-4a2e-934c-1aeadccff5ca.tmp
[2024-06-27T08:51:08.520+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/.36.278b6f03-9c4f-4a2e-934c-1aeadccff5ca.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/offsets/36
[2024-06-27T08:51:08.520+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1719478268291,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-27T08:51:08.520+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.521+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.591+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.610+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.726+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.734+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-27T08:51:08.809+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
[2024-06-27T08:51:08.832+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-27T08:51:08.835+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-06-27T08:51:08.836+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)
[2024-06-27T08:51:08.837+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Parents of final stage: List()
[2024-06-27T08:51:08.839+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Missing parents: List()
[2024-06-27T08:51:08.843+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[110] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-27T08:51:08.854+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 9.4 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.903+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 434.4 MiB)
[2024-06-27T08:51:08.916+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on localhost:37687 (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:51:08.923+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
24/06/27 08:51:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[110] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/27 08:51:08 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
24/06/27 08:51:08 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11071 bytes)
[2024-06-27T08:51:08.924+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
[2024-06-27T08:51:08.981+0000] {docker.py:436} INFO - 24/06/27 08:51:08 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-27T08:51:09.080+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO AppInfoParser: Kafka version: 2.8.1
[2024-06-27T08:51:09.098+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
[2024-06-27T08:51:09.099+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO AppInfoParser: Kafka startTimeMs: 1719478269079
[2024-06-27T08:51:09.099+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Subscribed to partition(s): store_source_data-0
[2024-06-27T08:51:09.105+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to offset 63 for partition store_source_data-0
[2024-06-27T08:51:09.116+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Cluster ID: DvxeahjVTmCRqiJU0Wz8qg
[2024-06-27T08:51:09.182+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to EARLIEST offset of partition store_source_data-0
[2024-06-27T08:51:09.678+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Seeking to LATEST offset of partition store_source_data-0
[2024-06-27T08:51:09.679+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor-4, groupId=spark-kafka-source-1236f316-8b5f-4272-8ab9-f1f2d569f4a9-1554011195-executor] Resetting offset for partition store_source_data-0 to position FetchPosition{offset=64, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-27T08:51:09.682+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/27 08:51:09 INFO DataWritingSparkTask: Committed partition 0 (task 36, attempt 0, stage 36.0)
[2024-06-27T08:51:09.705+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 2119 bytes result sent to driver
[2024-06-27T08:51:09.706+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 784 ms on localhost (executor driver) (1/1)
[2024-06-27T08:51:09.708+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.860 s
24/06/27 08:51:09 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-06-27T08:51:09.714+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/06/27 08:51:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2024-06-27T08:51:09.718+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.882973 s
24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
[2024-06-27T08:51:09.719+0000] {docker.py:436} INFO - -------------------------------------------
Batch: 36
-------------------------------------------
[2024-06-27T08:51:09.823+0000] {docker.py:436} INFO - +------------------------+
|substring(value, 11, 30)|
+------------------------+
|    s": [{"item_purch...|
+------------------------+
[2024-06-27T08:51:09.824+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
[2024-06-27T08:51:09.884+0000] {docker.py:436} INFO - 24/06/27 08:51:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/36 using temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.36.bc0c3cd8-14f0-4568-b079-53485cca7c97.tmp
[2024-06-27T08:51:10.058+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/.36.bc0c3cd8-14f0-4568-b079-53485cca7c97.tmp to file:/tmp/temporary-dd88ac07-9eb3-444c-bd9f-122c37e5e13a/commits/36
[2024-06-27T08:51:10.059+0000] {docker.py:436} INFO - 24/06/27 08:51:10 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dfb325bf-f699-4842-a1b7-b61e75930027",
  "runId" : "70448083-fb9f-4c88-b6d7-0434aedb98db",
  "name" : null,
  "timestamp" : "2024-06-27T08:51:08.286Z",
  "batchId" : 36,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 58.8235294117647,
  "processedRowsPerSecond" : 0.5643340857787811,
  "durationMs" : {
    "addBatch" : 1295,
    "commitOffsets" : 238,
    "getBatch" : 1,
    "latestOffset" : 5,
    "queryPlanning" : 31,
    "triggerExecution" : 1772,
    "walCommit" : 202
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[store_source_data]]",
    "startOffset" : {
      "store_source_data" : {
        "0" : 63
      }
    },
    "endOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "latestOffset" : {
      "store_source_data" : {
        "0" : 64
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 58.8235294117647,
    "processedRowsPerSecond" : 0.5643340857787811,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@24f3253b",
    "numOutputRows" : 1
  }
}
[2024-06-27T08:51:13.845+0000] {docker.py:436} INFO - 24/06/27 08:51:13 INFO BlockManagerInfo: Removed broadcast_36_piece0 on localhost:37687 in memory (size: 4.6 KiB, free: 434.4 MiB)
[2024-06-27T08:51:20.059+0000] {docker.py:436} INFO - 24/06/27 08:51:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:30.064+0000] {docker.py:436} INFO - 24/06/27 08:51:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:40.070+0000] {docker.py:436} INFO - 24/06/27 08:51:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:51:50.075+0000] {docker.py:436} INFO - 24/06/27 08:51:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:00.085+0000] {docker.py:436} INFO - 24/06/27 08:52:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:10.086+0000] {docker.py:436} INFO - 24/06/27 08:52:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:20.092+0000] {docker.py:436} INFO - 24/06/27 08:52:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:30.103+0000] {docker.py:436} INFO - 24/06/27 08:52:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:40.113+0000] {docker.py:436} INFO - 24/06/27 08:52:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:52:50.123+0000] {docker.py:436} INFO - 24/06/27 08:52:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:00.134+0000] {docker.py:436} INFO - 24/06/27 08:53:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:10.145+0000] {docker.py:436} INFO - 24/06/27 08:53:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:20.151+0000] {docker.py:436} INFO - 24/06/27 08:53:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:30.151+0000] {docker.py:436} INFO - 24/06/27 08:53:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2024-06-27T08:53:36.462+0000] {local_task_job_runner.py:124} ERROR - Received SIGTERM. Terminating subprocesses
[2024-06-27T08:53:36.615+0000] {process_utils.py:132} INFO - Sending 15 to group 391. PIDs of all processes in the group: [391]
[2024-06-27T08:53:36.640+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 391
[2024-06-27T08:53:36.644+0000] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-06-27T08:53:36.645+0000] {docker.py:528} INFO - Stopping docker container
[2024-06-27T08:53:36.665+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-27T08:53:36.668+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 265, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.45/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 371, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 398, in _run_image_with_mounts
    self.container = self.cli.create_container(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 439, in create_container
    return self.create_container_from_config(config, name, platform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 456, in create_container_from_config
    return self._result(res, True)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 271, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 267, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.45/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/airflowtmpox1kdw7k")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c016d0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/e3f94c7a6549e6e68357d8c80e4fb7db1306e625b2ee6635c64d571a19501084/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c016d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 438, in _run_image_with_mounts
    result = self.cli.wait(self.container["Id"])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1346, in wait
    res = self._post(url, timeout=timeout, params=params)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/e3f94c7a6549e6e68357d8c80e4fb7db1306e625b2ee6635c64d571a19501084/wait (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c016d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c88f20>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/e3f94c7a6549e6e68357d8c80e4fb7db1306e625b2ee6635c64d571a19501084?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c88f20>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 380, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 462, in _run_image_with_mounts
    self.cli.remove_container(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1033, in remove_container
    res = self._delete(
          ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 244, in _delete
    return self.delete(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 671, in delete
    return self.request("DELETE", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/e3f94c7a6549e6e68357d8c80e4fb7db1306e625b2ee6635c64d571a19501084?v=False&link=False&force=False (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c88f20>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 496, in _make_request
    conn.request(
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 400, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 238, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x730f26c89550>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='docker-proxy', port=2375): Max retries exceeded with url: /v1.45/containers/e3f94c7a6549e6e68357d8c80e4fb7db1306e625b2ee6635c64d571a19501084/stop (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x730f26c89550>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 509, in execute
    return self._run_image()
           ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 368, in _run_image
    with TemporaryDirectory(prefix="airflowtmp", dir=self.host_tmp_dir) as host_tmp_dir_generated:
  File "/usr/local/lib/python3.12/tempfile.py", line 946, in __exit__
    self.cleanup()
  File "/usr/local/lib/python3.12/tempfile.py", line 950, in cleanup
    self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)
  File "/usr/local/lib/python3.12/tempfile.py", line 930, in _rmtree
    _shutil.rmtree(name, onexc=onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 785, in rmtree
    _rmtree_safe_fd(fd, path, onexc)
  File "/usr/local/lib/python3.12/shutil.py", line 661, in _rmtree_safe_fd
    onexc(os.scandir, path, err)
  File "/usr/local/lib/python3.12/shutil.py", line 658, in _rmtree_safe_fd
    entries = list(scandir_it)
              ^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 2612, in signal_handler
    self.task.on_kill()
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/docker/operators/docker.py", line 532, in on_kill
    self.cli.stop(self.container["Id"])
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/container.py", line 1210, in stop
    res = self._post(url, params=params, timeout=conn_timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/utils/decorators.py", line 44, in inner
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/docker/api/client.py", line 232, in _post
    return self.post(url, **self._set_request_timeout(kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/requests/adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: [Errno None] None: '/tmp/airflowtmpox1kdw7k'
[2024-06-27T08:53:36.993+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=Stage_1, task_id=run_bronze_job, run_id=manual__2024-06-27T07:14:19.818705+00:00, execution_date=20240627T071419, start_date=20240627T071426, end_date=20240627T085336
[2024-06-27T08:53:37.233+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 136 for task run_bronze_job ([Errno None] None: '/tmp/airflowtmpox1kdw7k'; 391)
[2024-06-27T08:53:37.269+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=391, status='terminated', exitcode=1, started='07:14:25') (391) terminated with exit code 1
[2024-06-27T08:53:37.270+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 143
[2024-06-27T08:53:37.329+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-27T08:53:37.766+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
